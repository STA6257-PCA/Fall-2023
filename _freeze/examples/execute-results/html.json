{
  "hash": "dbf4cc30c2f3f3a1ad1509a055800d20",
  "result": {
    "markdown": "# Examples\n\nMoving beyond the theoretical foundations of principal components, how is PCA applied to data? We offer two examples; the first a demonstration of the manual calculation of principal components, and the second implementing PCA on a large dataset using R.\n\n## Manual calculation of principal components\n\nIn this illustration, we have access to the two grades of four students in a statistics subject. We aim to employ principal component analysis as a means to reduce the dimensionality from two variables to a singular variable. This transformation will effectively represent students' performance in the subject with a more compact and interpretable measure. This example is adapted from the resource *How to compute principal components* @eduflair2020.\n\n| Scores        | **Basic Stats** | **Advanced Stats** |\n|---------------|-----------------|--------------------|\n| **Student 1** | **4**           | **11**             |\n| **Student 2** | **8**           | **4**              |\n| **Student 3** | **13**          | **5**              |\n| **Student 4** | **7**           | **14**             |\n| **Mean**      | $\\bar{x}$=8     | $\\bar{y}$=8.5      |\n\n### Calculate the covariance matrix $M$\n\n$$\n\\begin{bmatrix}\n\\text{cov}(x,x) & \\text{cov}(x,y) \\\\\n\\text{cov}(y,x) & \\text{cov}(y,y) \\\\\n\\end{bmatrix}\n$$\n\n$\\Rightarrow$ $\\text{cov}(x,x)$ = $\\text{var}(x)$ = $\\mathbf{E}$($x^2$) - $\\mathbf{E}$ $(x)^2$ = $$ \\frac{(16+0+25+1)}{3}=14 $$\n\n$\\Rightarrow$ $\\text{cov}(y,y)$ = $\\text{var}(y)$ = $\\mathbf{E}$($y^2$) - $\\mathbf{E}$ $(y)^2$ = $$ \\frac{(6.25+20.25+12.25+30.25)}{3}=23 $$\n\n$\\Rightarrow$ $\\text{cov}(x,y)$ = $\\text{cov}(y,x)$ = $\\mathbf{E}$($xy$) - $\\mathbf{E}$ ($x$)$\\mathbf{E}$ ($y$) = $$ \\frac{(-10+0-17.5-5.5)}{3}=-11 $$\n\n$\\Rightarrow$ Covariance Matrix $M$ $$ \n\\begin{bmatrix}\n14 & -11 \\\\\n-11 & 23 \\\\\n\\end{bmatrix} $$\n\n### Compute the singular value decomposition (SVD)\n\nWe can obtain the principal components and loadings from SVD of the covariance matrix M since covariance matrix M is a square matrix:\n\n$$ \n\\begin{bmatrix}\n14 & -11 \\\\\n-11 & 23 \\\\\n\\end{bmatrix} \\times Any\\ vector = \\lambda \\times Any\\ vector\\ ,\\ (vector\\neq 0) \n$$\n\n$$det(M - \\lambda I) = 0$$\n\n#### Obtain eigenvalues of the covariance matrix $\\rightarrow$ $\\lambda_1$ & $\\lambda_2$\n\n$$\nI = \\begin{bmatrix}\n1 & 0 \\\\\n0 & 1 \\\\\n\\end{bmatrix}\n$$\n\n$$\n\\lambda I = \\lambda \\times \\begin{bmatrix}\n1 & 0 \\\\\n0 & 1 \\\\\n\\end{bmatrix} = \\begin{bmatrix} \n\\lambda  & 0 \\\\\n0 & \\lambda  \\\\\n\\end{bmatrix}\n$$\n\n$$\n\\begin{bmatrix}\n14 & -11 \\\\\n-11&  23 \\\\\n\\end{bmatrix} -\\begin{bmatrix}\n\\lambda  & 0 \\\\\n0 & \\lambda  \\\\\n\\end{bmatrix} = \\begin{bmatrix}\n14-\\lambda  & -11 \\\\\n-11 & 23-\\lambda  \\\\\n\\end{bmatrix}\n$$\n\n$\\Rightarrow$\n\n$$\ndet\\begin{bmatrix}\n 14-\\lambda & -11 \\\\\n -11& 23-\\lambda  \\\\\n\\end{bmatrix}=0\n$$\n\n$\\Rightarrow$ $$(14-\\lambda)(23-\\lambda) - (-11)(-11) = 0 $$\n\n$$\\lambda^2 +37\\lambda -201 = 0$$\n\n$\\Rightarrow$ $\\lambda_1$ = 30.3849, $\\lambda_2$ = 6.6152 (eigenvalues for Covariance Matrix $M$)\n\n#### Obtain eigenvector of $\\lambda_1$\n\n$$(M - \\lambda_1I)\\times U_1 = \\mathbf{0} $$\n\n$\\Rightarrow$ $$\\begin{bmatrix}\n 14-\\lambda & -11 \\\\\n -11& 23-\\lambda  \\\\\n\\end{bmatrix}\\times\\begin{bmatrix}\nu_1 \\\\\nu_2 \\\\\n\\end{bmatrix}=\\mathbf{0}\n$$\n\n$$(14- \\lambda)u_1 - 11u_2 = 0 $$ $\\Rightarrow$ $$-16.3849u_1 -11u_2 = 0$$ $\\Rightarrow$ $$-16.3849u_1= 11u_2 $$ $\\Rightarrow$ $$u_1 = \\frac{11}{-16.3849}u_2$$\n\n$$\n\\begin{bmatrix}\nu_1 \\\\\nu_2 \\\\\n\\end{bmatrix}= u_2\\begin{bmatrix}\n\\frac{11}{-16.3849}\\\\\n1 \\\\\n\\end{bmatrix}\n$$\n\n$\\Rightarrow$ $$u_2\\begin{bmatrix}\n-11\\\\ 16.3849\n\\end{bmatrix}$$\n\n$$\n\\begin{bmatrix}\n16.3849 & 11 \\\\\n\\end{bmatrix} \\times \\begin{bmatrix}\nu_1 \\\\ u_2\n\\end{bmatrix} = \\begin{bmatrix}\n11 \\\\ -16.3849\n\\end{bmatrix}\n$$\n\n**Normalized Eigenvector**\n\n$\\Rightarrow$ $\\lambda_1$: $e_1$\n\n$$\n\\frac{1}{\\sqrt{{11^2 +16.3849^2}}}\\begin{bmatrix}\n11 \\\\ -16.3849\n\\end{bmatrix}= \\begin{bmatrix}\n0.5574 \\\\ -0.8303\n\\end{bmatrix} \n$$\n\n$\\Rightarrow$ $\\lambda_2$ $e_2$ (Right singular vector) =\n\n$$\n\\begin{bmatrix}\n0.8303 \\\\ 0.5574\n\\end{bmatrix} \n$$\n\n|               | **Basic Stats** | **Advanced Stats** |\n|---------------|-----------------|--------------------|\n| **Student 1** | **4**           | **11**             |\n| **Student 2** | **8**           | **4**              |\n| **Student 3** | **13**          | **5**              |\n| **Student 4** | **7**           | **14**             |\n| **Mean**      | $\\bar{x}$=8     | $\\bar{y}$=8.5      |\n\n### Derive The new dataset\n\n**First Principal Component (PC1)**\n\n$$\nP_{11} = e_1^T \\times \n\\begin{bmatrix}\n4-mean(x) \\\\ 11 -mean(y)\n\\end{bmatrix}\n = \\begin{bmatrix}\n0.5574 & -0.8303 \\\\\n\\end{bmatrix} \n\\begin{bmatrix}\n4-8 \\\\ 11-8.5\n\\end{bmatrix} =-4.3052 \n$$\n\n$$\nP_{12} = e_1^T \\times \n\\begin{bmatrix}\n8-mean(x) \\\\ 4 -mean(y)\n\\end{bmatrix}\n = \\begin{bmatrix}\n0.5574 & -0.8303 \\\\\n\\end{bmatrix} \n\\begin{bmatrix}\n8-8 \\\\ 4-8.5\n\\end{bmatrix} =3.7361 \n$$\n\n$$\nP_{13} = e_1^T \\times \n\\begin{bmatrix}\n13-mean(x) \\\\ 5 -mean(y)\n\\end{bmatrix}\n = \\begin{bmatrix}\n0.5574 & -0.8303 \\\\\n\\end{bmatrix} \n\\begin{bmatrix}\n13-8 \\\\ 5-8.5\n\\end{bmatrix}= 5.6928\n$$\n\n$$\nP_{14}= e_1^T \\times \n\\begin{bmatrix}\n7-mean(x) \\\\ 14 -mean(y)\n\\end{bmatrix}\n = \\begin{bmatrix}\n0.5574 & -0.8303 \\\\\n\\end{bmatrix} \n\\begin{bmatrix}\n7-8 \\\\ 14-8.5\n\\end{bmatrix} = -5.1238\n$$\n\n<!-- -->\n\n**The new dataset (Left singular vector)**\n\n| **First Principal Component (PC1)** | **Student 1** | **Student 2** | **Student 3** | **Student 4** |\n|:-----------------------------------:|:-------------:|:-------------:|:-------------:|:-------------:|\n|                                     |  **-4.3052**  |  **3.7361**   |  **5.6928**   |  **-5.1238**  |\n\n### Verifying and visualizing the results\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n'data.frame':\t4 obs. of  3 variables:\n $ Student      : int  1 2 3 4\n $ BasicStats   : num  4 8 13 7\n $ AdvancedStats: num  11 4 5 14\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nImportance of components:\n                          PC1    PC2\nStandard deviation     1.2700 0.6221\nProportion of Variance 0.8065 0.1935\nCumulative Proportion  0.8065 1.0000\n```\n:::\n\n::: {.cell-output-display}\n![](examples_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](examples_files/figure-html/unnamed-chunk-1-2.png){width=672}\n:::\n:::\n\n\n### Summary\n\nThe first principal component of the data captures 80.7% of the variation (or information) while reducing the dimensionality of the dataset from 2 variables to 1. Small datasets such as this make the hand-calculation of principal components feasible and easy to follow, but the strengths of PCA are especially evident when software is used to enable principal component analysis for large datasets. In the next example, we demonstrate how PCA can be used with a large dataset using the R programming language.\n\n## PCA on a large dataset using R\n\nFor this application of PCA, the Abalone dataset from the UCI Machine Learning Repository is used @misc_abalone_1. This dataset contain 4177 observations of 9 variables which record characteristics of each abalone including sex, length, diameter, height, weights, and the number of rings. The variables, apart from sex, are continuous and correlated making the dataset an ideal candidate for demonstrating dimensionality reduction via PCA.\n\n### Libraries\n\nFirst, the appropriate and necessary libraries are loaded in R. These provide the functions which serve as the backbone of the analysis, handling the computational aspects of PCA as well as visualizing the results.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load necessary libraries\nlibrary(tidyverse)  # for handling missing values\nlibrary(corrplot) # for plotting the correlation matrix\nlibrary(factoextra) # PCA plots\n```\n:::\n\n\n### Data Preparation\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# clear environment\nrm(list = ls())\n\n# Load dataset\nabalone <- read.csv('./abalone/abalone.csv')\n\n# Dataset structure\nstr(abalone)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'data.frame':\t4177 obs. of  9 variables:\n $ Sex           : chr  \"M\" \"M\" \"F\" \"M\" ...\n $ Length        : num  0.455 0.35 0.53 0.44 0.33 0.425 0.53 0.545 0.475 0.55 ...\n $ Diameter      : num  0.365 0.265 0.42 0.365 0.255 0.3 0.415 0.425 0.37 0.44 ...\n $ Height        : num  0.095 0.09 0.135 0.125 0.08 0.095 0.15 0.125 0.125 0.15 ...\n $ Whole_weight  : num  0.514 0.226 0.677 0.516 0.205 ...\n $ Shucked_weight: num  0.2245 0.0995 0.2565 0.2155 0.0895 ...\n $ Viscera_weight: num  0.101 0.0485 0.1415 0.114 0.0395 ...\n $ Shell_weight  : num  0.15 0.07 0.21 0.155 0.055 0.12 0.33 0.26 0.165 0.32 ...\n $ Rings         : int  15 7 9 10 7 8 20 16 9 19 ...\n```\n:::\n\n```{.r .cell-code}\n# Missing values\ncolSums(is.na(abalone))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           Sex         Length       Diameter         Height   Whole_weight \n             0              0              0              0              0 \nShucked_weight Viscera_weight   Shell_weight          Rings \n             0              0              0              0 \n```\n:::\n:::\n\n\nThe dataset contains 9 variables with 1 categorical variable and 8 numeric variables. The dataset contains no missing values. For this example in applying principal component analysis, we exclude the categorical variable 'Sex' and focus the PCA on the numerical dimensions of the Abalone. For analyses involving a mix of numeric and non-numeric variables other factor analysis techniques can be used, such as factor analysis of mixed data @pages2014multiple.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Select only the numeric variables \nabalone <- abalone %>% select(where(is.numeric))\n\nsummary(abalone)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     Length         Diameter          Height        Whole_weight   \n Min.   :0.075   Min.   :0.0550   Min.   :0.0000   Min.   :0.0020  \n 1st Qu.:0.450   1st Qu.:0.3500   1st Qu.:0.1150   1st Qu.:0.4415  \n Median :0.545   Median :0.4250   Median :0.1400   Median :0.7995  \n Mean   :0.524   Mean   :0.4079   Mean   :0.1395   Mean   :0.8287  \n 3rd Qu.:0.615   3rd Qu.:0.4800   3rd Qu.:0.1650   3rd Qu.:1.1530  \n Max.   :0.815   Max.   :0.6500   Max.   :1.1300   Max.   :2.8255  \n Shucked_weight   Viscera_weight    Shell_weight        Rings       \n Min.   :0.0010   Min.   :0.0005   Min.   :0.0015   Min.   : 1.000  \n 1st Qu.:0.1860   1st Qu.:0.0935   1st Qu.:0.1300   1st Qu.: 8.000  \n Median :0.3360   Median :0.1710   Median :0.2340   Median : 9.000  \n Mean   :0.3594   Mean   :0.1806   Mean   :0.2388   Mean   : 9.934  \n 3rd Qu.:0.5020   3rd Qu.:0.2530   3rd Qu.:0.3290   3rd Qu.:11.000  \n Max.   :1.4880   Max.   :0.7600   Max.   :1.0050   Max.   :29.000  \n```\n:::\n:::\n\n\nThe summary statistics show the differences in measurement between variables, with some variables such as diameter and viscera weight having small ranges and others, namely rings, having relatively large ranges. For this reason, scaling of the variables is a crucial step in PCA to ensure results accurately capture the variance in the data.\n\n### Feature Scaling\n\nStandardization ensures all variables, also called features, are on the same scale, and the scale function allows us to center the data to a mean of 0 and variance of 1. This ensures no single feature has an outsized effect during the principal component analysis.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Standardization of numerical features\nabalone_sc <- scale(abalone, center = TRUE, scale = TRUE)\n\nsummary(abalone_sc)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     Length           Diameter           Height          Whole_weight     \n Min.   :-3.7387   Min.   :-3.5558   Min.   :-3.33555   Min.   :-1.68589  \n 1st Qu.:-0.6161   1st Qu.:-0.5832   1st Qu.:-0.58614   1st Qu.:-0.78966  \n Median : 0.1749   Median : 0.1725   Median : 0.01156   Median :-0.05963  \n Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.00000   Mean   : 0.00000  \n 3rd Qu.: 0.7578   3rd Qu.: 0.7267   3rd Qu.: 0.60926   3rd Qu.: 0.66123  \n Max.   : 2.4232   Max.   : 2.4397   Max.   :23.68045   Max.   : 4.07178  \n Shucked_weight    Viscera_weight      Shell_weight         Rings        \n Min.   :-1.6145   Min.   :-1.64298   Min.   :-1.7049   Min.   :-2.7708  \n 1st Qu.:-0.7811   1st Qu.:-0.79455   1st Qu.:-0.7818   1st Qu.:-0.5997  \n Median :-0.1053   Median :-0.08752   Median :-0.0347   Median :-0.2896  \n Mean   : 0.0000   Mean   : 0.00000   Mean   : 0.0000   Mean   : 0.0000  \n 3rd Qu.: 0.6426   3rd Qu.: 0.66056   3rd Qu.: 0.6478   3rd Qu.: 0.3307  \n Max.   : 5.0848   Max.   : 5.28587   Max.   : 5.5040   Max.   : 5.9136  \n```\n:::\n:::\n\n\nViewing the data after scaling and centering, values greater than 3 or less than -3 represent outliers more than 3 standard deviations from the mean. Based on the ranges of the variables, we should view a boxplot of the data to further investigate.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot a boxplot to visualize potential outliers\npar(mar=c(4, 8, 4, 4))\nboxplot(abalone_sc, col = \"steelblue\", main = \"Visualization of scaled and centered data\", horizontal = TRUE, las = 1)\n```\n\n::: {.cell-output-display}\n![](examples_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nAre there enough outliers to be a cause for concern? We can see how many lie outside of the third standard deviation of the data for each variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(colSums(abalone_sc > 3 | abalone_sc < -3))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        Length       Diameter         Height   Whole_weight Shucked_weight \n            15             13              5             19             37 \nViscera_weight   Shell_weight          Rings \n            22             27             62 \n```\n:::\n:::\n\n\nOf the 4177 observations, at most 62 in a single variable (rings) are outliers. The tolerance for outliers will differ depending on the investigation, but for our illustrative purposes this number is well within tolerance for principal component analysis.\n\nLastly, we can investigate the correlation among the variables. PCA is best used with linearly correlated data. If the data is not correlated, the results of PCA will be less meaningful.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate correlations and round to 2 digits\nabalone_corr <- cor(abalone_sc)\ncorrplot(abalone_corr, method=\"number\")\n```\n\n::: {.cell-output-display}\n![](examples_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nOur scaled and centered data has strong linear correlations and contains a relatively small number of outliers. We can now calculate the principal components of the dataset.\n\n### PCA via Singular Value Decomposition\n\nThe prcomp() function @prcomp2023ref performs principal component analysis on a dataset using the singular value decomposition method, which utilizes the covariance matrix of the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Apply PCA using prcomp()\nabalone_pca <- prcomp(abalone_sc)\nsummary(abalone_pca)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nImportance of components:\n                         PC1     PC2     PC3     PC4     PC5     PC6     PC7\nStandard deviation     2.591 0.83403 0.50837 0.40742 0.29146 0.25194 0.11267\nProportion of Variance 0.839 0.08695 0.03231 0.02075 0.01062 0.00793 0.00159\nCumulative Proportion  0.839 0.92601 0.95831 0.97906 0.98968 0.99761 0.99920\n                           PC8\nStandard deviation     0.07999\nProportion of Variance 0.00080\nCumulative Proportion  1.00000\n```\n:::\n:::\n\n\n#### PCA - Cumulative Variance and Number of Principal Components\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Principal Component scores vector\npc_scores <- abalone_pca$x\n\n# Std Deviation of Components\ncomponent_sdev <- abalone_pca$sdev\n\n# Eigenvector or Loadings\neigenvector <- abalone_pca$rotation\n\n# Mean of variables\ncomponent_mean <- abalone_pca$center \n\n# Scaling factor of Variables\ncomponent_scale <- abalone_pca$scale\n\n# Proportion of variance explained by each PC\nvariance_explained <- component_sdev^2 / sum(component_sdev^2)\n\n# Cumulative proportion of variance explained\ncumulative_variance_explained <- cumsum(variance_explained)\n\n# Retain components that explain a percentage of the variance\nnum_components <- which(cumulative_variance_explained >= 0.92)[1]\n\n# Select the desired number of principal components\nselected_pcs <- pc_scores[, 1:num_components]\n\n# Display cumulative variance\ncumulative_variance_explained\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.8390549 0.9260065 0.9583119 0.9790606 0.9896793 0.9976134 0.9992002\n[8] 1.0000000\n```\n:::\n:::\n\n\nThe first 2 principal components alone explain 92% of the variance in the data.\n\n#### Loading of First Two Components\n\nThe loading are the weights assigned to each variable for that particular principal component.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Access the loadings for the first two principal components\nloadings_first_two_components <- eigenvector[, 1:2]\n\n# Print the loadings for the first two principal components\nprint(\"Loadings for the first two principal components:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Loadings for the first two principal components:\"\n```\n:::\n\n```{.r .cell-code}\nprint(loadings_first_two_components)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                     PC1         PC2\nLength         0.3721385  0.06828270\nDiameter       0.3730941  0.04004804\nHeight         0.3400268 -0.07046315\nWhole_weight   0.3783075  0.13734619\nShucked_weight 0.3624545  0.29883992\nViscera_weight 0.3685578  0.17297852\nShell_weight   0.3707578 -0.04540040\nRings          0.2427128 -0.92120385\n```\n:::\n:::\n\n\n#### PCA - Elements\n\nThe values in **`abalone_pca$x`** are the coordinates of each observation in the new principal component space. These coordinates are the scores for each observation along each principal component. The eigenvectors of the covariance or correlation matrix of the data represent the directions of maximum variance in the dataset.\n\n### Visualization\n\n#### Scree Plot - Cumulative Variance Explained\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfviz_eig(abalone_pca, addlabels = TRUE)\n```\n\n::: {.cell-output-display}\n![](examples_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\nThe scree plot visualizes the variance captured by each PC. PC1 explains 83.9% of the variance, and PC2 explains 8.7% variance.\n\n#### Biplot\n\nThe correlation between a variable and a principal component is used as the coordinates of the variable on the PC, shown as dimensions on the biplot. Dim1 corresponds to PC1, and Dim2 to PC2. The representation of variables differs from the plot of the observations: The observations are represented by their projections, but the variables are represented by their correlations @abdi2010principal.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfviz_pca_biplot(abalone_pca, label = \"var\", alpha.ind = \"contrib\", col.var = \"blue\", repel = TRUE)\n```\n\n::: {.cell-output-display}\n![](examples_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n#### Variable Contribution\n\nTop variable contribution for the first two principal components.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Contributions of variables to PC1\npc2_contribution <- fviz_contrib(abalone_pca, choice = \"var\", axes = 1, top = 20)\n\n# Modify the theme to rotate X-axis labels to 90 degrees\npc2_contribution +\n  theme(\n    axis.text.x = element_text(angle = 0),\n    plot.title = element_text(hjust = 0)  # horizontal justification\n  ) +\n  coord_flip() +\n  labs(title = \"Contribution of Variables to PC1\",\n       y = \"Percentage Contribution\",\n       x = \"\",\n       caption = \"PC1 explains 83.9% of the variance\") +\n  scale_y_continuous(labels = scales::percent_format(scale = 1,\n                                                     accuracy = 1))\n```\n\n::: {.cell-output-display}\n![](examples_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Contributions of variables to PC2\npc2_contribution <- fviz_contrib(abalone_pca, choice = \"var\", axes = 2, top = 12)\n\n# Modify the theme to rotate X-axis labels to 90 degrees\npc2_contribution +\n  theme(\n    axis.text.x = element_text(angle = 0),\n    plot.title = element_text(hjust = 0)  # horizontal justification\n  ) +\n  coord_flip() +\n  labs(title = \"Contribution of Variables to PC2\",\n       y = \"Percentage Contribution\",\n       x = \"\",\n       caption = \"PC2 explains 8.7% of the variance\") +\n  scale_y_continuous(labels = scales::percent_format(scale = 1,\n                                                     accuracy = 1))\n```\n\n::: {.cell-output-display}\n![](examples_files/figure-html/unnamed-chunk-14-2.png){width=672}\n:::\n:::\n\n\n### Summary\n\nThe first principal component captures 83.9% of the variance in the data. This linear combination has relatively equal loadings for whole weight, diameter, length, shell weight, viscera weight, and shucked weight, with height and rings having lower loadings. The second principal component is mostly influenced by the variable rings which makes up over 80% of the contribution to PC2. The biplot is an effective visualization of how each variable contributes to PC1, or dimension 1 on the graph, and PC2, or dimension 2 on the graph. The length and direction of each vector represent the contribution of each variable to the principal components; whole weight and rings are the longest, representing the largest contributions to PC1 and PC2 respectively.\n\nPCA is primarily an exploratory tool, which allows us to visualize high-dimensional data in lower dimensions as shown above in the biplot and accompanying scree plot. These PCs can be used to explore data in other ways, such as looking for trends and patterns in the data or identifying clusters and outliers. In the formal analysis in the following chapter, the applications of PCA are further explored through the development of a regression model on the principal components of a dataset.\n",
    "supporting": [
      "examples_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}