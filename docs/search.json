[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "Home"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "In recent years, the exponential growth of large datasets with numerous variables have become increasingly prevalent across various fields [1], including but not limited to data analysis, image processing, genetics, finance, and signal processing. [2] Analyzing, processing, and visualizing these multivariate datasets pose significant challenges. Therefore, the significance of unsupervised learning algorithms comes into play, particularly in tasks like dimensionality reduction, feature extraction, and visualization of complex data sets. Unsupervised learning is a collection of algorithms to classify raw data. [3] Clustering, and density estimation methods, often serve as a crucial preliminary step in dimensionality reduction where the objective is to find patterns and correlations aiding in organizing the original data. The underlying structures and relationships within the data can inform the selection and application of dimensionality reduction techniques.\nMoreover, dimensionality reduction (DR) techniques aim to mitigate the challenge of extracting valuable insights from complex data by reducing the number of dimensions, decreasing computational complexity, eliminating irrelevant and redundant data, improving algorithm accuracy, and facilitating efficient data visualization. [4] Unsupervised learning and dimensionality reduction are indispensable tools in data analysis, and machine learning. While unsupervised learning aims to uncover patterns and correlations between raw data, dimensionality reduction simplifies data representation. One widely used DR technique is Principal Component Analysis (PCA). The objectives of the present research project are the comprehensive exploration of PCA, encompassing an introduction to its core concepts, a discussion of its purposes and functions as well as a demonstration of its application through step-by-step examples.\nPrincipal Component Analysis is best described as a dimension reduction technique for statistical data. PCA has its roots in the work of Karl Pearson in his 1901 paper “On Lines and Planes of Closest Fit to Systems of Points in Space” [5] with the later christening and formal development of the technique by Harold Hotelling in 1933. [6] Pearson initially conceptualized PCA as a geometric interpretation within statistics; subsequently, PCA gained recognition as a more suitable method than analysis of variance for modeling response data. [7] The aim of PCA is to reduce the dimensionality of a dataset without loss of information about the variability of the data. By creating principal components that act as analogues for variables, the statistical information of the dataset can be preserved and compressed into a more easily representable form. A common example of the application of PCA involves reducing an n-dimensional data set into 2 principal components which can be plotted on a graph representing the relationships among the original variables; for this reason, PCA is often described as an exploratory data analysis tool.\n\n1.0.1 Theoretical and mathematical foundations\nThe fundamental concept behind PCA is to explain the variability in a set of correlated variables with a smaller set of uncorrelated variables, thus mitigating issues such as multicollinearity. The geometric properties of PCs facilitate an intuitive interpretation of key features within complex multivariate datasets. [8] The first principal component represents the direction with the greatest variation in the original data, while subsequent components are uncorrelated with the previous ones. Each component can be interpreted as the direction that maximizes the variance of the original data when projecting new observations onto the components. [1] PCA aims to capture a significant proportion of the original variables’ variation in the first few components, offering a practical lower-dimensional summary. While other methods may involve weighted averages across related variables to reduce dimensions, PCA often achieves similar results with minimal loss of variance information. [9] The mathematical foundations of PCA center around data of p variables in n observations, represented by an n x p matrix X with the goal of finding a linear combination of the columns of X which maximize variance. By maximizing the variance in these linear combinations, we can capture the largest amount of statistical information possible among the dimensions of the dataset. As described by Jolliffe and Cadima [2] , this boils down to finding the eigenvectors (a) and the largest eigenvalues () of the covariance matrix S, where Xak are the linear combinations called the principal components. \nWith the development of software packages in computer languages such as R and Python, the computational burden of PCA for large datasets can be easily handled by software; PCA has become easy to use as part of any data processing pipeline. Abdi and Williams describe PCA as “probably the most popular multivariate statistical technique … used by almost all scientific disciplines.” [10] Abdi and Williams also emphasize that the goals of PCA should be extracting only the most important information from a dataset to both compress and simplify the dataset and provide a way to analyze the structure of the observations and variables more easily. Importantly, the authors also offer a geometric description of the principal components as orthogonal factors to the original axis of the dataset; another strength of PCA is the fact that the technique can be explained and expressed through several mathematical avenues. Finally, Abdi and Williams offer methods to evaluate the quality of the “PCA model” in reconstructing the original data matrix using the derived principal components. For example, calculating the residual sum of squares, or RESS, after rebuilding X provides a way to identify model accuracy by seeking a minimal RESS value from X and X. \nLever and Altman offer similar praises of PCA while echoing the cautions of prior authors as a powerful data exploration tool with clear limitations. [11] PCA is best when interesting patterns in data produce statistical variance which can be capture by the principal components, but the technique is far less effective when patterns in data are non-linear or non-orthogonal or when the maximization of variance fails to produce interesting clusters in the principal component space. Scaling is often necessary for PCA to ensure compatibility across variables with different scales and ranges. The original data is typically standardized to have a mean of 0 and a variance of 1. [8] PCA can be used improperly to produce results that obfuscate the actual statistical content of the dataset; scaling may influence the analysis with prior knowledge of the data, so the decision to scale the data and the scaling methodology should be considered carefully.\n\n\n1.0.2 Applications and extensions\n\n1.0.2.1 In Archeology, Neuroscience, and the Arts \nPCA has been utilized by many authors across numerous fields as an essential part of a data analysis pipeline. As an example of the application of PCA, Jolliffe and Cadima [2] present a dataset of 88 observations with 9 variables of measurement for fossilized mammal teeth including length in two dimensions, width in two dimensions, and more. With the R statistical language, finding and displaying the principal components of the dataset becomes trivial work where patterns can be identified graphically in a two-dimensional (PC1 x PC2) plot, versus the original scatterplot which would be displayed in nine-dimensional space. The development of statistical software has made it straightforward for practitioners to use PCA for data exploration and dimensionality reduction. Authors such as Maindonald and Braun [12] have produced publications which present techniques and step-by-step methodologies of using R to calculate principal components and graphically display the results. Their presentation includes code snippets which can be run in R software environments along with worked examples and a discussion of results, facilitating the use of the technique for practitioners of all experience levels.\nFelipe Gewers and their research team directly expound on Abdi and Williams’ description of PCA as “the most popular multivariate statistical technique [...]” in an analysis of publications which use PCA: Across twenty-three disciplines ranging from Neuroscience to the Arts, PCA is used to explore a dataset before analysis, used as part of a spectrum of statistical analysis tools prior to modeling and analysis, or used to preprocess and simplify data for direct analysis and modeling. [13] The authors also discuss the broad effectiveness of PCA in representing more than 50% of the variance in most datasets using only the first three principal components. They also highlight how differences in captured variance appear between standardization and non-standardization, and in different fields of study; PCA can be effective in an incredibly diverse array of data when applied appropriately. As an exploratory technique with a long tenure in the field of statistics and data analysis, PCA is tried and true in simplifying datasets and allowing practitioners to identify and explore the largest sources of statistical information present in their data.\nPrincipal Component Analysis is also implemented in Scikit Learn with randomized Singular Value Decomposition (SVD) for dimensionality reduction in data analysis, particularly for face recognition and similar high-dimensional data applications. For instance, with an image of 4096 dimensions (64x64 pixel gray scale images) PCA can transform the data into a lower 200 dimension format that still captures the essential information. [14] With randomized SVD it becomes computationally more efficient to approximate the singular vectors, which are then used to perform the transformation. This approach significantly reduces computation time and memory usage. Additionally, PCA decomposes a multivariate dataset into orthogonal components that explain the maximum amount of variance. It can center and optionally scale the input data before applying SVD. Scaling can be useful for downstream models, such as Support Vector Machines with the RBF kernel and K-Means clustering, which assume certain properties of the data distribution.\n\n\n1.0.2.2 Computer Vision and Pattern Recognition\nThe Eigenfaces concept has emerged as a groundbreaking approach to facial detection and recognition. The Eigenfaces algorithm employs PCA to extract essential facial features, reducing the dimensionality of face images while retaining crucial information. The Eigenfaces technique represents facial images in high-dimensional space while projecting the images onto a lower-dimensional space. Turk and Pentland demonstrated the effectiveness of Eigenfaces in recognizing faces under various conditions, including variations in lighting, facial expressions, and pose. [15] The potential implications of this research extend beyond face recognition, and have broader implications for image analysis, computer vision, and biometric systems. The paper Eigenfaces by Zhang and Turk provided an insightful exploration of the Eigenfaces method, considered as the first working technique in facial recognition. [16] Eigenfaces leverage the power of PCA to represent facial images compactly and efficiently, making it possible to recognize faces in various contexts.\nThe idea of principal components to represent human faces was developed by Sirovich and Kirby in 1987 and used by Turk and Pentland in 1991 for face detection and recognition. [15] The authors describe the mathematical principles behind PCA, and its adaptation for facial recognition, capturing the most prominent facial features while reducing the dimensionality of the data. Specifically, the eigenfaces are the principal components of a distribution of faces, or the eigenvectors of the covariance matrix of the set of face images, where an image with N pixels is considered a point (or vector) in N-dimensional space. Emphasis is placed on the versatility of Eigenfaces in handling variations in lighting, pose, and facial expressions, making it a robust tool for real-world applications.\nThis application of PCA represents one of many for a longstanding technique in the rapidly growing field of statistics and data analysis. The unsupervised learning algorithm plays a vital role in dimensionality reduction, feature extraction, and visualization of complex data sets. Understanding PCA is crucial for researchers, and students seeking to extract meaningful insights and patterns from high-dimensional data to simplify the decision-making process.\n\n\n\n\n[1] M. Ringnér, “What is principal component analysis?” Nature biotechnology, vol. 26, no. 3, pp. 303–304, 2008.\n\n\n[2] I. T. Jolliffe and J. Cadima, “Principal component analysis: A review and recent developments,” Philosophical transactions of the royal society A: Mathematical, Physical and Engineering Sciences, vol. 374, no. 2065, p. 20150202, 2016.\n\n\n[3] D. Esposito and F. Esposito, Introducing machine learning. Microsoft Press, 2020.\n\n\n[4] B. M. S. Hasan and A. M. Abdulazeez, “A review of principal component analysis algorithm for dimensionality reduction,” Journal of Soft Computing and Data Mining, vol. 2, no. 1, pp. 20–30, 2021.\n\n\n[5] K. Pearson, “LIII. On lines and planes of closest fit to systems of points in space,” The London, Edinburgh, and Dublin philosophical magazine and journal of science, vol. 2, no. 11, pp. 559–572, 1901.\n\n\n[6] H. Hotelling, “Analysis of a complex of statistical variables into principal components.” Journal of educational psychology, vol. 24, no. 6, p. 417, 1933.\n\n\n[7] R. A. Fisher and W. A. Mackenzie, “Studies in crop variation. II. The manurial response of different potato varieties,” The Journal of Agricultural Science, vol. 13, no. 3, pp. 311–320, 1923.\n\n\n[8] M. Greenacre, P. J. Groenen, T. Hastie, A. I. d’Enza, A. Markos, and E. Tuzhilina, “Principal component analysis,” Nature Reviews Methods Primers, vol. 2, no. 1, p. 100, 2022.\n\n\n[9] B. Everitt and T. Hothorn, An introduction to applied multivariate analysis with r. Springer Science & Business Media, 2011.\n\n\n[10] H. Abdi and L. Williams, “Principal component analysis. WIREs comp stat 2: 433–459.” 2010.\n\n\n[11] J. Lever, M. Krzywinski, and N. Altman, “Points of significance: Principal component analysis,” Nature methods, vol. 14, no. 7, pp. 641–643, 2017.\n\n\n[12] J. Maindonald and J. Braun, Data analysis and graphics using r: An example-based approach, vol. 10. Cambridge University Press, 2006.\n\n\n[13] F. L. Gewers et al., “Principal component analysis: A natural approach to data exploration,” ACM Computing Surveys (CSUR), vol. 54, no. 4, pp. 1–34, 2021.\n\n\n[14] F. Pedregosa et al., “Scikit-learn: Machine learning in python,” the Journal of machine Learning research, vol. 12, pp. 2825–2830, 2011.\n\n\n[15] M. Turk and A. Pentland, “Eigenfaces for recognition,” Journal of cognitive neuroscience, vol. 3, no. 1, pp. 71–86, 1991.\n\n\n[16] S. Zhang and M. Turk, “Eigenfaces,” Scholarpedia, vol. 3, no. 9, p. 4244, 2008."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "1  Summary",
    "section": "",
    "text": "In recent times, the utilization of large datasets has become widespread across numerous fields. To confront the challenges posed by these complex datasets with multiple variables, unsupervised learning algorithms, particularly Principal Component Analysis (PCA), assume a crucial role in various tasks such as dimensionality reduction, feature extraction, and data visualization. The roots of PCA can be traced back to Karl Pearson’s conceptualization in 1901, with subsequent formalization by Harold Hotelling in 1933. PCA’s primary objective is to reduce data dimensionality while preserving its inherent variability. It achieves this by generating principal components, which serve as representatives for variables, thereby simplifying data representation and enhancing exploratory data analysis. The underlying mathematical framework of PCA revolves around identifying eigenvectors and eigenvalues of the covariance matrix. With the development of software packages, PCA has become easily accessible for data analysis. However, PCA is most effective when data patterns result in statistical variance that can be captured by the principal components, making it a potent tool for data exploration but less suitable for non-linear or non-orthogonal data. Careful consideration must be given to proper data scaling. PCA finds application in diverse fields such as archaeology, neuroscience, and the arts, enabling practitioners to explore datasets, preprocess data, and streamline analysis. Moreover, it has found utility in computer vision for tasks like facial recognition, showcasing its prowess in reducing dimensionality while preserving crucial information. Overall, PCA is a versatile and widely used technique in statistics and data analysis, providing a valuable means to extract insights from complex datasets and simplify decision-making processes across diverse domains."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "7  References",
    "section": "",
    "text": "[1] M.\nRingnér, “What is principal component analysis?” Nature\nbiotechnology, vol. 26, no. 3, pp. 303–304, 2008.\n\n\n[2] I.\nT. Jolliffe and J. Cadima, “Principal component analysis: A review\nand recent developments,” Philosophical transactions of the\nroyal society A: Mathematical, Physical and Engineering Sciences,\nvol. 374, no. 2065, p. 20150202, 2016.\n\n\n[3] B.\nM. S. Hasan and A. M. Abdulazeez, “A review of principal component\nanalysis algorithm for dimensionality reduction,” Journal of\nSoft Computing and Data Mining, vol. 2, no. 1, pp. 20–30,\n2021.\n\n\n[4] B.\nEveritt and T. Hothorn, An introduction to applied multivariate\nanalysis with r. Springer Science & Business Media, 2011.\n\n\n[5] M.\nGreenacre, P. J. Groenen, T. Hastie, A. I. d’Enza, A. Markos, and E.\nTuzhilina, “Principal component analysis,” Nature\nReviews Methods Primers, vol. 2, no. 1, p. 100, 2022.\n\n\n[6] K.\nPearson, “LIII. On lines and planes of closest fit to systems of\npoints in space,” The London, Edinburgh, and Dublin\nphilosophical magazine and journal of science, vol. 2, no. 11, pp.\n559–572, 1901.\n\n\n[7] R.\nA. Fisher and W. A. Mackenzie, “Studies in crop variation. II. The\nmanurial response of different potato varieties,” The Journal\nof Agricultural Science, vol. 13, no. 3, pp. 311–320, 1923.\n\n\n[8] H.\nAbdi and L. Williams, “Principal component analysis. WIREs comp\nstat 2: 433–459.” 2010.\n\n\n[9] H.\nHotelling, “Analysis of a complex of statistical variables into\nprincipal components.” Journal of educational\npsychology, vol. 24, no. 6, p. 417, 1933.\n\n\n[10] D.\nEsposito and F. Esposito, Introducing machine learning.\nMicrosoft Press, 2020.\n\n\n[11] M.\nTurk and A. Pentland, “Eigenfaces for recognition,”\nJournal of cognitive neuroscience, vol. 3, no. 1, pp. 71–86,\n1991.\n\n\n[12] S.\nZhang and M. Turk, “Eigenfaces,” Scholarpedia,\nvol. 3, no. 9, p. 4244, 2008.\n\n\n[13] F.\nPedregosa et al., “Scikit-learn: Machine learning in\npython,” the Journal of machine Learning research, vol.\n12, pp. 2825–2830, 2011.\n\n\n[14] J.\nMaindonald and J. Braun, Data analysis and graphics using r: An\nexample-based approach, vol. 10. Cambridge University Press,\n2006.\n\n\n[15] J.\nLever, M. Krzywinski, and N. Altman, “Points of significance:\nPrincipal component analysis,” Nature methods, vol. 14,\nno. 7, pp. 641–643, 2017.\n\n\n[16] F.\nL. Gewers et al., “Principal component analysis: A\nnatural approach to data exploration,” ACM Computing Surveys\n(CSUR), vol. 54, no. 4, pp. 1–34, 2021.\n\n\n[17] J.\nHopcroft and R. Kannan, Foundations of data science.\n2014.\n\n\n[18] “Quarterly dialysis facility care compare\n(QDFCC) report: July 2023.” Centers for Medicare & Medicaid\nServices (CMS). Available: https://data.cms.gov/provider-data/dataset/2fpu-cgbb.\n[Accessed: Oct. 11, 2023]\n\n\n[19] R\nCore Team, “Prcomp, a function of r: A language and environment\nfor statistical computing.” R Foundation for Statistical\nComputing, Vienna, Austria, 2023. Available: https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/prcomp.\n[Accessed: Oct. 16, 2023]\n\n\n[20] S.\nR. Bennett, “Linear algebra for data science.” 2021.\nAvailable: https://shainarace.github.io/LinearAlgebra/index.html.\n[Accessed: Oct. 16, 2023]\n\n\n[21] D.\nG. Luenberger, Optimization by vector space methods. John Wiley\n& Sons, 1997."
  },
  {
    "objectID": "intro.html#theoretical-and-mathematical-foundations",
    "href": "intro.html#theoretical-and-mathematical-foundations",
    "title": "2  Introduction",
    "section": "2.1 Theoretical and mathematical foundations",
    "text": "2.1 Theoretical and mathematical foundations\nThe fundamental concept behind PCA is to explain the variability in a set of correlated variables with a smaller set of uncorrelated variables, thus mitigating issues such as multicollinearity. The geometric properties of PCs facilitate an intuitive interpretation of key features within complex multivariate datasets. [8] The first principal component represents the direction with the greatest variation in the original data, while subsequent components are uncorrelated with the previous ones. Each component can be interpreted as the direction that maximizes the variance of the original data when projecting new observations onto the components. [1] PCA aims to capture a significant proportion of the original variables’ variation in the first few components, offering a practical lower-dimensional summary. While other methods may involve weighted averages across related variables to reduce dimensions, PCA often achieves similar results with minimal loss of variance information. [9] The mathematical foundations of PCA center around data of p variables in n observations, represented by an n x p matrix X with the goal of finding a linear combination of the columns of X which maximize variance. By maximizing the variance in these linear combinations, we can capture the largest amount of statistical information possible among the dimensions of the dataset. As described by Jolliffe and Cadima [2] , this boils down to finding the eigenvectors (a) and the largest eigenvalues () of the covariance matrix S, where Xak are the linear combinations called the principal components. \nWith the development of software packages in computer languages such as R and Python, the computational burden of PCA for large datasets can be easily handled by software; PCA has become easy to use as part of any data processing pipeline. Abdi and Williams describe PCA as “probably the most popular multivariate statistical technique … used by almost all scientific disciplines.” [10] Abdi and Williams also emphasize that the goals of PCA should be extracting only the most important information from a dataset to both compress and simplify the dataset and provide a way to analyze the structure of the observations and variables more easily. Importantly, the authors also offer a geometric description of the principal components as orthogonal factors to the original axis of the dataset; another strength of PCA is the fact that the technique can be explained and expressed through several mathematical avenues. Finally, Abdi and Williams offer methods to evaluate the quality of the “PCA model” in reconstructing the original data matrix using the derived principal components. For example, calculating the residual sum of squares, or RESS, after rebuilding X provides a way to identify model accuracy by seeking a minimal RESS value from X and X. \nLever and Altman offer similar praises of PCA while echoing the cautions of prior authors as a powerful data exploration tool with clear limitations. [11] PCA is best when interesting patterns in data produce statistical variance which can be capture by the principal components, but the technique is far less effective when patterns in data are non-linear or non-orthogonal or when the maximization of variance fails to produce interesting clusters in the principal component space. Scaling is often necessary for PCA to ensure compatibility across variables with different scales and ranges. The original data is typically standardized to have a mean of 0 and a variance of 1. [8] PCA can be used improperly to produce results that obfuscate the actual statistical content of the dataset; scaling may influence the analysis with prior knowledge of the data, so the decision to scale the data and the scaling methodology should be considered carefully."
  },
  {
    "objectID": "intro.html#applications-and-extensions",
    "href": "intro.html#applications-and-extensions",
    "title": "2  Introduction",
    "section": "2.2 Applications and extensions",
    "text": "2.2 Applications and extensions\n\n2.2.1 In Archeology, Neuroscience, and the Arts \nPCA has been utilized by many authors across numerous fields as an essential part of a data analysis pipeline. As an example of the application of PCA, Jolliffe and Cadima [2] present a dataset of 88 observations with 9 variables of measurement for fossilized mammal teeth including length in two dimensions, width in two dimensions, and more. With the R statistical language, finding and displaying the principal components of the dataset becomes trivial work where patterns can be identified graphically in a two-dimensional (PC1 x PC2) plot, versus the original scatterplot which would be displayed in nine-dimensional space. The development of statistical software has made it straightforward for practitioners to use PCA for data exploration and dimensionality reduction. Authors such as Maindonald and Braun [12] have produced publications which present techniques and step-by-step methodologies of using R to calculate principal components and graphically display the results. Their presentation includes code snippets which can be run in R software environments along with worked examples and a discussion of results, facilitating the use of the technique for practitioners of all experience levels.\nFelipe Gewers and their research team directly expound on Abdi and Williams’ description of PCA as “the most popular multivariate statistical technique [...]” in an analysis of publications which use PCA: Across twenty-three disciplines ranging from Neuroscience to the Arts, PCA is used to explore a dataset before analysis, used as part of a spectrum of statistical analysis tools prior to modeling and analysis, or used to preprocess and simplify data for direct analysis and modeling. [13] The authors also discuss the broad effectiveness of PCA in representing more than 50% of the variance in most datasets using only the first three principal components. They also highlight how differences in captured variance appear between standardization and non-standardization, and in different fields of study; PCA can be effective in an incredibly diverse array of data when applied appropriately. As an exploratory technique with a long tenure in the field of statistics and data analysis, PCA is tried and true in simplifying datasets and allowing practitioners to identify and explore the largest sources of statistical information present in their data.\nPrincipal Component Analysis is also implemented in Scikit Learn with randomized Singular Value Decomposition (SVD) for dimensionality reduction in data analysis, particularly for face recognition and similar high-dimensional data applications. For instance, with an image of 4096 dimensions (64x64 pixel gray scale images) PCA can transform the data into a lower 200 dimension format that still captures the essential information. [14] With randomized SVD it becomes computationally more efficient to approximate the singular vectors, which are then used to perform the transformation. This approach significantly reduces computation time and memory usage. Additionally, PCA decomposes a multivariate dataset into orthogonal components that explain the maximum amount of variance. It can center and optionally scale the input data before applying SVD. Scaling can be useful for downstream models, such as Support Vector Machines with the RBF kernel and K-Means clustering, which assume certain properties of the data distribution.\n\n\n2.2.2 Computer Vision and Pattern Recognition\nThe Eigenfaces concept has emerged as a groundbreaking approach to facial detection and recognition. The Eigenfaces algorithm employs PCA to extract essential facial features, reducing the dimensionality of face images while retaining crucial information. The Eigenfaces technique represents facial images in high-dimensional space while projecting the images onto a lower-dimensional space. Turk and Pentland demonstrated the effectiveness of Eigenfaces in recognizing faces under various conditions, including variations in lighting, facial expressions, and pose. [15] The potential implications of this research extend beyond face recognition, and have broader implications for image analysis, computer vision, and biometric systems. The paper Eigenfaces by Zhang and Turk provided an insightful exploration of the Eigenfaces method, considered as the first working technique in facial recognition. [16] Eigenfaces leverage the power of PCA to represent facial images compactly and efficiently, making it possible to recognize faces in various contexts.\nThe idea of principal components to represent human faces was developed by Sirovich and Kirby in 1987 and used by Turk and Pentland in 1991 for face detection and recognition. [15] The authors describe the mathematical principles behind PCA, and its adaptation for facial recognition, capturing the most prominent facial features while reducing the dimensionality of the data. Specifically, the eigenfaces are the principal components of a distribution of faces, or the eigenvectors of the covariance matrix of the set of face images, where an image with N pixels is considered a point (or vector) in N-dimensional space. Emphasis is placed on the versatility of Eigenfaces in handling variations in lighting, pose, and facial expressions, making it a robust tool for real-world applications.\nThis application of PCA represents one of many for a longstanding technique in the rapidly growing field of statistics and data analysis. The unsupervised learning algorithm plays a vital role in dimensionality reduction, feature extraction, and visualization of complex data sets. Understanding PCA is crucial for researchers, and students seeking to extract meaningful insights and patterns from high-dimensional data to simplify the decision-making process.\n\n\n\n\n[1] M. Ringnér, “What is principal component analysis?” Nature biotechnology, vol. 26, no. 3, pp. 303–304, 2008.\n\n\n[2] I. T. Jolliffe and J. Cadima, “Principal component analysis: A review and recent developments,” Philosophical transactions of the royal society A: Mathematical, Physical and Engineering Sciences, vol. 374, no. 2065, p. 20150202, 2016.\n\n\n[3] D. Esposito and F. Esposito, Introducing machine learning. Microsoft Press, 2020.\n\n\n[4] B. M. S. Hasan and A. M. Abdulazeez, “A review of principal component analysis algorithm for dimensionality reduction,” Journal of Soft Computing and Data Mining, vol. 2, no. 1, pp. 20–30, 2021.\n\n\n[5] K. Pearson, “LIII. On lines and planes of closest fit to systems of points in space,” The London, Edinburgh, and Dublin philosophical magazine and journal of science, vol. 2, no. 11, pp. 559–572, 1901.\n\n\n[6] H. Hotelling, “Analysis of a complex of statistical variables into principal components.” Journal of educational psychology, vol. 24, no. 6, p. 417, 1933.\n\n\n[7] R. A. Fisher and W. A. Mackenzie, “Studies in crop variation. II. The manurial response of different potato varieties,” The Journal of Agricultural Science, vol. 13, no. 3, pp. 311–320, 1923.\n\n\n[8] M. Greenacre, P. J. Groenen, T. Hastie, A. I. d’Enza, A. Markos, and E. Tuzhilina, “Principal component analysis,” Nature Reviews Methods Primers, vol. 2, no. 1, p. 100, 2022.\n\n\n[9] B. Everitt and T. Hothorn, An introduction to applied multivariate analysis with r. Springer Science & Business Media, 2011.\n\n\n[10] H. Abdi and L. Williams, “Principal component analysis. WIREs comp stat 2: 433–459.” 2010.\n\n\n[11] J. Lever, M. Krzywinski, and N. Altman, “Points of significance: Principal component analysis,” Nature methods, vol. 14, no. 7, pp. 641–643, 2017.\n\n\n[12] J. Maindonald and J. Braun, Data analysis and graphics using r: An example-based approach, vol. 10. Cambridge University Press, 2006.\n\n\n[13] F. L. Gewers et al., “Principal component analysis: A natural approach to data exploration,” ACM Computing Surveys (CSUR), vol. 54, no. 4, pp. 1–34, 2021.\n\n\n[14] F. Pedregosa et al., “Scikit-learn: Machine learning in python,” the Journal of machine Learning research, vol. 12, pp. 2825–2830, 2011.\n\n\n[15] M. Turk and A. Pentland, “Eigenfaces for recognition,” Journal of cognitive neuroscience, vol. 3, no. 1, pp. 71–86, 1991.\n\n\n[16] S. Zhang and M. Turk, “Eigenfaces,” Scholarpedia, vol. 3, no. 9, p. 4244, 2008."
  },
  {
    "objectID": "dataset.html",
    "href": "dataset.html",
    "title": "2  Dataset",
    "section": "",
    "text": "This is where we can put everything about our dataset description and visualization."
  },
  {
    "objectID": "EDA-draft.html#visual-report",
    "href": "EDA-draft.html#visual-report",
    "title": "5  EDA",
    "section": "5.1 Visual Report",
    "text": "5.1 Visual Report"
  },
  {
    "objectID": "EDA.html#visual-report",
    "href": "EDA.html#visual-report",
    "title": "3  EDA",
    "section": "3.1 Visual Report",
    "text": "3.1 Visual Report"
  },
  {
    "objectID": "dataset.html#report",
    "href": "dataset.html#report",
    "title": "2  Dataset",
    "section": "2.1 Report",
    "text": "2.1 Report"
  },
  {
    "objectID": "dataset.html#visualization-report",
    "href": "dataset.html#visualization-report",
    "title": "2  Dataset",
    "section": "2.2 Visualization Report",
    "text": "2.2 Visualization Report\n\n\n\n  \n\n\n\n\n  \n\n\n\n\n\n\n\n\n[1] “Quarterly dialysis facility care compare (QDFCC) report: July 2023.” Centers for Medicare & Medicaid Services (CMS). Available: https://data.cms.gov/provider-data/dataset/2fpu-cgbb. [Accessed: Oct. 11, 2023]"
  },
  {
    "objectID": "dataset.html#statistical-summary",
    "href": "dataset.html#statistical-summary",
    "title": "3  Dataset",
    "section": "3.2 Statistical Summary",
    "text": "3.2 Statistical Summary\nThe dataset contains 55 observations of 38 variables with 1 discrete variable (States/Territories) and 38 continuous variables. 13 observations have at least 1 missing record, with 34 missing observations in total. Most missing data in the dataset occurs in variables relating to pediatric patient data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbetter_\nfistula\nbetter_\nhospital_\nreadmission\nbetter_\nhospitalizat\nion\nbetter_\ninfection\nbetter_\nsurvival\nbetter_\ntransfusion\nexpected_\nfistula\nexpected_\nhospital_\nreadmission\nexpected_\nhospitalizat\nion\nexpected_\ninfection\nexpected_\nsurvival\nexpected_\ntransfusion\nHgb_10g\nHgb_12g\nhypercalcemi\na_calcium &gt;\n10.2Mg\nincident_\ntransplant_\nwaitlist_\nbetter\nincident_\ntransplant_\nwaitlist_\nexpected\nincident_\ntransplant_\nwaitlist_\nworse\nKt_v_1.2\nKt_v_1.7\nlong_term_\ncatheter\npediatric_\nKt_v_1.8\npediatric_\nnPCR\npedriatic_\nKt_v_1.2\nphosphorus (\n3.5 - 4.5)\nMg\nphosphorus (\n4.6 - 5.5)\nMg\nphosphorus (\n5.6 - 7) Mg\nphosphorus\n&lt; 3.5Mg\nphosphorus\n&gt; 7Mg\nprevalent_\ntransplant_\nwaitlist_\nbetter\nprevalent_\ntransplant_\nwaitlist_\nexpected\nprevalent_\ntransplant_\nwaitlist_\nworse\nworse_\nfistula\nworse_\nhospital_\nreadmission\nworse_\nhospitalizat\nion\nworse_\ninfection\nworse_\nsurvival\nworse_\ntransfusion\n\n\n\n\nMean\n4.93\n2.45\n1.25\n40.00\n3.48\n0.18\n116.57\n119.59\n122.96\n74.00\n118.93\n99.23\n21.47\n0.24\n2.40\n4.71\n61.02\n2.86\n96.40\n91.72\n17.02\n74.54\n91.02\n91.20\n23.13\n29.09\n23.71\n7.60\n16.45\n7.84\n120.91\n2.89\n5.25\n3.50\n4.93\n1.14\n4.12\n7.29\n\n\nStd.Dev\n9.44\n3.20\n2.23\n54.68\n5.13\n0.47\n137.77\n143.41\n144.86\n80.79\n141.58\n120.25\n7.87\n0.43\n4.33\n9.69\n84.74\n5.53\n1.87\n4.36\n3.92\n23.31\n14.85\n10.38\n2.41\n1.67\n1.59\n1.10\n3.06\n24.43\n144.05\n3.53\n8.24\n6.27\n10.76\n1.71\n6.08\n9.42\n\n\nMin\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n2.00\n1.00\n0.00\n2.00\n1.00\n9.00\n0.00\n1.00\n0.00\n0.00\n0.00\n87.00\n69.00\n10.00\n0.00\n15.00\n47.00\n11.00\n24.00\n20.00\n5.00\n8.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n\n\nQ1\n0.00\n0.00\n0.00\n5.50\n0.00\n0.00\n24.50\n22.50\n26.00\n13.00\n23.00\n21.50\n18.00\n0.00\n1.00\n0.00\n8.50\n0.00\n96.00\n90.00\n15.00\n64.00\n89.00\n86.00\n22.00\n28.00\n23.00\n7.00\n15.00\n0.00\n26.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n1.00\n\n\nMedian\n2.00\n1.50\n1.00\n22.00\n1.00\n0.00\n66.00\n68.50\n71.50\n47.50\n67.50\n53.00\n20.00\n0.00\n2.00\n1.00\n32.00\n1.00\n97.00\n93.00\n17.00\n80.50\n96.00\n93.00\n23.00\n29.00\n24.00\n8.00\n16.00\n1.00\n63.00\n2.00\n1.00\n1.00\n1.00\n0.00\n1.00\n3.00\n\n\nQ3\n5.50\n3.50\n2.00\n52.00\n4.50\n0.00\n158.50\n157.50\n169.50\n107.00\n161.00\n135.50\n23.00\n0.00\n2.00\n4.50\n80.00\n3.00\n97.00\n94.00\n18.00\n93.00\n100.00\n100.00\n24.00\n30.00\n24.00\n8.00\n18.00\n4.00\n168.00\n4.50\n7.00\n3.00\n4.50\n2.00\n5.00\n10.50\n\n\nMax\n50.00\n17.00\n15.00\n290.00\n24.00\n2.00\n659.00\n665.00\n675.00\n397.00\n657.00\n596.00\n60.00\n1.00\n33.00\n52.00\n420.00\n34.00\n99.00\n96.00\n29.00\n100.00\n100.00\n100.00\n27.00\n32.00\n29.00\n11.00\n26.00\n168.00\n714.00\n18.00\n40.00\n38.00\n67.00\n8.00\n24.00\n47.00\n\n\nMAD\n2.97\n2.22\n1.48\n26.69\n1.48\n0.00\n76.35\n80.80\n83.03\n54.86\n77.10\n62.27\n4.45\n0.00\n1.48\n1.48\n43.00\n1.48\n1.48\n2.97\n2.97\n20.02\n5.93\n10.38\n1.48\n1.48\n1.48\n1.48\n2.97\n1.48\n77.84\n2.97\n1.48\n1.48\n1.48\n0.00\n1.48\n4.45\n\n\nIQR\n5.25\n3.25\n2.00\n45.25\n4.25\n0.00\n132.50\n133.00\n142.25\n92.50\n136.50\n111.50\n5.00\n0.00\n1.00\n4.25\n70.25\n3.00\n1.00\n4.00\n3.00\n28.50\n10.50\n14.00\n2.00\n2.00\n1.00\n1.00\n3.00\n4.00\n139.00\n4.25\n6.50\n3.00\n4.25\n2.00\n5.00\n9.25\n\n\nCV\n1.92\n1.31\n1.78\n1.37\n1.47\n2.64\n1.18\n1.20\n1.18\n1.09\n1.19\n1.21\n0.37\n1.81\n1.80\n2.06\n1.39\n1.93\n0.02\n0.05\n0.23\n0.31\n0.16\n0.11\n0.10\n0.06\n0.07\n0.14\n0.19\n3.12\n1.19\n1.22\n1.57\n1.79\n2.18\n1.50\n1.48\n1.29\n\n\nSkewness\n3.36\n2.33\n4.35\n2.64\n2.26\n2.58\n2.10\n2.09\n2.11\n1.78\n2.14\n2.31\n2.74\n1.21\n6.42\n3.54\n2.73\n3.66\n-2.89\n-2.84\n1.04\n-1.27\n-3.16\n-1.85\n-2.29\n-0.66\n0.85\n0.33\n0.19\n5.27\n2.06\n1.90\n2.01\n3.38\n3.96\n1.80\n1.96\n2.22\n\n\nSE.Skewness\n0.32\n0.32\n0.32\n0.32\n0.32\n0.32\n0.32\n0.32\n0.32\n0.32\n0.32\n0.32\n0.32\n0.32\n0.32\n0.32\n0.32\n0.32\n0.32\n0.32\n0.32\n0.35\n0.33\n0.34\n0.32\n0.32\n0.32\n0.32\n0.32\n0.32\n0.32\n0.32\n0.32\n0.32\n0.32\n0.32\n0.32\n0.32\n\n\nKurtosis\n11.89\n6.70\n23.38\n7.99\n5.33\n5.98\n4.76\n4.73\n4.91\n3.58\n4.95\n6.09\n10.10\n-0.55\n42.31\n12.93\n8.09\n16.04\n11.35\n11.47\n1.57\n1.43\n11.98\n4.79\n9.58\n0.57\n1.26\n0.59\n1.71\n30.44\n4.66\n4.55\n4.18\n14.26\n18.22\n3.26\n3.11\n5.96\n\n\nN.Valid\n56\n56\n56\n56\n56\n56\n56\n56\n56\n56\n56\n56\n55\n55\n55\n56\n56\n56\n55\n54\n55\n46\n51\n49\n55\n55\n55\n55\n55\n56\n56\n56\n56\n56\n56\n56\n56\n56\n\n\nPct.Valid\n100.00\n100.00\n100.00\n100.00\n100.00\n100.00\n100.00\n100.00\n100.00\n100.00\n100.00\n100.00\n98.21\n98.21\n98.21\n100.00\n100.00\n100.00\n98.21\n96.43\n98.21\n82.14\n91.07\n87.50\n98.21\n98.21\n98.21\n98.21\n98.21\n100.00\n100.00\n100.00\n100.00\n100.00\n100.00\n100.00\n100.00\n100.00\n\n\n\n\nGenerated by summarytools 1.0.1 (R version 4.3.0)2023-10-31\n\n\n\n\nThe histograms of the data reveal that majority of the variables are skewed right, with the QQ plots supporting that very few of the variables are normally distributed. Finally, many of the variables are highly correlated as expected based on the design of the dataset."
  },
  {
    "objectID": "methods.html#assumptions",
    "href": "methods.html#assumptions",
    "title": "4  Methods",
    "section": "4.1 Assumptions",
    "text": "4.1 Assumptions\nFor PCA to be effective, the data should be continuous (although adaptations of PCA exist for other numeric data structures) and normally distributed, although the the distribution of the data does not truly matter when utilizing PCA as an exploratory methodology. More importantly, the data should be linearly related or the linear combinations of the principal components cannot meaningfully capture the variance of the data. Ideally, the variables should be similar in scale and free from extreme or missing values, although this can be addressed in preprocessing, and implementations of PCA such as robust PCA have been developed to address these challenges. [1]"
  },
  {
    "objectID": "methods.html#preprocessing",
    "href": "methods.html#preprocessing",
    "title": "4  Methods",
    "section": "4.2 Preprocessing",
    "text": "4.2 Preprocessing\nPreprocessing data for PCA is straightforward. Missing data should be handled using a method appropriate for the dataset, such as imputation based on the mean or median of the variable observations. After this the variables should be centered and scaled, to a mean of 0 and a standard deviation of 1, although statistical software libraries for SVD and PCA may include this as an option within the function. [2]"
  },
  {
    "objectID": "methods.html#pca",
    "href": "methods.html#pca",
    "title": "3  Methods (under construction)",
    "section": "3.3 PCA",
    "text": "3.3 PCA"
  },
  {
    "objectID": "methods.html#pca-proper-latex-formatting-coming-soon",
    "href": "methods.html#pca-proper-latex-formatting-coming-soon",
    "title": "3  Methods (under construction)",
    "section": "3.3 PCA (proper latex formatting coming soon)",
    "text": "3.3 PCA (proper latex formatting coming soon)\nLet our dataset be represented by the I x J matrix X comprised of I observations of J variables in the data set. Any element xij represents the ith observation of variable j in the dataset. The data in X is centered and scaled , such that the mean of each column Xj is 0 and every xij has been standardized with unit variance. We can represent this with the formula:\n\\[\nz_{ij} = \\frac{x_{ij} - \\bar{x}_{j}}{\\sqrt{\\sigma_{j}}}\n\\tag{3.1}\\]"
  },
  {
    "objectID": "methods.html#principal-component-analysis",
    "href": "methods.html#principal-component-analysis",
    "title": "4  Methods",
    "section": "4.4 Principal Component Analysis",
    "text": "4.4 Principal Component Analysis\nIn this approach to PCA, SVD is used to extract the most information (variance) from the data matrix while reducing the dimensionality of the data. The first principal component will have the largest possible variance (also called inertia), whose value is defined as a factor score. Factor scores represent a geometric projection of the observations onto the PCs. The second PC, orthogonal to the first, has the second largest variance, and the third PC would continue this pattern. The calculation of PCs via SVD can be understood with the use of matrix operations on a dataset. [4]\n\n4.4.1 Centering and Scaling\nLet our dataset be represented by the \\(N \\times P\\) matrix \\(X\\) comprised of \\(N\\) observations of \\(P\\) variables in the data set, where any element \\(x_{np}\\) represents the \\(n\\)th observation of variable \\(p\\) in the dataset. The matrix \\(X\\) has rank \\(A\\) where \\(A \\leq min\\{N, P\\}\\). The data in \\(X\\) is centered and scaled, such that the mean of each column \\(X_p\\) is 0 and every \\(x_{np}\\) has been standardized with unit variance. We can represent this with the formula:\n\\[\nz_{np} = \\frac{x_{np} - \\bar{x}_{p}}{\\sqrt{\\sigma_{p}}}\n\\tag{4.2}\\]\n\n\n4.4.2 Eigendecomposition of the Covariance Matrix\nThe aim of PCA is to find some linear combination of the columns of \\(X\\) which maximizes the variance. If we define \\(a\\) as a vector of constants \\(a_1, a_2, a_3, …, a_p\\), then \\(Xa\\) represents the linear combination of interest. The variance of \\(Xa\\) is represented by \\(var(Xa) = a^TSa\\), with the covariance matrix \\(S\\), and \\(T\\) representing the transpose operator. Finding the \\(Xa\\) with maximum variance equates to finding the vector \\(a\\) which maximizes the quadratic \\(a^TSa\\), where \\(a^Ta = 1\\). We can write this as \\(a^TSa - \\lambda(a^Ta-1)\\), with the Lagrange multiplier \\(\\lambda\\). [5] Equating this expression to the null vector \\(0\\) allows us to differentiate with respect to \\(a\\):\n\\[\nSa - \\lambda a = 0 \\Rightarrow Sa = \\lambda a\n\\tag{4.3}\\]\nTherefore, \\(a\\) is a unit-norm eigenvector with eigenvalue \\(\\lambda\\) of the covariance matrix \\(S\\). The largest eigenvalue of \\(S\\) is \\(\\lambda_1\\) with the eigenvector \\(a_1\\), which we can define for any eigenvector \\(a\\):\n\\[\nvar(Xa) = a^TSa = \\lambda a^Ta = \\lambda\n\\tag{4.4}\\]\nAny \\(p \\times p\\) real symmetric matrix has exactly \\(p\\) real eigenvalues \\(\\lambda_k\\) for \\(k = 1,...,p\\). The corresponding eigenvectors of these eigenvalues can be defined to form an orthonormal set of vectors such that \\(a_k^Ta_{k^T} = 1\\) if \\(k = k^T\\) and zero otherwise. If we consider that \\(S\\) is such a matrix and impose the restriction of orthogonality to the different coefficient vectors of \\(S\\), the full set of eigenvectors of \\(S\\) represent the solutions to finding linear combinations \\(Xa_k\\) which maximize variance while minimizing correlation with prior linear combinations. \\(Xa_k\\) then represent the linear combinations which are the principal components of the dataset with eigenvectors \\(a_k\\) and eigenvalues \\(\\lambda_k\\). The elements of \\(Xa_k\\) are the factor scores of the PCs, while the elements of the eigenvectors \\(a_k\\) represent the loadings of the PCs. [1]\n\n\n4.4.3 Singular Value Decomposition\nNext we define the singular value decomposition of \\(X\\). Let \\(L\\) be the \\(N \\times A\\) matrix of left singular vectors of the matrix; that is, the columns of \\(L\\) are made up of the eigenvectors of \\(XX^T\\). Let \\(R\\) be the \\(P \\times A\\) matrix of right singular vectors; the columns of \\(R\\) are made up of the eigenvectors of \\(X^TX\\). Finally, let \\(D\\) be the diagonal matrix of singular values, meaning the singular values in \\(D\\) are the square roots of the eigenvalues of \\(XX^T\\) and \\(X^TX\\), and \\(D^2\\) is defined as the diagonal matrix of the non-zero eigenvalues. We can define the singular value decomposition of matrix \\(X\\) as:\n\\[\nX = LD{R}^T\n\\tag{4.5}\\]\nIn this context, the eigenvalues represent the variances of the principal components and summarily contain the important information for the dataset, and we can obtain the PCs of \\(X\\) from the SVD. [6] With the identity matrix \\(I\\), the \\(I \\times R\\) matrix of factor scores can be expressed as:\n\\[\nF = LD\n\\tag{4.6}\\]\nThese factor scores are calculated from the coefficients of the linear combinations in matrix \\(R\\), which can be defined as a projection matrix of the original observations onto the PCs, i.e. the product of \\(X\\) and \\(R\\):\n\\[\nF = LD = LDR^TR = XR\n\\tag{4.7}\\]\nThe matrix \\(R\\) is also referred to as a loading matrix, and \\(X\\) is often described as the product of the factor score matrix and the loading matrix:\n\\[\nX = FR^T\n\\tag{4.8}\\]\nwith the decomposition of \\(F^TF = D^2\\) and \\(R^TR = I\\).\nThe loadings represent the weights of the original variables in the computation of the PCs; in other words, the correlation from -1 to 1 of each variable with the factor score.\nIn a geometric interpretation of PCA, the factor scores measure length on the Cartesian plane. This length represents the projection of the original observations onto the PCs from the origin at \\((0, 0)\\). This is especially useful as a visualization of higher dimension data in two dimensions by utilizing the first two PCs which capture the most variance in the original data. [7]"
  },
  {
    "objectID": "methods.html#principal-component-analysis-via-singular-value-decomposition",
    "href": "methods.html#principal-component-analysis-via-singular-value-decomposition",
    "title": "3  Methods",
    "section": "3.3 Principal Component Analysis via Singular Value Decomposition",
    "text": "3.3 Principal Component Analysis via Singular Value Decomposition\nIn this approach to PCA, SVD is used to extract the most information (variance) from the data matrix while reducing the dimensionality of the data. The first PC will have the largest possible variance (also called inertia), whose value is defined as a factor score. Factor scores represent a geometric projection of the observations onto the PCs. The calculation of PCs via SVD can be understood with the implementation of matrix operations on a dataset. [3]\nLet our dataset be represented by the \\(N \\times P\\) matrix \\(X\\) comprised of \\(N\\) observations of \\(P\\) variables in the data set, where any element \\(x_{np}\\) represents the \\(n\\)th observation of variable \\(p\\) in the dataset. The matrix \\(X\\) has rank \\(K\\) where \\(K \\leq min\\{N, P\\}\\). The data in \\(X\\) is centered and scaled, such that the mean of each column \\(X_p\\) is 0 and every \\(x_{np}\\) has been standardized with unit variance. We can represent this with the formula:\n\\[\nz_{np} = \\frac{x_{np} - \\bar{x}_{p}}{\\sqrt{\\sigma_{p}}}\n\\tag{3.1}\\]\nNext we define the singular value decomposition of \\(X\\). Let \\(L\\) be the \\(N \\times K\\) matrix of left singular vectors of the matrix; that is, the columns of \\(L\\) are made up of the eigenvectors of \\(XX^T\\). Let \\(R\\) be the \\(P \\times K\\) matrix of right singular vector; the columns of \\(R\\) are made up of the eigenvectors of \\(X^TX\\). Finally, let \\(D\\) be the diagonal matrix of singular values, meaning the singular values in \\(D\\) are the square roots of the eigenvalues of \\(XX^T\\) and \\(X^TX\\), and \\(D^2\\) is defined as the diagonal matrix of the non-zero eigenvalues. We can define the singular value decomposition of matrix \\(X\\) as:\n\\[\nX = LD{R}^T\n\\tag{3.2}\\]\nIn this context, the eigenvalues represent the variances of the principal components and summarily contain the important information for the dataset, and we can obtain the principal components of \\(X\\) from the SVD. [4] With the identity matrix \\(I\\), the \\(I \\times R\\) matrix of factor scores can be expressed as:\n\\[\nF = LD\n\\tag{3.3}\\]\nThese factor scores are calculated from the coefficients of the linear combinations in matrix \\(R\\), which can be defined as a projection matrix of the original observations onto the principal components, i.e. the product of \\(X\\) and \\(R\\):\n\\[\nF = LD = LDR^TR = XR\n\\tag{3.4}\\]\nThe matrix \\(R\\) is also referred to as a loading matrix, and \\(X\\) is often described as the product of the factor score matrix and the loading matrix:\n\\[\nX = FR^T\n\\tag{3.5}\\]\nwith the decomposition of \\(F^TF = D^2\\) and \\(R^TR = I\\).\nThe loadings represent the weights of the original variables in the computation of the principal components; in other words, the correlation from -1 to 1 of each variable with the factor score.\nIn a geometric interpretation of PCA, the factor scores measure length on the Cartesian plane. This length represents the projection of the original observations onto the PCs from the origin at \\((0, 0)\\). This is especially useful as a visualization of higher dimension data in two dimensions by utilizing the first two principal components which capture the most variance in the original data. [5]"
  },
  {
    "objectID": "methods.html#interpretation-of-the-prinicpal-components",
    "href": "methods.html#interpretation-of-the-prinicpal-components",
    "title": "3  Methods",
    "section": "3.4 Interpretation of the Prinicpal Components",
    "text": "3.4 Interpretation of the Prinicpal Components\nThere are several ways to interpret the PCs derived from the analysis. Since the eigenvalues represent the variance of the PCs, the proportion of the eigenvalues explain the proportion of variation in the dataset. Using a scree plot, these eigenvalues are plotted to show how much variation each PC explains. Another commonly used tool is a biplot, a combination of the plots of the factor scores (points) and the loadings (vectors) for two PCs (typically PC1 and PC2). The biplot is meant to visually capture the relationship between the original variables and the principal components. Clusters of points represent highly correlated variables, and vector lengths represent the variability captured in that direction on the principal component axis. While many methods and tools exist to interpret the results of PCA, the usefulness of each depends on the needs of the analysis. [6]\n\n\n\n\n[1] I. T. Jolliffe and J. Cadima, “Principal component analysis: A review and recent developments,” Philosophical transactions of the royal society A: Mathematical, Physical and Engineering Sciences, vol. 374, no. 2065, p. 20150202, 2016.\n\n\n[2] R Core Team, “Prcomp, a function of r: A language and environment for statistical computing.” R Foundation for Statistical Computing, Vienna, Austria, 2023. Available: https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/prcomp\n\n\n[3] J. Hopcroft and R. Kannan, Foundations of data science. 2014.\n\n\n[4] H. Abdi and L. Williams, “Principal component analysis. WIREs comp stat 2: 433–459.” 2010.\n\n\n[5] J. Lever, M. Krzywinski, and N. Altman, “Points of significance: Principal component analysis,” Nature methods, vol. 14, no. 7, pp. 641–643, 2017.\n\n\n[6] S. R. Bennett, Linear algebra for data science. 2021."
  },
  {
    "objectID": "methods.html#interpretation-of-the-principal-components",
    "href": "methods.html#interpretation-of-the-principal-components",
    "title": "4  Methods",
    "section": "4.5 Interpretation of the Principal Components",
    "text": "4.5 Interpretation of the Principal Components\nThere are several ways to interpret the PCs derived from the analysis. Since the eigenvalues represent the variance of the PCs, the proportion of the eigenvalues explain the proportion of variation in the dataset. Using a scree plot, these eigenvalues are plotted to show how much variation each PC explains. Another commonly used tool is a biplot, a combination of the plots of the factor scores (points) and the loadings (vectors) for two PCs (typically PC1 and PC2). The biplot is meant to visually capture the relationship between the original variables and the principal components. Clusters of points represent highly correlated variables, and vector lengths represent the variability captured in that direction on the principal component axis. While many methods and tools exist to interpret the results of PCA, the usefulness of each depends on the needs of the analysis. [3]"
  },
  {
    "objectID": "methods.html#eigenvectors",
    "href": "methods.html#eigenvectors",
    "title": "4  Methods",
    "section": "4.3 Eigenvectors",
    "text": "4.3 Eigenvectors\nPCA uses eigenvectors and their corresponding eigenvalues to calculate the principal components; a brief overview is given here. Eigen is a German word meaning inherent or characteristic, and an eigenvector can be described geometrically as a nonzero vector \\(a\\) of a linear transformation matrix \\(M\\) which does not change direction when the transformation is applied; the only change that occurs is a scaling by factor \\(\\lambda\\), the eigenvalue of the eigenvector \\(a\\). Such a characteristic vector is useful in PCA, where the goal is to maximize variance while reducing dimensionality, and in this context the eigenvectors and eigenvalues can be thought of as the inherent components of the dataset which contain the most important information. Eigenvalues can be calculated from the characteristic polynomial of the matrix, by taking the determinant of \\(M - \\lambda I\\), where \\(I\\) is the identity matrix. Setting this expression equal to zero allows the calculation of the eigenvalues as the roots of the characteristic polynomial; the resulting equation is called the characteristic equation:\n\\[\ndet(M - \\lambda I) = 0\n\\tag{4.1}\\]\nAn eigenvalue \\(\\lambda_k\\) can be used to solve for some eigenvector \\(a_k\\) with the equation \\((M - \\lambda I)a = 0\\). With PCA, we can use the eigenvectors of the covariance matrix to compute the PCs. [3]"
  },
  {
    "objectID": "dataset.html#renaming-variables",
    "href": "dataset.html#renaming-variables",
    "title": "3  Dataset",
    "section": "3.1 Renaming Variables",
    "text": "3.1 Renaming Variables\nIn our data preparation process, we have efficiently removed white spaces, and edited variable names, enhancing the readability and interpretability of the dataset. This meticulous effort adds to the overall clarity, making it quicker, and more meaningful for further examination."
  },
  {
    "objectID": "dataset.html#missing-values-detection",
    "href": "dataset.html#missing-values-detection",
    "title": "3  Dataset",
    "section": "3.3 Missing Values Detection",
    "text": "3.3 Missing Values Detection\n\nThe 34 missing observations represent 1.6% of the dataset.\n14 variables have missing observations.\nA table was generated to count missing values for all variables.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMissing_Values\n\n\n\n\nState\n0\n\n\nbetter_transfusion\n0\n\n\nexpected_transfusion\n0\n\n\nworse_transfusion\n0\n\n\nbetter_infection\n0\n\n\nexpected_infection\n0\n\n\nworse_infection\n0\n\n\nKt_v_1.2\n1\n\n\nKt_v_1.7\n2\n\n\npedriatic_Kt_v_1.2\n7\n\n\npediatric_Kt_v_1.8\n10\n\n\npediatric_nPCR\n5\n\n\nbetter_fistula\n0\n\n\nexpected_fistula\n0\n\n\nworse_fistula\n0\n\n\nlong_term_catheter\n1\n\n\nhypercalcemia_calcium &gt; 10.2Mg\n1\n\n\nphosphorus &lt; 3.5Mg\n1\n\n\nphosphorus (3.5 - 4.5) Mg\n1\n\n\nphosphorus (4.6 - 5.5) Mg\n1\n\n\nphosphorus (5.6 - 7) Mg\n1\n\n\nphosphorus &gt; 7Mg\n1\n\n\nbetter_hospitalization\n0\n\n\nexpected_hospitalization\n0\n\n\nworse_hospitalization\n0\n\n\nbetter_hospital_readmission\n0\n\n\nexpected_hospital_readmission\n0\n\n\nworse_hospital_readmission\n0\n\n\nbetter_survival\n0\n\n\nexpected_survival\n0\n\n\nworse_survival\n0\n\n\nincident_transplant_waitlist_better\n0\n\n\nincident_transplant_waitlist_expected\n0\n\n\nincident_transplant_waitlist_worse\n0\n\n\nprevalent_transplant_waitlist_better\n0\n\n\nprevalent_transplant_waitlist_expected\n0\n\n\nprevalent_transplant_waitlist_worse\n0\n\n\nHgb_10g\n1\n\n\nHgb_12g\n1"
  },
  {
    "objectID": "dataset.html#data-distribution",
    "href": "dataset.html#data-distribution",
    "title": "3  Dataset",
    "section": "3.4 Data Distribution",
    "text": "3.4 Data Distribution\n\nHistograms were used to display a sample (8 variables) of the distribution in respect to the predictor variable.\nNormality is not assumed. The majority of the observations in each variable do not meet the normality assumption."
  },
  {
    "objectID": "dataset.html#normal-qq-plot-of-residuals",
    "href": "dataset.html#normal-qq-plot-of-residuals",
    "title": "3  Dataset",
    "section": "3.5 Normal QQ Plot of Residuals",
    "text": "3.5 Normal QQ Plot of Residuals\n\nIt is apparent that the variables have heavy left and right tails.\nThe presence of outliers is consistent though the entire dataset."
  },
  {
    "objectID": "dataset.html#imputation-of-missing-values",
    "href": "dataset.html#imputation-of-missing-values",
    "title": "3  Dataset",
    "section": "3.6 Imputation of Missing Values",
    "text": "3.6 Imputation of Missing Values\n\nAfter substituting missing values with the mean, we are now able to proceed with standardizing the dataset.\nThe standardization process will be implemented in the Analysis chapter.\nA table was created to confirm the absence of any missing observations in the data frame.\nDue to its categorical data type, the “State” variable was omitted from the analysis.\n\n\n\n\n\n\n\nMissing_Values\nType\n\n\n\n\nbetter_transfusion\n0\ndouble\n\n\nexpected_transfusion\n0\ndouble\n\n\nworse_transfusion\n0\ndouble\n\n\nbetter_infection\n0\ndouble\n\n\nexpected_infection\n0\ndouble\n\n\nworse_infection\n0\ndouble\n\n\nKt_v_1.2\n0\ndouble\n\n\nKt_v_1.7\n0\ndouble\n\n\npedriatic_Kt_v_1.2\n0\ndouble\n\n\npediatric_Kt_v_1.8\n0\ndouble\n\n\npediatric_nPCR\n0\ndouble\n\n\nbetter_fistula\n0\ndouble\n\n\nexpected_fistula\n0\ndouble\n\n\nworse_fistula\n0\ndouble\n\n\nlong_term_catheter\n0\ndouble\n\n\nhypercalcemia_calcium &gt; 10.2Mg\n0\ndouble\n\n\nphosphorus &lt; 3.5Mg\n0\ndouble\n\n\nphosphorus (3.5 - 4.5) Mg\n0\ndouble\n\n\nphosphorus (4.6 - 5.5) Mg\n0\ndouble\n\n\nphosphorus (5.6 - 7) Mg\n0\ndouble\n\n\nphosphorus &gt; 7Mg\n0\ndouble\n\n\nbetter_hospitalization\n0\ndouble\n\n\nexpected_hospitalization\n0\ndouble\n\n\nworse_hospitalization\n0\ndouble\n\n\nbetter_hospital_readmission\n0\ndouble\n\n\nexpected_hospital_readmission\n0\ndouble\n\n\nworse_hospital_readmission\n0\ndouble\n\n\nbetter_survival\n0\ndouble\n\n\nexpected_survival\n0\ndouble\n\n\nworse_survival\n0\ndouble\n\n\nincident_transplant_waitlist_better\n0\ndouble\n\n\nincident_transplant_waitlist_expected\n0\ndouble\n\n\nincident_transplant_waitlist_worse\n0\ndouble\n\n\nprevalent_transplant_waitlist_better\n0\ndouble\n\n\nprevalent_transplant_waitlist_expected\n0\ndouble\n\n\nprevalent_transplant_waitlist_worse\n0\ndouble\n\n\nHgb_10g\n0\ndouble\n\n\nHgb_12g\n0\ndouble\n\n\n\n\n\n\n\nIn conclusion, the dataset has undergone essential pre-processing steps, rendering it well-prepared for outliers detection and normalization, which are crucial for robust and accurate model development. Missing observations have been imputed using the mean, ensuring that all variables within the dataset are now numerical. These preliminary steps have not only enhanced the dataset completeness, but also set the stage for further advanced data analysis.\n\n\n\n\n[1] “Quarterly dialysis facility care compare (QDFCC) report: July 2023.” Centers for Medicare & Medicaid Services (CMS). Available: https://data.cms.gov/provider-data/dataset/2fpu-cgbb. [Accessed: Oct. 11, 2023]"
  },
  {
    "objectID": "methods.html#step-by-step-calculation-of-principal-component-analysis",
    "href": "methods.html#step-by-step-calculation-of-principal-component-analysis",
    "title": "4  Methods",
    "section": "4.6 Step-by-Step Calculation of Principal Component Analysis",
    "text": "4.6 Step-by-Step Calculation of Principal Component Analysis\nExample: In this illustration, we have access to the two grades of four students in a statistics subject. We aim to employ Principal Component Analysis (PCA) as a means to reduce the dimensionality from two variables to a singular variable. This transformation will effectively represent students’ performance in the subject with a more compact and interpretable measure. The example is adopted from “PCA problem” posted by EduFlair KTU CS on Youtube.com.\n\n\n\nScores\nBasic Stats\nAdvanced Stats\n\n\n\n\nStudent 1\n4\n11\n\n\nStudent 2\n8\n4\n\n\nStudent 3\n13\n5\n\n\nStudent 4\n7\n14\n\n\nMean\n\\(\\bar{x}\\)=8\n\\(\\bar{y}\\)=8.5\n\n\n\nCalculate the Covariance Matrix \\(M\\)\n\\[\n\\begin{bmatrix}\n\\text{cov}(x,x) & \\text{cov}(x,y) \\\\\n\\text{cov}(y,x) & \\text{cov}(y,y) \\\\\n\\end{bmatrix}\n\\]\n\\(\\Rightarrow\\) \\(\\text{cov}(x,x)\\) = \\(\\text{var}(x)\\) = \\(\\mathbf{E}\\)(\\(x^2\\)) - \\(\\mathbf{E}\\) \\((x)^2\\) = \\[ \\frac{(16+0+25+1)}{3}=14 \\]\n\\(\\Rightarrow\\) \\(\\text{cov}(y,y)\\) = \\(\\text{var}(y)\\) = \\(\\mathbf{E}\\)(\\(y^2\\)) - \\(\\mathbf{E}\\) \\((y)^2\\) = \\[ \\frac{(6.25+20.25+12.25+30.25)}{3}=23 \\]\n\\(\\Rightarrow\\) \\(\\text{cov}(x,y)\\) = \\(\\text{cov}(y,x)\\) = \\(\\mathbf{E}\\)(\\(xy\\)) - \\(\\mathbf{E}\\) (\\(x\\))\\(\\mathbf{E}\\) (\\(y\\)) = \\[ \\frac{(-10+0-17.5-5.5)}{3}=-11 \\]\n\\(\\Rightarrow\\) Covariance Matrix \\(M\\) \\[\n\\begin{bmatrix}\n14 & -11 \\\\\n-11 & 23 \\\\\n\\end{bmatrix}\n\\]\nCompute the singular value decomposition (SVD): Calculate Eigenvalues and Eigenvectors of the Covariance Matrix \\(M\\)\nWe can obtain the principal component and loadings from SVD of the covariance matrix M since covariance matrix M is a square matrix:\n\\[\n\\begin{bmatrix}\n14 & -11 \\\\\n-11 & 23 \\\\\n\\end{bmatrix} \\times Any\\ vector = \\lambda \\times Any\\ vector\\ ,\\ (vector\\neq 0)\n\\]\n\\[det(M - \\lambda I) = 0\\]\nObtain Eigenvalues of the Covariance Matrix \\(\\rightarrow\\) \\(\\lambda_1\\) & \\(\\lambda_2\\)\nIdentity Matrix \\(I\\) =\n\\[\n\\begin{bmatrix}\n1 & 0 \\\\\n0 & 1 \\\\\n\\end{bmatrix}\n\\]\n\\(\\lambda I\\) = \\[ \\lambda \\times \\begin{bmatrix}\n1 & 0 \\\\\n0 & 1 \\\\\n\\end{bmatrix} = \\begin{bmatrix}\n\\lambda  & 0 \\\\\n0 & \\lambda  \\\\\n\\end{bmatrix}\n\\]\n\\[\n\\begin{bmatrix}\n14 & -11 \\\\\n-11&  23 \\\\\n\\end{bmatrix} -\\begin{bmatrix}\n\\lambda  & 0 \\\\\n0 & \\lambda  \\\\\n\\end{bmatrix} = \\begin{bmatrix}\n14-\\lambda  & -11 \\\\\n-11 & 23-\\lambda  \\\\\n\\end{bmatrix}\n\\]\n\\(\\Rightarrow\\)\n\\[\ndet\\begin{bmatrix}\n14-\\lambda & -11 \\\\\n-11& 23-\\lambda  \\\\\n\\end{bmatrix}=0\n\\]\n\\(\\Rightarrow\\) \\[(14-\\lambda)(23-\\lambda) - (-11)(-11) = 0 \\]\n\\[\\lambda^2 +37\\lambda -201 = 0\\]\n\\(\\Rightarrow\\) \\(\\lambda_1\\) = 30.3849, \\(\\lambda_2\\) = 6.6152 (eigenvalues for Covariance Matrix \\(M\\))\nObtain Eigenvector of \\(\\lambda_1\\)\n\\[(M - \\lambda_1I)\\times U_1 = \\mathbf{0} \\]\n\\(\\Rightarrow\\) \\[\\begin{bmatrix}\n14-\\lambda & -11 \\\\\n-11& 23-\\lambda  \\\\\n\\end{bmatrix}\\times\\begin{bmatrix}\nu_1 \\\\\nu_2 \\\\\n\\end{bmatrix}=\\mathbf{0}\n\\]\n\\[(14- \\lambda)u_1 - 11u_2 = 0 \\] \\(\\Rightarrow\\) \\[-16.3849u_1 -11u_2 = 0\\] \\(\\Rightarrow\\) \\[-16.3849u_1= 11u_2 \\] \\(u_1 = \\frac{11}{-16.3849}u_2\\)\n\\[\n\\begin{bmatrix}\nu_1 \\\\\nu_2 \\\\\n\\end{bmatrix}= u_2\\begin{bmatrix}\n\\frac{11}{-16.3849}\\\\\n1 \\\\\n\\end{bmatrix}\n\\]\n\\(\\Rightarrow\\) \\[u_2\\begin{bmatrix}\n-11\\\\ 16.3849\n\\end{bmatrix}\\]\n\\[\n\\begin{bmatrix}\n16.3849 & 11 \\\\\n\\end{bmatrix} \\times \\begin{bmatrix}\nu_1 \\\\ u_2\n\\end{bmatrix} = \\begin{bmatrix}\n11 \\\\ -16.3849\n\\end{bmatrix}\n\\]\nNormalized Eigenvector\n\n\\(\\Rightarrow\\) \\(\\lambda_1\\): \\(e_1\\)\n\\[\n\\frac{1}{\\sqrt{{11^2 +16.3849^2}}}\\begin{bmatrix}\n11 \\\\ -16.3849\n\\end{bmatrix}= \\begin{bmatrix}\n0.5574 \\\\ -0.8303\n\\end{bmatrix}\n\\]\n\\(\\Rightarrow\\) \\(\\lambda_2\\) \\(e_2\\) (Right singular vector) =\n\\[\n\\begin{bmatrix}\n0.8303 \\\\ 0.5574\n\\end{bmatrix}\n\\]\n\n\n\n\nBasic Stats\nAdvanced Stats\n\n\n\n\nStudent 1\n4\n11\n\n\nStudent 2\n8\n4\n\n\nStudent 3\n13\n5\n\n\nStudent 4\n7\n14\n\n\nMean\n\\(\\bar{x}\\)=8\n\\(\\bar{y}\\)=8.5\n\n\n\nDerive The New Dataset\n\n\n\n\n\n\n\n\n\n\nFirst Principal Component (PC1)\n\\(P_{11}\\)\n\\(P_{12}\\)\n\\(P_{13}\\)\n\\(P_{14}\\)\n\n\n\n\n\n\\[\nP_{11} = e_1^T \\times\n\\begin{bmatrix}\n4-mean(x) \\\\ 11 -mean(y)\n\\end{bmatrix}\n= \\begin{bmatrix}\n0.5574 & -0.8303 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n4-8 \\\\ 11-8.5\n\\end{bmatrix} =-4.3052\n\\]\n\\[\nP_{12} = e_1^T \\times\n\\begin{bmatrix}\n8-mean(x) \\\\ 4 -mean(y)\n\\end{bmatrix}\n= \\begin{bmatrix}\n0.5574 & -0.8303 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n8-8 \\\\ 4-8.5\n\\end{bmatrix} =3.7361\n\\]\n\\[\nP_{13} = e_1^T \\times\n\\begin{bmatrix}\n13-mean(x) \\\\ 5 -mean(y)\n\\end{bmatrix}\n= \\begin{bmatrix}\n0.5574 & -0.8303 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n13-8 \\\\ 5-8.5\n\\end{bmatrix}= 5.6928\n\\]\n\\[\nP_{14}= e_1^T \\times\n\\begin{bmatrix}\n7-mean(x) \\\\ 14 -mean(y)\n\\end{bmatrix}\n= \\begin{bmatrix}\n0.5574 & -0.8303 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n7-8 \\\\ 14-8.5\n\\end{bmatrix} = -5.1238\n\\]\n\n4.6.1 The New Dataset (Left singular vector)\n\n\n\n\n\n\n\n\n\n\nFirst Principal Component (PC1)\nStudent 1\nStudent 2\nStudent 3\nStudent 4\n\n\n\n\n\n-4.3052\n3.7361\n5.6928\n-5.1238\n\n\n\nUse R code to analyze the same example\n\n\nCode\n#Load Libraries\nlibrary(MASS)\nlibrary(factoextra)\n\n\nLoading required package: ggplot2\n\n\nWelcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\n\n\nCode\nlibrary(ggplot2)\n\n#Import Example Data\ndata = data.frame(Student = 1:4, BasicStats=c(4,8,13,7), AdvancedStats=c(11,4,5,14))\ndim(data)\n\n\n[1] 4 3\n\n\nCode\n#Structure of Data\nstr(data) #check variable types which matters in PCA \n\n\n'data.frame':   4 obs. of  3 variables:\n $ Student      : int  1 2 3 4\n $ BasicStats   : num  4 8 13 7\n $ AdvancedStats: num  11 4 5 14\n\n\nCode\nsummary(data) #details about variable scales and missing values\n\n\n    Student       BasicStats    AdvancedStats  \n Min.   :1.00   Min.   : 4.00   Min.   : 4.00  \n 1st Qu.:1.75   1st Qu.: 6.25   1st Qu.: 4.75  \n Median :2.50   Median : 7.50   Median : 8.00  \n Mean   :2.50   Mean   : 8.00   Mean   : 8.50  \n 3rd Qu.:3.25   3rd Qu.: 9.25   3rd Qu.:11.75  \n Max.   :4.00   Max.   :13.00   Max.   :14.00  \n\n\nCode\n#Based on Info from Summary: handle missing values and exclude categorical variable\n#na.omit(data) No missing values here\ndata_sample = data[,-c(1)] #exclude categorical variable\n\n#Run PCA\ndata_pca = prcomp(data_sample, scale = TRUE)\n\n\n#Summary of Analysis\nsummary(data_pca)\n\n\nImportance of components:\n                          PC1    PC2\nStandard deviation     1.2700 0.6221\nProportion of Variance 0.8065 0.1935\nCumulative Proportion  0.8065 1.0000\n\n\nCode\n#Elements of PCA Object (all outputs of PCA analysis)\nnames(data_pca) \n\n\n[1] \"sdev\"     \"rotation\" \"center\"   \"scale\"    \"x\"       \n\n\nCode\n#sdev:standard deviation\n#rotation: eigenvectors (loadings per variable within each PC)\n#center: mean of the original variable\n#scale: standard deviations of the original variable\n#x:principal component values/scores\n\n#Scree Plot of Variance\nfviz_eig(data_pca,\n         addlabels = TRUE)\n\n\n\n\n\nCode\n#Biplot with Default Settings\nfviz_pca_biplot(data_pca)\n\n\n\n\n\nCode\n#Biplotwith Labeled Variables\n#fviz_pca_biplot(data_pca,label = \"var\")\n\n\n\n\n\n\n[1] I. T. Jolliffe and J. Cadima, “Principal component analysis: A review and recent developments,” Philosophical transactions of the royal society A: Mathematical, Physical and Engineering Sciences, vol. 374, no. 2065, p. 20150202, 2016.\n\n\n[2] R Core Team, “Prcomp, a function of r: A language and environment for statistical computing.” R Foundation for Statistical Computing, Vienna, Austria, 2023. Available: https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/prcomp. [Accessed: Oct. 16, 2023]\n\n\n[3] S. R. Bennett, “Linear algebra for data science.” 2021. Available: https://shainarace.github.io/LinearAlgebra/index.html. [Accessed: Oct. 16, 2023]\n\n\n[4] J. Hopcroft and R. Kannan, Foundations of data science. 2014.\n\n\n[5] D. G. Luenberger, Optimization by vector space methods. John Wiley & Sons, 1997.\n\n\n[6] H. Abdi and L. Williams, “Principal component analysis. WIREs comp stat 2: 433–459.” 2010.\n\n\n[7] J. Lever, M. Krzywinski, and N. Altman, “Points of significance: Principal component analysis,” Nature methods, vol. 14, no. 7, pp. 641–643, 2017."
  },
  {
    "objectID": "abalone.html",
    "href": "abalone.html",
    "title": "6  Abalone",
    "section": "",
    "text": "7 Data Preparation\nCode\n# clear environment\nrm(list = ls())\n\n# Constant seed\nmy_seed = 95\n\n# Load dataset\nabalone &lt;- read.csv('./abalone/abalone.csv')\n\n# Dataset structure\nstr(abalone)\n\n\n'data.frame':   4177 obs. of  9 variables:\n $ Sex           : chr  \"M\" \"M\" \"F\" \"M\" ...\n $ Length        : num  0.455 0.35 0.53 0.44 0.33 0.425 0.53 0.545 0.475 0.55 ...\n $ Diameter      : num  0.365 0.265 0.42 0.365 0.255 0.3 0.415 0.425 0.37 0.44 ...\n $ Height        : num  0.095 0.09 0.135 0.125 0.08 0.095 0.15 0.125 0.125 0.15 ...\n $ Whole_weight  : num  0.514 0.226 0.677 0.516 0.205 ...\n $ Shucked_weight: num  0.2245 0.0995 0.2565 0.2155 0.0895 ...\n $ Viscera_weight: num  0.101 0.0485 0.1415 0.114 0.0395 ...\n $ Shell_weight  : num  0.15 0.07 0.21 0.155 0.055 0.12 0.33 0.26 0.165 0.32 ...\n $ Rings         : int  15 7 9 10 7 8 20 16 9 19 ...\n\n\nCode\n# Missing values\ncolSums(is.na(abalone))\n\n\n           Sex         Length       Diameter         Height   Whole_weight \n             0              0              0              0              0 \nShucked_weight Viscera_weight   Shell_weight          Rings \n             0              0              0              0\nThe dataset contains 9 variables with 1 categorical variable and 8 numeric variables. The dataset contains no values. For this example in applying principal component analysis, we exclude the categorical variable ‘Sex’ and focus the PCA on the numerical dimensions of the Abalone. For analyses involving a mix of numeric and non-numeric variables other factor analysis techniques can be used, such as factor analysis of mixed data. [ref]\nCode\n# Select only the numeric variables \nabalone &lt;- abalone %&gt;% select(where(is.numeric))\n\nsummary(abalone)\n\n\n     Length         Diameter          Height        Whole_weight   \n Min.   :0.075   Min.   :0.0550   Min.   :0.0000   Min.   :0.0020  \n 1st Qu.:0.450   1st Qu.:0.3500   1st Qu.:0.1150   1st Qu.:0.4415  \n Median :0.545   Median :0.4250   Median :0.1400   Median :0.7995  \n Mean   :0.524   Mean   :0.4079   Mean   :0.1395   Mean   :0.8287  \n 3rd Qu.:0.615   3rd Qu.:0.4800   3rd Qu.:0.1650   3rd Qu.:1.1530  \n Max.   :0.815   Max.   :0.6500   Max.   :1.1300   Max.   :2.8255  \n Shucked_weight   Viscera_weight    Shell_weight        Rings       \n Min.   :0.0010   Min.   :0.0005   Min.   :0.0015   Min.   : 1.000  \n 1st Qu.:0.1860   1st Qu.:0.0935   1st Qu.:0.1300   1st Qu.: 8.000  \n Median :0.3360   Median :0.1710   Median :0.2340   Median : 9.000  \n Mean   :0.3594   Mean   :0.1806   Mean   :0.2388   Mean   : 9.934  \n 3rd Qu.:0.5020   3rd Qu.:0.2530   3rd Qu.:0.3290   3rd Qu.:11.000  \n Max.   :1.4880   Max.   :0.7600   Max.   :1.0050   Max.   :29.000\nThe summary statistics show the differences in measurement between variables, with some variables such as diameter and viscera weight having small ranges and others, namely rings, having relatively large ranges. For this reason, scaling of the variables is a crucial step in PCA to ensure results accurately capture the variance in the data.\nStandardization ensures all variables, also called features, are on the same scale, and the scale function allows us to center the data to a mean of 0 and variance of 1. This ensures no single feature has an outsized effect during the principal component analysis.\nCode\n# Standardization of numerical features\nabalone_sc &lt;- scale(abalone, center = TRUE, scale = TRUE)\n\nsummary(abalone_sc)\n\n\n     Length           Diameter           Height          Whole_weight     \n Min.   :-3.7387   Min.   :-3.5558   Min.   :-3.33555   Min.   :-1.68589  \n 1st Qu.:-0.6161   1st Qu.:-0.5832   1st Qu.:-0.58614   1st Qu.:-0.78966  \n Median : 0.1749   Median : 0.1725   Median : 0.01156   Median :-0.05963  \n Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.00000   Mean   : 0.00000  \n 3rd Qu.: 0.7578   3rd Qu.: 0.7267   3rd Qu.: 0.60926   3rd Qu.: 0.66123  \n Max.   : 2.4232   Max.   : 2.4397   Max.   :23.68045   Max.   : 4.07178  \n Shucked_weight    Viscera_weight      Shell_weight         Rings        \n Min.   :-1.6145   Min.   :-1.64298   Min.   :-1.7049   Min.   :-2.7708  \n 1st Qu.:-0.7811   1st Qu.:-0.79455   1st Qu.:-0.7818   1st Qu.:-0.5997  \n Median :-0.1053   Median :-0.08752   Median :-0.0347   Median :-0.2896  \n Mean   : 0.0000   Mean   : 0.00000   Mean   : 0.0000   Mean   : 0.0000  \n 3rd Qu.: 0.6426   3rd Qu.: 0.66056   3rd Qu.: 0.6478   3rd Qu.: 0.3307  \n Max.   : 5.0848   Max.   : 5.28587   Max.   : 5.5040   Max.   : 5.9136\nViewing the data after scaling and centering, values greater than 3 or less than -3 represent outliers more than 3 standard deviations from the mean. Based on the ranges of the variables, we should view a boxplot of the data to further investigate.\nCode\n# Plot a boxplot to visualize potential outliers\npar(mar=c(4, 8, 4, 4))\nboxplot(abalone_sc, main = \"Visualization of scaled and centered data\", horizontal = TRUE, las = 1)\nAre there enough outliers to be a cause for concern? We can see how many lie outside of the third standard deviation of the data for each variable.\nCode\nprint(colSums(abalone_sc &gt; 3 | abalone_sc &lt; -3))\n\n\n        Length       Diameter         Height   Whole_weight Shucked_weight \n            15             13              5             19             37 \nViscera_weight   Shell_weight          Rings \n            22             27             62\nOf the 4177 observations, at most 62 in a single variable (rings) are outliers. The tolerance for outliers will differ depending on the investigation, but for our purposes this number is well within tolerance for principal component analysis.\nLastly, we can investigate the correlation among the variables. PCA is best used with linearly correlated data. If the data is not correlated, the results of PCA will be less meaningful.\nCode\n# Calculate correlations and round to 2 digits\nabalone_corr &lt;- cor(abalone_sc)\ncorrplot(abalone_corr, method=\"number\")\nOur scaled and centered data has strong linear correlations and contains a relatively small number of outliers. We can now calculate the principal components of the dataset.\nThe prcomp() function in R performs principal component analysis on a dataset using the singular value decomposition method which utilizes the covariance matrix of the data.\nCode\n# Apply PCA using prcomp()\nabalone_pca &lt;- prcomp(abalone_sc)\nsummary(abalone_pca)\n\n\nImportance of components:\n                         PC1     PC2     PC3     PC4     PC5     PC6     PC7\nStandard deviation     2.591 0.83403 0.50837 0.40742 0.29146 0.25194 0.11267\nProportion of Variance 0.839 0.08695 0.03231 0.02075 0.01062 0.00793 0.00159\nCumulative Proportion  0.839 0.92601 0.95831 0.97906 0.98968 0.99761 0.99920\n                           PC8\nStandard deviation     0.07999\nProportion of Variance 0.00080\nCumulative Proportion  1.00000"
  },
  {
    "objectID": "abalone.html#libraries",
    "href": "abalone.html#libraries",
    "title": "6  Abalone",
    "section": "6.1 Libraries",
    "text": "6.1 Libraries\n\n\nCode\n# Load necessary libraries\nlibrary(dplyr)\nlibrary(tidyverse)  # for handling missing values\n# library(EnvStats) # for rosnerTest\n# library(caTools)\nlibrary(caret)\nlibrary(corrplot)\n# library(Metrics)\n# library(car)        # for outliers test\n# library(corrr)      # correlation matrix\n# library(ggcorrplot) # correlation graph\n# library(FactoMineR) # PCA analysis\nlibrary(factoextra) # PCA plots\n# library(pls)        # PC regression\n# library(e1071)      # to fit transform PCA  \n# library(plotly)"
  },
  {
    "objectID": "abalone.html#pca---cumulative-variance-and-number-of-principal-components",
    "href": "abalone.html#pca---cumulative-variance-and-number-of-principal-components",
    "title": "6  Abalone",
    "section": "9.1 PCA - Cumulative Variance and Number of Principal Components",
    "text": "9.1 PCA - Cumulative Variance and Number of Principal Components\n\n\nCode\n# Principal Component scores vector\npc_scores &lt;- abalone_pca$x\n\n# Std Deviation of Components\ncomponent_sdev &lt;- abalone_pca$sdev\n\n# Eigenvector or Loadings\neigenvector &lt;- abalone_pca$rotation\n\n# Mean of variables\ncomponent_mean &lt;- abalone_pca$center \n\n# Scaling factor of Variables\ncomponent_scale &lt;- abalone_pca$scale\n\n# Proportion of variance explained by each PC\nvariance_explained &lt;- component_sdev^2 / sum(component_sdev^2)\n\n# Cumulative proportion of variance explained\ncumulative_variance_explained &lt;- cumsum(variance_explained)\n\n# Retain components that explain a percentage of the variance\nnum_components &lt;- which(cumulative_variance_explained &gt;= 0.92)[1]\n\n# Select the desired number of principal components\nselected_pcs &lt;- pc_scores[, 1:num_components]\n\n# Display cumulative variance\ncumulative_variance_explained\n\n\n[1] 0.8390549 0.9260065 0.9583119 0.9790606 0.9896793 0.9976134 0.9992002\n[8] 1.0000000\n\n\nThe first 2 principal components alone explain 92% of the variance in the data."
  },
  {
    "objectID": "abalone.html#loading-of-first-two-components",
    "href": "abalone.html#loading-of-first-two-components",
    "title": "6  Abalone",
    "section": "9.2 Loading of First Two Components",
    "text": "9.2 Loading of First Two Components\nThe loading are the weights assigned to each variable for that particular principal component.\n\n\nCode\n# Access the loadings for the first two principal components\nloadings_first_two_components &lt;- eigenvector[, 1:2]\n\n# Print the loadings for the first two principal components\nprint(\"Loadings for the first two principal components:\")\n\n\n[1] \"Loadings for the first two principal components:\"\n\n\nCode\nprint(loadings_first_two_components)\n\n\n                     PC1         PC2\nLength         0.3721385  0.06828270\nDiameter       0.3730941  0.04004804\nHeight         0.3400268 -0.07046315\nWhole_weight   0.3783075  0.13734619\nShucked_weight 0.3624545  0.29883992\nViscera_weight 0.3685578  0.17297852\nShell_weight   0.3707578 -0.04540040\nRings          0.2427128 -0.92120385"
  },
  {
    "objectID": "abalone.html#pca---elements",
    "href": "abalone.html#pca---elements",
    "title": "6  Abalone",
    "section": "9.3 PCA - Elements",
    "text": "9.3 PCA - Elements\nThe values in abalone_pca$x are the coordinates of each observation in the new principal component space. These coordinates are the scores for each observation along each principal component. The eigenvectors of the covariance or correlation matrix of the data represent the directions of maximum variance in the dataset."
  },
  {
    "objectID": "abalone.html#scree-plot---cumulative-variance-explained",
    "href": "abalone.html#scree-plot---cumulative-variance-explained",
    "title": "6  Abalone",
    "section": "10.1 Scree Plot - Cumulative Variance Explained",
    "text": "10.1 Scree Plot - Cumulative Variance Explained\n\n\nCode\nfviz_eig(abalone_pca, addlabels = TRUE)\n\n\n\n\n\nThe scree plot visualizes the variance captured by each PC. PC1 explains 83.9% of the variance, and PC2 explains 8.7% variance."
  },
  {
    "objectID": "abalone.html#biplot",
    "href": "abalone.html#biplot",
    "title": "6  Abalone",
    "section": "10.2 Biplot",
    "text": "10.2 Biplot\nThe correlation between a variable and a principal component is used as the coordinates of the variable on the PC, shown as dimensions on the biplot. Dim1 corresponds to PC1, and Dim2 to PC2. The representation of variables differs from the plot of the observations: The observations are represented by their projections, but the variables are represented by their correlations. (Abdi and Williams 2010) [ref]\n\n\nCode\nfviz_pca_biplot(abalone_pca, label = \"var\", alpha.ind = \"contrib\", col.var = \"blue\", repel = TRUE)"
  },
  {
    "objectID": "abalone.html#variable-contribution",
    "href": "abalone.html#variable-contribution",
    "title": "6  Abalone",
    "section": "10.3 Variable Contribution",
    "text": "10.3 Variable Contribution\nTop variable contribution for the first two principal components.\n\n\nCode\n# Contributions of variables to PC1\npc2_contribution &lt;- fviz_contrib(abalone_pca, choice = \"var\", axes = 1, top = 20)\n\n# Modify the theme to rotate X-axis labels to 90 degrees\npc2_contribution +\n  theme(\n    axis.text.x = element_text(angle = 0),\n    plot.title = element_text(hjust = 0)  # horizontal justification\n  ) +\n  coord_flip() +\n  labs(title = \"Contribution of Variables to PC1\",\n       y = \"Percentage Contribution\",\n       x = \"\",\n       caption = \"PC1 explains 83.9% of the variance\") +\n  scale_y_continuous(labels = scales::percent_format(scale = 1,\n                                                     accuracy = 1))\n\n\n\n\n\nCode\n# Contributions of variables to PC2\npc2_contribution &lt;- fviz_contrib(abalone_pca, choice = \"var\", axes = 2, top = 12)\n\n# Modify the theme to rotate X-axis labels to 90 degrees\npc2_contribution +\n  theme(\n    axis.text.x = element_text(angle = 0),\n    plot.title = element_text(hjust = 0)  # horizontal justification\n  ) +\n  coord_flip() +\n  labs(title = \"Contribution of Variables to PC2\",\n       y = \"Percentage Contribution\",\n       x = \"\",\n       caption = \"PC2 explains 8.7% of the variance\") +\n  scale_y_continuous(labels = scales::percent_format(scale = 1,\n                                                     accuracy = 1))"
  },
  {
    "objectID": "analysis.html#libraries",
    "href": "analysis.html#libraries",
    "title": "5  Analysis",
    "section": "5.1 Libraries",
    "text": "5.1 Libraries\n\n\nCode\n# Load necessary libraries\nlibrary(tidyverse)    # for handling missing values\nlibrary(EnvStats) # for rosnerTest\nlibrary(caTools)\nlibrary(caret)\nlibrary(corrplot)\nlibrary(Metrics)\nlibrary(car)        # for outliers test\nlibrary(corrr)      # correlation matrix\nlibrary(ggcorrplot) # correlation graph\nlibrary(FactoMineR) # PCA analysis\nlibrary(factoextra) # PCA plots\nlibrary(pls)        # PC regression\nlibrary(e1071)      # to fit transform PCA \nlibrary(olsrr)        # outliers, leverage plot"
  },
  {
    "objectID": "analysis.html#data-preparation",
    "href": "analysis.html#data-preparation",
    "title": "5  Analysis",
    "section": "5.2 Data Preparation",
    "text": "5.2 Data Preparation\nIn the previous Dataset section, we renamed the variables to be more significant, and easier to make inference. Moreover, we imputed the missing values with the Mean."
  },
  {
    "objectID": "analysis.html#feature-scaling",
    "href": "analysis.html#feature-scaling",
    "title": "5  Analysis",
    "section": "5.3 Feature Scaling",
    "text": "5.3 Feature Scaling\n\nStandardization ensures all features are on the same scale, and this method is less sensitive to outliers.\n\n\n\nCode\n# Find the index position of the target feature \ntarget_name &lt;- \"expected_survival\"\ntarget_index &lt;- grep(target_name, \n                     colnames(train_data))\n\n\n\n\nCode\n# Standardization Numerical Features\ntrain_data_sc &lt;- scale(train_data[, -target_index])"
  },
  {
    "objectID": "analysis.html#pca-requirements",
    "href": "analysis.html#pca-requirements",
    "title": "5  Analysis",
    "section": "5.4 PCA Requirements",
    "text": "5.4 PCA Requirements\n\n5.4.1 Outliers Detection\n\nThere are some outliers in the data frame.\nHowever, there are three outliers with no high leverage.\n\n\n\nCode\n# Plot a boxplot to visualize potential outliers\nboxplot(train_data_sc, main = \"Outliers Detection\")\n\n\n\n\n\n\n\n5.4.2 Leverage\n\n\nCode\nset.seed(my_seed)\n\n# Fit regression model\nordinary_model &lt;- lm(expected_survival ~ ., data = train_data)\n\n# Print the model summary\nsummary(ordinary_model)\n\n# Residual Diagnostics\nols_plot_resid_lev(ordinary_model)\n\n\n\n\n\n\n\n5.4.3 Removing Outliers\nAfter removing one outlier from the dataset, we discovered that the final results produced minimal variability. Therefore, we decided to present a model that captures all the data points’ information; thus, no observations will be removed from the final model.\n\n\nCode\n# No Outliers subset\nno_outliers_df &lt;- slice(train_data, -c(56))\n\nset.seed(my_seed)\n\n# Fit regression model\nno_outliers_model &lt;- lm(expected_survival ~ ., data = no_outliers_df)\n\n# Print the model summary\nsummary(no_outliers_model)\n\n# Residual Diagnostics\nols_plot_resid_lev(no_outliers_model)\n\n\n\n\n\n\n\n5.4.4 Correlations\n\nThere are high correlated features.\nMulticollinearity is present in the data set.\n\n\n\nCode\n# Calculate correlations and round to 2 digits\ncorr_matrix &lt;- cor(train_data_sc)\ncorr_matrix &lt;- round(corr_matrix, digits = 2)\n\n# Print names of highly correlated features; threshold &gt; 0.30\nhigh &lt;- findCorrelation(corr_matrix, cutoff = 0.30, names = TRUE)\nhigh\n\n\n [1] \"expected_transfusion\"                  \n [2] \"better_infection\"                      \n [3] \"expected_hospitalization\"              \n [4] \"expected_hospital_readmission\"         \n [5] \"expected_fistula\"                      \n [6] \"prevalent_transplant_waitlist_expected\"\n [7] \"incident_transplant_waitlist_expected\" \n [8] \"expected_infection\"                    \n [9] \"worse_survival\"                        \n[10] \"incident_transplant_waitlist_better\"   \n[11] \"better_hospital_readmission\"           \n[12] \"worse_transfusion\"                     \n[13] \"worse_hospital_readmission\"            \n[14] \"worse_fistula\"                         \n[15] \"incident_transplant_waitlist_worse\"    \n[16] \"better_fistula\"                        \n[17] \"prevalent_transplant_waitlist_better\"  \n[18] \"better_survival\"                       \n[19] \"worse_infection\"                       \n[20] \"phosphorus (5.6 - 7) Mg\"               \n[21] \"prevalent_transplant_waitlist_worse\"   \n[22] \"phosphorus (3.5 - 4.5) Mg\"             \n[23] \"phosphorus &gt; 7Mg\"                      \n[24] \"better_transfusion\"                    \n[25] \"Hgb 10g\"                               \n[26] \"hypercalcemia_calcium &gt; 10.2Mg\"        \n[27] \"pediatric_nPCR\"                        \n[28] \"long_term_catheter\""
  },
  {
    "objectID": "analysis.html#full-model-regression",
    "href": "analysis.html#full-model-regression",
    "title": "5  Analysis",
    "section": "5.5 Full Model Regression",
    "text": "5.5 Full Model Regression\n\nThe Adjusted R^2 = 99.99% is an indication of over-fitting, or bias.\n\n\n\nCode\nset.seed(my_seed)\n\n# Fit a multiple linear regression model\nfull_model &lt;- lm(expected_survival ~ ., data = train_data)\n\n# Print a summary of the regression model\nsummary(full_model)\n\n\n\nCall:\nlm(formula = expected_survival ~ ., data = train_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.44543 -0.31575 -0.05481  0.39273  1.13134 \n\nCoefficients:\n                                         Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)                            93.7141729 41.8771301   2.238 0.038114\nbetter_transfusion                      1.0433623  0.7504271   1.390 0.181374\nexpected_transfusion                    0.0011165  0.0339108   0.033 0.974098\nworse_transfusion                       0.1321425  0.0752163   1.757 0.095943\nbetter_infection                       -0.2665210  0.0913321  -2.918 0.009178\nexpected_infection                     -0.0353924  0.0617830  -0.573 0.573833\nworse_infection                        -0.1114838  0.2720510  -0.410 0.686796\nKt_v_1.2                                0.2515467  0.2538274   0.991 0.334814\nKt_v_1.7                               -0.0002502  0.0523052  -0.005 0.996236\npedriatic_Kt_v_1.2                     -0.0399970  0.0330110  -1.212 0.241322\npediatric_Kt_v_1.8                     -0.0179608  0.0095227  -1.886 0.075523\npediatric_nPCR                          0.0444777  0.0254264   1.749 0.097275\nbetter_fistula                         -0.1839466  0.1065136  -1.727 0.101292\nexpected_fistula                        0.0061566  0.0750108   0.082 0.935492\nworse_fistula                          -0.0301080  0.1060795  -0.284 0.779783\nlong_term_catheter                      0.3116122  0.0743814   4.189 0.000551\n`hypercalcemia_calcium &gt; 10.2Mg`       -0.1874432  0.0881679  -2.126 0.047599\n`phosphorus &lt; 3.5Mg`                    0.3493258  0.4794052   0.729 0.475581\n`phosphorus (3.5 - 4.5) Mg`            -1.7332425  0.4642343  -3.734 0.001521\n`phosphorus (4.6 - 5.5) Mg`            -1.3625429  0.4593524  -2.966 0.008270\n`phosphorus (5.6 - 7) Mg`              -0.7983529  0.4384021  -1.821 0.085264\n`phosphorus &gt; 7Mg`                     -1.4438916  0.4577542  -3.154 0.005486\nbetter_hospitalization                  0.0845105  0.3231725   0.262 0.796674\nexpected_hospitalization                0.7045325  0.1549119   4.548 0.000249\nworse_hospitalization                   0.5177565  0.2210441   2.342 0.030859\nbetter_hospital_readmission             0.3755861  0.2422979   1.550 0.138521\nexpected_hospital_readmission           0.3149471  0.1346618   2.339 0.031081\nworse_hospital_readmission              0.0922372  0.2390186   0.386 0.704094\nbetter_survival                        -0.5713254  0.1451346  -3.937 0.000967\nworse_survival                         -0.9107860  0.1372411  -6.636 3.14e-06\nincident_transplant_waitlist_better     0.0124309  0.1326932   0.094 0.926397\nincident_transplant_waitlist_expected   0.0335712  0.0407496   0.824 0.420813\nincident_transplant_waitlist_worse      0.3111204  0.1192201   2.610 0.017736\nprevalent_transplant_waitlist_better    0.1505592  0.1108400   1.358 0.191136\nprevalent_transplant_waitlist_expected  0.0410843  0.1012918   0.406 0.689815\nprevalent_transplant_waitlist_worse     0.0060570  0.1878661   0.032 0.974635\n`Hgb 10g`                              -0.1026231  0.0478780  -2.143 0.045990\n`Hgb 12g`                              -0.3572380  0.6630944  -0.539 0.596664\n                                          \n(Intercept)                            *  \nbetter_transfusion                        \nexpected_transfusion                      \nworse_transfusion                      .  \nbetter_infection                       ** \nexpected_infection                        \nworse_infection                           \nKt_v_1.2                                  \nKt_v_1.7                                  \npedriatic_Kt_v_1.2                        \npediatric_Kt_v_1.8                     .  \npediatric_nPCR                         .  \nbetter_fistula                            \nexpected_fistula                          \nworse_fistula                             \nlong_term_catheter                     ***\n`hypercalcemia_calcium &gt; 10.2Mg`       *  \n`phosphorus &lt; 3.5Mg`                      \n`phosphorus (3.5 - 4.5) Mg`            ** \n`phosphorus (4.6 - 5.5) Mg`            ** \n`phosphorus (5.6 - 7) Mg`              .  \n`phosphorus &gt; 7Mg`                     ** \nbetter_hospitalization                    \nexpected_hospitalization               ***\nworse_hospitalization                  *  \nbetter_hospital_readmission               \nexpected_hospital_readmission          *  \nworse_hospital_readmission                \nbetter_survival                        ***\nworse_survival                         ***\nincident_transplant_waitlist_better       \nincident_transplant_waitlist_expected     \nincident_transplant_waitlist_worse     *  \nprevalent_transplant_waitlist_better      \nprevalent_transplant_waitlist_expected    \nprevalent_transplant_waitlist_worse       \n`Hgb 10g`                              *  \n`Hgb 12g`                                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.052 on 18 degrees of freedom\nMultiple R-squared:      1, Adjusted R-squared:  0.9999 \nF-statistic: 2.69e+04 on 37 and 18 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "analysis.html#pca-analysis",
    "href": "analysis.html#pca-analysis",
    "title": "5  Analysis",
    "section": "5.6 PCA Analysis",
    "text": "5.6 PCA Analysis\n\n5.6.1 SVD - Singular Value Decomposition\nWe will focus on Singular Value Decomposition which is a classic approach for PCA analysis.\nSingular Value Decomposition is a factorization technique used in linear algebra to decompose a matrix into three matrices.\n\nU: A matrix whose columns are the left singular vectors of the original matrix.\nD: A diagonal matrix whose entries are the singular values of the original matrix.\nV: A matrix whose columns are the right singular vectors of the original matrix.\n\nThe SVD is closely related to the eigenvalue decomposition (EVD), which is another factorization technique used in linear algebra. While the EVD can only be applied to square matrices, the SVD can be applied to any matrix, including rectangular matrices. The SVD is also more numerically stable than the EVD, making it a preferred method for many applications.\n\nNote: The Spectral Decomposition approach is used with the princomp() function.\n\n\n\nCode\n# Apply PCA using prcomp()\ndata_pca &lt;- prcomp(train_data_sc, center = TRUE, scale. = TRUE)\nsummary(data_pca)\n\n\nImportance of components:\n                         PC1     PC2     PC3     PC4    PC5     PC6     PC7\nStandard deviation     3.885 1.87172 1.83774 1.74868 1.4227 1.25078 1.12796\nProportion of Variance 0.408 0.09468 0.09128 0.08265 0.0547 0.04228 0.03439\nCumulative Proportion  0.408 0.50269 0.59397 0.67661 0.7313 0.77360 0.80799\n                           PC8     PC9    PC10    PC11   PC12    PC13    PC14\nStandard deviation     1.09332 0.94491 0.90813 0.81855 0.7790 0.68145 0.63663\nProportion of Variance 0.03231 0.02413 0.02229 0.01811 0.0164 0.01255 0.01095\nCumulative Proportion  0.84029 0.86442 0.88671 0.90482 0.9212 0.93378 0.94473\n                          PC15    PC16   PC17    PC18    PC19   PC20    PC21\nStandard deviation     0.57103 0.50547 0.5018 0.48845 0.46247 0.4169 0.35338\nProportion of Variance 0.00881 0.00691 0.0068 0.00645 0.00578 0.0047 0.00338\nCumulative Proportion  0.95354 0.96045 0.9673 0.97370 0.97948 0.9842 0.98755\n                          PC22    PC23    PC24    PC25    PC26    PC27    PC28\nStandard deviation     0.32474 0.28837 0.26042 0.22590 0.21491 0.19820 0.16698\nProportion of Variance 0.00285 0.00225 0.00183 0.00138 0.00125 0.00106 0.00075\nCumulative Proportion  0.99040 0.99265 0.99448 0.99586 0.99711 0.99817 0.99893\n                          PC29    PC30    PC31    PC32    PC33    PC34    PC35\nStandard deviation     0.12679 0.10914 0.08380 0.05094 0.03167 0.02871 0.01433\nProportion of Variance 0.00043 0.00032 0.00019 0.00007 0.00003 0.00002 0.00001\nCumulative Proportion  0.99936 0.99968 0.99987 0.99994 0.99997 0.99999 1.00000\n                           PC36     PC37\nStandard deviation     0.008057 0.004918\nProportion of Variance 0.000000 0.000000\nCumulative Proportion  1.000000 1.000000\n\n\n\n\n5.6.2 PCA - Elements\n\nThe values in data_pca$x are the coordinates of each observation in the new principal component space. These coordinates are the scores for each observation along each principal component.\nThe eigenvectors of the covariance or correlation matrix of the data represent the directions of maximum variance or information in the dataset.\n\n\n\nCode\n# Principal Component scores vector\npc_scores &lt;- data_pca$x\n\n# Std Deviation of Components\ncomponent_sdev &lt;- data_pca$sdev\n\n# Eigenvector, or Loadings\neigenvector &lt;- data_pca$rotation\n\n# Mean of variables\ncomponent_mean &lt;- data_pca$center \n\n# Scaling factor of Variables\ncomponent_scale &lt;- data_pca$scale\n\n\n\n\n5.6.3 Loadings of First Two Components\n\nThe loading are the weights assigned to each variable for that particular principal component.\n\n\n\nCode\n# Access the loadings for the first two principal components\nloadings_first_two_components &lt;- eigenvector[, 1:2]\n\n# Print the loadings for the first two principal components\nprint(\"Loadings for the first two principal components:\")\n\n\n[1] \"Loadings for the first two principal components:\"\n\n\nCode\nprint(loadings_first_two_components)\n\n\n                                                 PC1          PC2\nbetter_transfusion                      0.0505988170  0.015071362\nexpected_transfusion                    0.2535655387 -0.021428805\nworse_transfusion                       0.2058083243 -0.164735938\nbetter_infection                        0.2526036797 -0.007341849\nexpected_infection                      0.2455149674 -0.065172327\nworse_infection                         0.1381076997  0.063702113\nKt_v_1.2                                0.0030888370  0.008117565\nKt_v_1.7                               -0.0077627944  0.044354338\npedriatic_Kt_v_1.2                     -0.0635839673  0.052731221\npediatric_Kt_v_1.8                     -0.0139161654  0.002781654\npediatric_nPCR                         -0.0394255772  0.076824152\nbetter_fistula                          0.1701351094  0.184888641\nexpected_fistula                        0.2521168144 -0.055750964\nworse_fistula                           0.1932023133 -0.048145074\nlong_term_catheter                     -0.0054339687  0.040853832\nhypercalcemia_calcium &gt; 10.2Mg         -0.0276031313  0.104909219\nphosphorus &lt; 3.5Mg                      0.0161465158  0.375887922\nphosphorus (3.5 - 4.5) Mg               0.0419719997  0.425799543\nphosphorus (4.6 - 5.5) Mg               0.0888141257  0.266545080\nphosphorus (5.6 - 7) Mg                -0.0954250049 -0.329731283\nphosphorus &gt; 7Mg                       -0.0293097443 -0.439982654\nbetter_hospitalization                  0.1554828676  0.039769206\nexpected_hospitalization                0.2534938280 -0.035294582\nworse_hospitalization                   0.1862740734 -0.104759047\nbetter_hospital_readmission             0.2100006566 -0.046207863\nexpected_hospital_readmission           0.2537238605 -0.041416989\nworse_hospital_readmission              0.2009955081 -0.043217623\nbetter_survival                         0.1522860091  0.168831835\nworse_survival                          0.2288119832 -0.066222930\nincident_transplant_waitlist_better     0.2015728228  0.151287289\nincident_transplant_waitlist_expected   0.2498798110 -0.057824581\nincident_transplant_waitlist_worse      0.1912997762 -0.019853974\nprevalent_transplant_waitlist_better    0.1617720280  0.176291623\nprevalent_transplant_waitlist_expected  0.2500769232 -0.070274998\nprevalent_transplant_waitlist_worse     0.1201730964 -0.228472026\nHgb 10g                                -0.0298792287 -0.080370928\nHgb 12g                                -0.0005836729  0.176481318\n\n\n\n\n5.6.4 PCA - Cumulative Variance\n\n\nCode\n# Proportion of variance explained by each PC\nvariance_explained &lt;- component_sdev^2 / sum(component_sdev^2)\n\n# Cumulative proportion of variance explained\ncumulative_variance_explained &lt;- cumsum(variance_explained)\ncumulative_variance_explained\n\n\n [1] 0.4080054 0.5026901 0.5939685 0.6766143 0.7313169 0.7735996 0.8079857\n [8] 0.8402922 0.8644234 0.8867126 0.9048213 0.9212244 0.9337751 0.9447292\n[15] 0.9535419 0.9604473 0.9672517 0.9736998 0.9794804 0.9841779 0.9875530\n[22] 0.9904031 0.9926506 0.9944835 0.9958627 0.9971109 0.9981727 0.9989263\n[29] 0.9993607 0.9996827 0.9998725 0.9999427 0.9999698 0.9999920 0.9999976\n[36] 0.9999993 1.0000000\n\n\n\n\n5.6.5 PCA - Number of Principal Components\n\nWe conclude that 9 Principal Components explain 86% of the variance.\n\n\n\nCode\n# Retain components that explain a percentage of the variance\nnum_components &lt;- which(cumulative_variance_explained &gt;= 0.86)[1]\n\n# Select the desired number of principal components\nselected_pcs &lt;- pc_scores[, 1:num_components]\nselected_pcs\n\n\n          PC1          PC2         PC3          PC4         PC5         PC6\n1  -3.1377897  0.771802333 -0.73643610 -0.922235383 -0.35927448  0.38542561\n2   0.9108769 -1.373614712 -0.89066560 -0.046036426  0.02278572  0.32672341\n3  -1.3650210 -1.765641978 -0.14492292  0.259189212 -1.28569119 -0.37616348\n4  -3.0594171  0.334783454 -0.01406268 -0.139540012  0.09202588 -0.28246147\n5  -0.1990728 -0.207347300 -0.71486169 -1.375622870 -1.19927381  0.03193723\n6  15.2031917  2.618707806 -7.73627571  4.051949166 -0.82443732 -2.82184549\n7  -1.1596436  1.121551314 -0.71119818 -0.684236835  0.22859575 -0.38210147\n8  -1.7566946  2.446404457  0.23358063 -0.787124212 -0.10982171 -1.06033591\n9  -2.5604524  1.078492112 -0.02070808 -0.001800478 -0.24737807  1.19300909\n10 -2.2725299  0.767002717 -0.05524655 -1.004506232  2.44302298 -1.80923423\n11 10.2100889 -3.095058992  5.56314501 -2.707865857 -3.23281430 -1.47878094\n12  5.1285806 -1.770508028  1.78490237 -0.162837263  0.51758150 -0.36715154\n13 -3.4958456 -4.420614532 -3.14253662  0.040387883 -0.79275740  0.42165629\n14 -1.7133698  0.822955825 -1.96628840 -1.087070858  0.75045171  0.43567043\n15 -1.9510677  0.907515363 -0.34586585 -1.470709937 -0.77348447 -0.78101531\n16 -2.6295662  0.914891571 -0.52633428 -1.348424988 -0.93480439  0.51812412\n17  4.7463371  0.314584946  2.37090668 -0.252790732  0.83290377  1.42903927\n18  0.3868186 -0.929048000  0.68704793 -0.611740653  1.53736445 -0.11125974\n19 -1.6766975  0.284981397 -0.98417172 -1.813309386 -0.81355933  0.75181767\n20 -0.7344809 -1.322183158  0.37001336 -0.844751806  2.05528745 -1.60184302\n21  0.9392629 -0.699900216 -0.13989705 -0.597819034 -0.02027423 -0.00460663\n22 -0.8326091  1.587389983 -0.73121315  0.290973848  0.11987111  1.10998873\n23  0.8724526  1.045058855  0.27746739  0.217533123 -0.62257760  0.54288629\n24 -2.7729541  0.532823236 -0.38569030 -0.934135557  0.89428334 -2.72370097\n25  2.0146426  0.001409232  0.64033906 -0.692861288 -0.16774408  0.45551037\n26 -1.1376241  0.667578303  0.06120837  0.244365584 -0.44647286  0.92651259\n27  0.1255818 -0.995109875  1.15033009 -0.489711370  0.71084468  1.38539681\n28 -3.6485929 -5.124718566  0.04913529  6.332224295 -1.44380969  1.47256020\n29 -0.3307121  0.279250049 -0.09852864 -0.338816118 -0.59497247 -0.57611719\n30 -3.1386137 -2.484679699 -2.00441012  1.066087068 -0.58765600 -0.02923877\n31  1.8304255 -0.865860455 -0.62921128 -0.288806050 -0.35848754  1.15223519\n32 -2.6299851  1.882042739  1.47902941  1.519749388  0.14837356  0.99433001\n33 -1.9654200  1.443737323  0.42388464 -1.421726497  0.34517137  0.14894489\n34 -2.9433777 -0.445332786 -0.18685858  0.050108988 -0.34712485 -0.38836224\n35  1.9334685  4.227422253 -1.54289340 -0.835373008 -0.54123998  2.37389028\n36 -1.4611450  0.555081258 -1.44670031 -0.273171460  0.41677320 -0.33354615\n37 -1.6820407 -0.925297600 -0.31774506 -0.273721372 -0.78520268  0.20360927\n38  6.7594616  3.638815421  1.05618063  2.351356035  2.49702755  3.43721377\n39  3.4506790 -1.182398955  2.56989757 -1.112911235 -0.05458367  1.43612628\n40 -1.0983772 -1.524068762 -0.53200487 -0.477669191 -0.99945377 -0.80297280\n41 -1.9193470 -0.016455120 -1.08062219 -0.587236913 -0.63239954 -0.20135310\n42  3.4962356  1.519916753  1.31092517 -0.300656841  2.59852281  1.04990713\n43 -0.2522021  3.071775273  2.14067078 -1.925517343 -2.27190695 -3.40926804\n44 -3.0134705 -0.685824171 -1.17123942 -0.855106884 -0.44275792  0.83905452\n45  0.3392488 -0.513300425  0.03908522 -0.176642934 -0.59240146  1.05933117\n46 -2.1136312  2.610044185  0.53298151 -0.847818228  0.07520510  0.14163876\n47  1.3706778 -2.910358069  1.08475124  1.316261684  6.13811703 -2.14363119\n48 13.0708364 -2.725721891  0.42574820 -0.475180731 -0.78335087  0.57890473\n49 -2.6464920 -0.380710708 -0.69436207  1.036469413 -0.92018764  0.45363960\n50  1.7504813 -0.179844084 -0.26891623 -0.341129747  0.60754876 -0.77667306\n51 -3.3724826  3.536458694  5.32705688  7.762674701 -2.31626791 -1.79609628\n52 -3.1242143 -1.042445343  0.80437184  2.133119681  2.85592676 -0.90733360\n53 -1.0176612 -1.671769226 -1.82862519  0.394960511 -0.46534146  0.86157778\n54 -0.3396014  0.750547277  0.95957147 -0.301395219  0.57260088  0.01894730\n55 -2.2360197 -0.655876540  0.77645594  0.231805497 -0.27848656 -0.34479229\n56 -3.1511256  0.180665062 -1.07019444 -0.491205128 -0.21428914 -0.62572389\n           PC7         PC8         PC9\n1   1.76446455  0.71374642 -1.27763836\n2   1.90946042  0.32026834  0.31575168\n3  -0.78560815 -0.92036817 -1.26256359\n4  -0.12860754  0.13732138  0.07796673\n5   0.74994371  0.71599423 -0.19541169\n6  -0.39368588  0.75396577  0.12358783\n7  -0.51476890 -1.37312839  0.55597712\n8  -0.14340022 -1.64114704  1.14619211\n9  -0.12337260  1.64840877  1.12228458\n10  1.30112932  0.04017681  0.78534979\n11 -1.01949796  0.39458663  0.01823775\n12  1.27365641  0.38385941 -0.19143125\n13  0.10866259 -1.13950353 -1.57906009\n14  1.10103413  0.88533889 -1.19056845\n15 -1.01157314 -1.47740924  0.22146676\n16  0.35695340  0.88633035 -0.44875630\n17 -0.84922663  0.01277585 -0.04793206\n18  0.22648544  0.76907240 -0.83845390\n19  0.84498090 -0.26045393  0.78929137\n20  0.75707090 -1.18661916  1.67512391\n21  1.31702137  0.49437991  0.47455077\n22 -1.64549969  1.13118828 -0.04484940\n23 -0.21184648  0.96993606  1.61645155\n24 -0.47378261 -2.62659005  1.14510308\n25 -0.38399443  0.27541717  1.00447793\n26 -0.69664564  0.94135363  0.17566660\n27  0.02791903 -0.36150463 -0.45207374\n28 -0.77192847 -0.90604294  2.92091797\n29  2.01369673  0.05235085  0.94851213\n30 -0.76889273 -0.04319356 -0.93018809\n31  1.40987972 -0.23868533  0.11171350\n32  1.01379575  1.75474773 -0.29044327\n33  0.22672970  0.82917850  0.05368812\n34 -1.84413987  0.45197913  0.38530687\n35 -0.29186460  0.24777941  0.05260103\n36  0.03060274  1.07595713 -0.12917118\n37 -0.46866588  0.53016534 -0.43879593\n38 -1.42758292 -3.60748873 -1.60176642\n39 -0.65914865 -1.03448059 -0.16051740\n40 -0.19504945 -1.60842789 -0.69636619\n41 -1.31580368 -1.30270411  0.17184914\n42 -0.23125332 -0.09689562  0.84640868\n43 -2.15443994  0.22710813 -1.25870358\n44 -1.00388333  0.40847078 -0.11206689\n45  1.16629583  0.52067375 -0.32481693\n46  1.09265840 -0.35878735  1.34775109\n47  0.97170555  0.19459095 -1.28733565\n48  0.96197055  0.51964456 -0.22818731\n49  0.21464465 -0.60479462 -0.75320369\n50 -0.00577914  0.27805102  1.52332281\n51  2.17235909 -0.18750827 -1.11305739\n52 -3.95021078  3.06573217 -0.25289702\n53 -0.24568069 -0.17110049 -0.38166238\n54 -0.04332971  0.06369494  0.62713705\n55 -0.03710043  0.79675151 -1.07516939\n56  0.78314261 -1.34416256 -1.67360042"
  },
  {
    "objectID": "analysis.html#visualization",
    "href": "analysis.html#visualization",
    "title": "5  Analysis",
    "section": "5.7 Visualization",
    "text": "5.7 Visualization\n\n5.7.1 Scree Plot - Cumulative Variance Explained\n\nPC1 explains 40.8% variance.\nPC2 explains 9.5% variance.\n\n\n\nCode\nfviz_eig(data_pca, addlabels = TRUE)\n\n\n\n\n\n\n\n5.7.2 Biplot\nThe correlation between a variable and a principal component (PC) is used as the coordinates of the variable on the PC. The representation of variables differs from the plot of the observations: The observations are represented by their projections, but the variables are represented by their correlations (Abdi and Williams 2010).\n\nPC1 is represented in black which displays the longest distance of its projection.\nPC2 is represented in blue which displays a shorter distance as expected.\n\n\n\nCode\nfviz_pca_biplot(data_pca, \n                geom = c(\"point\", \"arrow\"),\n                geom.var = \"arrow\")\n\n\n\n\n\n\n\n5.7.3 Correlation Circle\nThe plot below is also known as variable correlation plots. It shows the relationships between all variables. It can be interpreted as follow:\n\nPositively correlated variables are grouped together.\nNegatively correlated variables are positioned on opposite sides of the plot origin (opposed quadrants).\nThe distance between variables and the origin measures the quality of the variables on the factor map. Variables that are away from the origin are well represented on the factor map.\n\n\n\nCode\n# Control variable colors using their contributions\nfviz_pca_var(data_pca, col.var = \"contrib\",\n   gradient.cols = c(\"white\", \"blue\", \"red\"),\n   geom.var = \"arrow\",\n   ggtheme = theme_minimal())\n\n\n\n\n\n\n\n5.7.4 Variable Contribution\nTop variable contribution for the first two principal components.\n\n\nCode\n# Contributions of variables to PC1\npc2_contribution &lt;- fviz_contrib(data_pca, choice = \"var\", axes = 1, top = 20)\n\n# Modify the theme to rotate X-axis labels to 90 degrees\npc2_contribution +\n  theme(\n    axis.text.x = element_text(angle = 0),\n    plot.title = element_text(hjust = 0)  # horizontal justification\n  ) +\n  coord_flip() +\n  labs(title = \"Contribution of Variables to PC1\",\n       y = \"Percentage Contribution\",\n       x = \"\",\n       caption = \"PC1 explains 40.8% variance\") +\n  scale_y_continuous(labels = scales::percent_format(scale = 1,\n                                                     accuracy = 1))\n\n\n\n\n\nCode\n# Contributions of variables to PC2\npc2_contribution &lt;- fviz_contrib(data_pca, choice = \"var\", axes = 2, top = 12)\n\n# Modify the theme to rotate X-axis labels to 90 degrees\npc2_contribution +\n  theme(\n    axis.text.x = element_text(angle = 0),\n    plot.title = element_text(hjust = 0)  # horizontal justification\n  ) +\n  coord_flip() +\n  labs(title = \"Contribution of Variables to PC2\",\n       y = \"Percentage Contribution\",\n       x = \"\",\n       caption = \"PC2 explains 9.5% variance\") +\n  scale_y_continuous(labels = scales::percent_format(scale = 1,\n                                                     accuracy = 1))"
  },
  {
    "objectID": "analysis.html#model-building",
    "href": "analysis.html#model-building",
    "title": "5  Analysis",
    "section": "5.8 Model Building",
    "text": "5.8 Model Building\n\n5.8.1 Data Splitting into Training & Test set\n\n\nCode\n# reproducible random sampling\nset.seed(my_seed)  \n \n# Create Target y-variable for the training set\ny &lt;- train_data$expected_survival  \n# Split the data into training and test sets \nsplit &lt;- sample.split(y, SplitRatio = 0.7) \ntraining_set &lt;- subset(train_data, split == TRUE) \ntest_set &lt;- subset(train_data, split == FALSE) \n\n\n\n\n5.8.2 Feature Scaling: Standardization\n\nIt is important to Mean-Center the data prior to PCA model building to ensure the first Principal Component is in the direction of maximum variance.\nStandardization produces Mean, \\(\\mu\\)= 0, and Variance, \\(\\sigma\\) = 1.\nWe can rewrite this as:\n\\[\nZ \\sim N(0,1)\n\\]\n\n\n\nCode\n# Feature Scaling: Standardization\n# Perform centering and scaling on the training and test sets\nsc &lt;- preProcess(training_set[, -target_index], \n                 method = c(\"center\", \"scale\"))\ntraining_set[, -target_index] &lt;- predict(\n  sc, training_set[, -target_index])\ntest_set[, -target_index] &lt;- predict(sc, test_set[, -target_index])\n\n# training_set[,-target_index] = scale(training_set[, -target_index])\n# test_set[,-target_index] = scale(test_set[, -target_index])\n\n\n\n\n5.8.3 Applying PCA to Training & Test sets\n\n\nCode\n# Perform Principal Component Analysis (PCA) preprocessing on the training data\npca &lt;- preProcess(training_set[, -target_index], \n                  method = 'pca', pcaComp = 8)\n\n# Apply PCA transformation to original training set\ntraining_set &lt;- predict(pca, training_set)\n\n# Reorder columns, moving the dependent feature index to the end\ntraining_set &lt;- training_set[c(2:9, 1)]\n\n# Apply PCA transformation to original test set\ntest_set &lt;- predict(pca, test_set)\n\n# Reorder columns, moving the dependent feature index to the end\ntest_set &lt;- test_set[c(2:9, 1)]\n\n\n\n\n5.8.4 PRESS & Predicted R^2 Functions\n\n\nCode\n#PRESS - predicted residual sums of squares\nPRESS &lt;- function(linear.model) {\n  # calculate the predictive residuals\n  pr &lt;- residuals(linear.model)/(1-lm.influence(linear.model)$hat)\n  # calculate the PRESS\n  PRESS &lt;- sum(pr^2)\n  \n  return(PRESS)\n}\n\npred_r_squared &lt;- function(linear.model) {\n  # Use anova() to get the sum of squares for the linear model\n  lm.anova &lt;- anova(linear.model)\n  # Calculate the total sum of squares\n  tss &lt;- sum(lm.anova$'Sum Sq')\n  # Calculate the predictive R^2\n  pred.r.squared &lt;- 1-PRESS(linear.model)/(tss)\n  \n  return(pred.r.squared)\n}\n\n\n\n\n5.8.5 PCA Full Model - 8 Principal Components\n\n\nCode\n# reproducible random sampling\nset.seed(my_seed)\n\n# Fit a multiple linear regression model\npca_full_model &lt;- lm(expected_survival ~ ., data = training_set)\n\n# Print a summary of the regression model\nsummary(pca_full_model)\n\n\n\nCall:\nlm(formula = expected_survival ~ ., data = training_set)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-36.203  -9.273   2.228   6.583  35.083 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  97.0513     2.8389  34.187  &lt; 2e-16 ***\nPC1          29.4050     0.7133  41.222  &lt; 2e-16 ***\nPC2           4.5890     1.3329   3.443  0.00172 ** \nPC3          -1.1111     1.6420  -0.677  0.50381    \nPC4           4.9871     1.7472   2.854  0.00775 ** \nPC5          -3.2728     2.2463  -1.457  0.15552    \nPC6           6.9790     2.4355   2.866  0.00754 ** \nPC7          -6.6036     2.5605  -2.579  0.01505 *  \nPC8           3.8007     2.8211   1.347  0.18799    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 17.73 on 30 degrees of freedom\nMultiple R-squared:  0.983, Adjusted R-squared:  0.9785 \nF-statistic: 217.3 on 8 and 30 DF,  p-value: &lt; 2.2e-16\n\n\nCode\n# Calculate PRESS\ncat(\"PRESS: \", PRESS(pca_full_model), \"\\n\")\n\n\nPRESS:  31662.39 \n\n\nCode\n# Calculate predicted R^2\ncat(\"Predicted R^2: \", pred_r_squared(pca_full_model), \"\\n\")\n\n\nPredicted R^2:  0.9430373 \n\n\n\n\n5.8.6 Visualization of Uncorrelated PCA Matrix\n\n\nCode\n# Visual of Principal Components un-correlation\ncorr_matrix &lt;- cor(training_set)\nggcorrplot(corr_matrix)"
  },
  {
    "objectID": "analysis.html#pca---2-principal-components",
    "href": "analysis.html#pca---2-principal-components",
    "title": "5  Analysis",
    "section": "5.9 PCA - 2 Principal Components",
    "text": "5.9 PCA - 2 Principal Components\n\n\nCode\n# Create a subset with 2 principal components\nsignificant_pcs = c(1,2,9)\ntrain_pca &lt;- training_set[, significant_pcs]\ntest_pca &lt;- test_set[, significant_pcs]\n\n\n\n\nCode\n# reproducible random sampling\nset.seed(my_seed)\n\n# Fit a multiple linear regression model\nreg_model &lt;- lm(expected_survival ~ ., \n                data = train_pca)\n\n# Print a summary of the regression model\nsummary(reg_model)\n\n\n\nCall:\nlm(formula = expected_survival ~ ., data = train_pca)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-71.156  -9.381  -3.187   8.080  59.653 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  97.0513     3.5849  27.073  &lt; 2e-16 ***\nPC1          29.4050     0.9008  32.644  &lt; 2e-16 ***\nPC2           4.5890     1.6832   2.726  0.00983 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 22.39 on 36 degrees of freedom\nMultiple R-squared:  0.9675,    Adjusted R-squared:  0.9657 \nF-statistic: 536.5 on 2 and 36 DF,  p-value: &lt; 2.2e-16\n\n\nCode\n# Calculate PRESS\ncat(\"PRESS: \", PRESS(reg_model), \"\\n\")\n\n\nPRESS:  25699.6 \n\n\nCode\n# Calculate predicted R^2\ncat(\"Predicted R^2: \", pred_r_squared(reg_model), \"\\n\")\n\n\nPredicted R^2:  0.9537647 \n\n\n\n5.9.1 Principal Components Regression\n\nPCA is used to calculate principal components that can then be used in principal components regression. This type of regression is often used when multicollinearity exists between predictors in a data set.\n\n\n\nCode\n# reproducible random sampling\nset.seed(my_seed)\n\ny = train_pca$expected_survival\n\n# fit PCR\npcr_model &lt;- pcr(y ~ PC1+PC2, data=train_pca, validation=\"CV\")\n\nsummary(pcr_model)\n\n\nData:   X dimension: 39 2 \n    Y dimension: 39 1\nFit method: svdpc\nNumber of components considered: 2\n\nVALIDATION: RMSEP\nCross-validated using 10 random segments.\n       (Intercept)  1 comps  2 comps\nCV           122.5    36.39    24.27\nadjCV        122.5    34.68    24.11\n\nTRAINING: % variance explained\n   1 comps  2 comps\nX    77.74   100.00\ny    96.08    96.75\n\n\n\n\n5.9.2 Repeated Cross-Validation\n\n\nCode\n# reproducible random sampling\nset.seed(my_seed)\n\n# Repeated cross-validation with 5 folds, 3 repetitions\ntrain_control &lt;- trainControl(method = \"repeatedcv\",\n                                            number = 10, repeats = 3)\n\n# training the model \nmodel_cv_repeat &lt;- train(expected_survival ~ ., \n                         data = train_pca,\n                         method = \"lm\",\n                         trControl = train_control)\n\n\n# Print Overall Model Performance\nprint(model_cv_repeat)\n\n\nLinear Regression \n\n39 samples\n 2 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 35, 35, 35, 35, 35, 35, ... \nResampling results:\n\n  RMSE      Rsquared   MAE     \n  21.19074  0.9728849  16.05276\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n\n\n\n5.9.3 Cross-Validation\n\n\nCode\n# reproducible random sampling\nset.seed(my_seed)\n\n# Cross-validation with n folds\nk_10 &lt;- trainControl(method = \"cv\", number = 10)\n\n# training the model \nmodel_cv &lt;- train(expected_survival ~ ., \n                  data = train_pca,\n                  method = \"lm\",\n                  trControl = k_10)\n\n# Print Model Performance\nprint(model_cv)\n\n\nLinear Regression \n\n39 samples\n 2 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 35, 35, 35, 35, 35, 35, ... \nResampling results:\n\n  RMSE     Rsquared   MAE     \n  21.7825  0.9761582  16.14872\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n\nCode\ncv_results = model_cv$results"
  },
  {
    "objectID": "analysis.html#predictions",
    "href": "analysis.html#predictions",
    "title": "5  Analysis",
    "section": "5.10 Predictions",
    "text": "5.10 Predictions\n\n\nCode\n# Find the index position of the target feature\npred_target_index &lt;- grep(target_name, \n                     colnames(test_pca))\ncat(\"Target Feature Index =\", pred_target_index)\n\n\nTarget Feature Index = 3\n\n\nCode\n# Create Predicted Target Feature (y-test) \ny_test &lt;- test_pca[pred_target_index]\n\n\n\n\nCode\n# Predictions using the Cross-Validation model\ny_pred = predict(model_cv, newdata = test_pca[, -pred_target_index])\n\n\n\n\nCode\n# Prediction Results from y_predictions\nround(y_pred, digits = 0)\n\n\n  9  11  12  16  17  18  21  23  25  26  28  40  43  47  48  52  53 \n 35 520 310  26 298 125 159 149 191  81   2  86 113 182 585  33  88 \n\n\n\n\nCode\n# Transform y_test from data frame to numeric\ny_test_numeric &lt;- as.numeric(unlist(y_test))\n\n# Original data\ny_test_numeric\n\n\n [1]  16 442 318  25 284 152 160 141 197  95   2  78  38 171 657   7  92\n\n\n\n5.10.1 Prediction Metrics\n\n\nCode\n# Calculate Mean Absolute Error (MAE)\nmae_value &lt;- mae(y_pred, y_test_numeric)\ncat(\"MAE =\", mae_value)\n\n\nMAE = 21.90924\n\n\nCode\n# Calculate MSE\nmse_predict &lt;- mean((y_pred - y_test_numeric)^2)\ncat(\"\\nMSE =\", mse_predict)\n\n\n\nMSE = 1140.189\n\n\nCode\n# Calculate RMSE\nrmse_predict &lt;- sqrt(mean((y_pred - y_test_numeric)^2))\ncat(\"\\nRMSE =\", rmse_predict)\n\n\n\nRMSE = 33.76669\n\n\nCode\n# Calculate R-squared (R^2)\npredicted_r2 &lt;- 1 - sum((y_test_numeric - y_pred)^2) / \n  sum((y_test_numeric - mean(y_test_numeric))^2)\ncat(\"\\nPredicted R^2 =\", predicted_r2)\n\n\n\nPredicted R^2 = 0.9600483"
  },
  {
    "objectID": "analysis.html#training-conclusion",
    "href": "analysis.html#training-conclusion",
    "title": "5  Analysis",
    "section": "5.11 Training Conclusion",
    "text": "5.11 Training Conclusion\nIn conclusion, this project has demonstrated the effectiveness of Principal Component Analysis (PCA) in dimension reduction with the following key points:\n\nPCA was able to reduce from 37 features down to just 2 principal components.\nThe best score of R^2 = 97.61% was from the Linear Regression with Cross-validation model.\nThe predicted R^2 = 96%\nThe average deviation between the predicted values, and observed values for ‘Expected Survival’ is RMSE = 33.77.\nThe model has not been exposed to unseen data with a large amount of observations to asses its robustness, and reliability."
  },
  {
    "objectID": "abalone.html#data-preparation",
    "href": "abalone.html#data-preparation",
    "title": "6  Abalone",
    "section": "6.2 Data Preparation",
    "text": "6.2 Data Preparation\n\n\nCode\n# clear environment\nrm(list = ls())\n\n# Constant seed\nmy_seed = 95\n\n# Load dataset\nabalone &lt;- read.csv('./abalone/abalone.csv')\n\n# Dataset structure\nstr(abalone)\n\n\n'data.frame':   4177 obs. of  9 variables:\n $ Sex           : chr  \"M\" \"M\" \"F\" \"M\" ...\n $ Length        : num  0.455 0.35 0.53 0.44 0.33 0.425 0.53 0.545 0.475 0.55 ...\n $ Diameter      : num  0.365 0.265 0.42 0.365 0.255 0.3 0.415 0.425 0.37 0.44 ...\n $ Height        : num  0.095 0.09 0.135 0.125 0.08 0.095 0.15 0.125 0.125 0.15 ...\n $ Whole_weight  : num  0.514 0.226 0.677 0.516 0.205 ...\n $ Shucked_weight: num  0.2245 0.0995 0.2565 0.2155 0.0895 ...\n $ Viscera_weight: num  0.101 0.0485 0.1415 0.114 0.0395 ...\n $ Shell_weight  : num  0.15 0.07 0.21 0.155 0.055 0.12 0.33 0.26 0.165 0.32 ...\n $ Rings         : int  15 7 9 10 7 8 20 16 9 19 ...\n\n\nCode\n# Missing values\ncolSums(is.na(abalone))\n\n\n           Sex         Length       Diameter         Height   Whole_weight \n             0              0              0              0              0 \nShucked_weight Viscera_weight   Shell_weight          Rings \n             0              0              0              0 \n\n\nThe dataset contains 9 variables with 1 categorical variable and 8 numeric variables. The dataset contains no missing values. For this example in applying principal component analysis, we exclude the categorical variable ‘Sex’ and focus the PCA on the numerical dimensions of the Abalone. For analyses involving a mix of numeric and non-numeric variables other factor analysis techniques can be used, such as factor analysis of mixed data. [1]\n\n\nCode\n# Select only the numeric variables \nabalone &lt;- abalone %&gt;% select(where(is.numeric))\n\nsummary(abalone)\n\n\n     Length         Diameter          Height        Whole_weight   \n Min.   :0.075   Min.   :0.0550   Min.   :0.0000   Min.   :0.0020  \n 1st Qu.:0.450   1st Qu.:0.3500   1st Qu.:0.1150   1st Qu.:0.4415  \n Median :0.545   Median :0.4250   Median :0.1400   Median :0.7995  \n Mean   :0.524   Mean   :0.4079   Mean   :0.1395   Mean   :0.8287  \n 3rd Qu.:0.615   3rd Qu.:0.4800   3rd Qu.:0.1650   3rd Qu.:1.1530  \n Max.   :0.815   Max.   :0.6500   Max.   :1.1300   Max.   :2.8255  \n Shucked_weight   Viscera_weight    Shell_weight        Rings       \n Min.   :0.0010   Min.   :0.0005   Min.   :0.0015   Min.   : 1.000  \n 1st Qu.:0.1860   1st Qu.:0.0935   1st Qu.:0.1300   1st Qu.: 8.000  \n Median :0.3360   Median :0.1710   Median :0.2340   Median : 9.000  \n Mean   :0.3594   Mean   :0.1806   Mean   :0.2388   Mean   : 9.934  \n 3rd Qu.:0.5020   3rd Qu.:0.2530   3rd Qu.:0.3290   3rd Qu.:11.000  \n Max.   :1.4880   Max.   :0.7600   Max.   :1.0050   Max.   :29.000  \n\n\nThe summary statistics show the differences in measurement between variables, with some variables such as diameter and viscera weight having small ranges and others, namely rings, having relatively large ranges. For this reason, scaling of the variables is a crucial step in PCA to ensure results accurately capture the variance in the data."
  },
  {
    "objectID": "abalone.html#feature-scaling",
    "href": "abalone.html#feature-scaling",
    "title": "6  Abalone",
    "section": "6.3 Feature Scaling",
    "text": "6.3 Feature Scaling\nStandardization ensures all variables, also called features, are on the same scale, and the scale function allows us to center the data to a mean of 0 and variance of 1. This ensures no single feature has an outsized effect during the principal component analysis.\n\n\nCode\n# Standardization of numerical features\nabalone_sc &lt;- scale(abalone, center = TRUE, scale = TRUE)\n\nsummary(abalone_sc)\n\n\n     Length           Diameter           Height          Whole_weight     \n Min.   :-3.7387   Min.   :-3.5558   Min.   :-3.33555   Min.   :-1.68589  \n 1st Qu.:-0.6161   1st Qu.:-0.5832   1st Qu.:-0.58614   1st Qu.:-0.78966  \n Median : 0.1749   Median : 0.1725   Median : 0.01156   Median :-0.05963  \n Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.00000   Mean   : 0.00000  \n 3rd Qu.: 0.7578   3rd Qu.: 0.7267   3rd Qu.: 0.60926   3rd Qu.: 0.66123  \n Max.   : 2.4232   Max.   : 2.4397   Max.   :23.68045   Max.   : 4.07178  \n Shucked_weight    Viscera_weight      Shell_weight         Rings        \n Min.   :-1.6145   Min.   :-1.64298   Min.   :-1.7049   Min.   :-2.7708  \n 1st Qu.:-0.7811   1st Qu.:-0.79455   1st Qu.:-0.7818   1st Qu.:-0.5997  \n Median :-0.1053   Median :-0.08752   Median :-0.0347   Median :-0.2896  \n Mean   : 0.0000   Mean   : 0.00000   Mean   : 0.0000   Mean   : 0.0000  \n 3rd Qu.: 0.6426   3rd Qu.: 0.66056   3rd Qu.: 0.6478   3rd Qu.: 0.3307  \n Max.   : 5.0848   Max.   : 5.28587   Max.   : 5.5040   Max.   : 5.9136  \n\n\nViewing the data after scaling and centering, values greater than 3 or less than -3 represent outliers more than 3 standard deviations from the mean. Based on the ranges of the variables, we should view a boxplot of the data to further investigate.\n\n\nCode\n# Plot a boxplot to visualize potential outliers\npar(mar=c(4, 8, 4, 4))\nboxplot(abalone_sc, main = \"Visualization of scaled and centered data\", horizontal = TRUE, las = 1)\n\n\n\n\n\nAre there enough outliers to be a cause for concern? We can see how many lie outside of the third standard deviation of the data for each variable.\n\n\nCode\nprint(colSums(abalone_sc &gt; 3 | abalone_sc &lt; -3))\n\n\n        Length       Diameter         Height   Whole_weight Shucked_weight \n            15             13              5             19             37 \nViscera_weight   Shell_weight          Rings \n            22             27             62 \n\n\nOf the 4177 observations, at most 62 in a single variable (rings) are outliers. The tolerance for outliers will differ depending on the investigation, but for our purposes this number is well within tolerance for principal component analysis.\nLastly, we can investigate the correlation among the variables. PCA is best used with linearly correlated data. If the data is not correlated, the results of PCA will be less meaningful.\n\n\nCode\n# Calculate correlations and round to 2 digits\nabalone_corr &lt;- cor(abalone_sc)\ncorrplot(abalone_corr, method=\"number\")\n\n\n\n\n\nOur scaled and centered data has strong linear correlations and contains a relatively small number of outliers. We can now calculate the principal components of the dataset."
  },
  {
    "objectID": "abalone.html#pca-via-singular-value-decomposition",
    "href": "abalone.html#pca-via-singular-value-decomposition",
    "title": "6  Abalone",
    "section": "6.4 PCA via Singular Value Decomposition",
    "text": "6.4 PCA via Singular Value Decomposition\nThe prcomp() function in R performs principal component analysis on a dataset using the singular value decomposition method which utilizes the covariance matrix of the data. [2]\n\n\nCode\n# Apply PCA using prcomp()\nabalone_pca &lt;- prcomp(abalone_sc)\nsummary(abalone_pca)\n\n\nImportance of components:\n                         PC1     PC2     PC3     PC4     PC5     PC6     PC7\nStandard deviation     2.591 0.83403 0.50837 0.40742 0.29146 0.25194 0.11267\nProportion of Variance 0.839 0.08695 0.03231 0.02075 0.01062 0.00793 0.00159\nCumulative Proportion  0.839 0.92601 0.95831 0.97906 0.98968 0.99761 0.99920\n                           PC8\nStandard deviation     0.07999\nProportion of Variance 0.00080\nCumulative Proportion  1.00000\n\n\n\n6.4.1 PCA - Cumulative Variance and Number of Principal Components\n\n\nCode\n# Principal Component scores vector\npc_scores &lt;- abalone_pca$x\n\n# Std Deviation of Components\ncomponent_sdev &lt;- abalone_pca$sdev\n\n# Eigenvector or Loadings\neigenvector &lt;- abalone_pca$rotation\n\n# Mean of variables\ncomponent_mean &lt;- abalone_pca$center \n\n# Scaling factor of Variables\ncomponent_scale &lt;- abalone_pca$scale\n\n# Proportion of variance explained by each PC\nvariance_explained &lt;- component_sdev^2 / sum(component_sdev^2)\n\n# Cumulative proportion of variance explained\ncumulative_variance_explained &lt;- cumsum(variance_explained)\n\n# Retain components that explain a percentage of the variance\nnum_components &lt;- which(cumulative_variance_explained &gt;= 0.92)[1]\n\n# Select the desired number of principal components\nselected_pcs &lt;- pc_scores[, 1:num_components]\n\n# Display cumulative variance\ncumulative_variance_explained\n\n\n[1] 0.8390549 0.9260065 0.9583119 0.9790606 0.9896793 0.9976134 0.9992002\n[8] 1.0000000\n\n\nThe first 2 principal components alone explain 92% of the variance in the data.\n\n\n6.4.2 Loading of First Two Components\nThe loading are the weights assigned to each variable for that particular principal component.\n\n\nCode\n# Access the loadings for the first two principal components\nloadings_first_two_components &lt;- eigenvector[, 1:2]\n\n# Print the loadings for the first two principal components\nprint(\"Loadings for the first two principal components:\")\n\n\n[1] \"Loadings for the first two principal components:\"\n\n\nCode\nprint(loadings_first_two_components)\n\n\n                     PC1         PC2\nLength         0.3721385  0.06828270\nDiameter       0.3730941  0.04004804\nHeight         0.3400268 -0.07046315\nWhole_weight   0.3783075  0.13734619\nShucked_weight 0.3624545  0.29883992\nViscera_weight 0.3685578  0.17297852\nShell_weight   0.3707578 -0.04540040\nRings          0.2427128 -0.92120385\n\n\n\n\n6.4.3 PCA - Elements\nThe values in abalone_pca$x are the coordinates of each observation in the new principal component space. These coordinates are the scores for each observation along each principal component. The eigenvectors of the covariance or correlation matrix of the data represent the directions of maximum variance in the dataset."
  },
  {
    "objectID": "abalone.html#visualization",
    "href": "abalone.html#visualization",
    "title": "6  Abalone",
    "section": "6.5 Visualization",
    "text": "6.5 Visualization\n\n6.5.1 Scree Plot - Cumulative Variance Explained\n\n\nCode\nfviz_eig(abalone_pca, addlabels = TRUE)\n\n\n\n\n\nThe scree plot visualizes the variance captured by each PC. PC1 explains 83.9% of the variance, and PC2 explains 8.7% variance.\n\n\n6.5.2 Biplot\nThe correlation between a variable and a principal component is used as the coordinates of the variable on the PC, shown as dimensions on the biplot. Dim1 corresponds to PC1, and Dim2 to PC2. The representation of variables differs from the plot of the observations: The observations are represented by their projections, but the variables are represented by their correlations. (Abdi and Williams 2010) [ref]\n\n\nCode\nfviz_pca_biplot(abalone_pca, label = \"var\", alpha.ind = \"contrib\", col.var = \"blue\", repel = TRUE)\n\n\n\n\n\n\n\n6.5.3 Variable Contribution\nTop variable contribution for the first two principal components.\n\n\nCode\n# Contributions of variables to PC1\npc2_contribution &lt;- fviz_contrib(abalone_pca, choice = \"var\", axes = 1, top = 20)\n\n# Modify the theme to rotate X-axis labels to 90 degrees\npc2_contribution +\n  theme(\n    axis.text.x = element_text(angle = 0),\n    plot.title = element_text(hjust = 0)  # horizontal justification\n  ) +\n  coord_flip() +\n  labs(title = \"Contribution of Variables to PC1\",\n       y = \"Percentage Contribution\",\n       x = \"\",\n       caption = \"PC1 explains 83.9% of the variance\") +\n  scale_y_continuous(labels = scales::percent_format(scale = 1,\n                                                     accuracy = 1))\n\n\n\n\n\nCode\n# Contributions of variables to PC2\npc2_contribution &lt;- fviz_contrib(abalone_pca, choice = \"var\", axes = 2, top = 12)\n\n# Modify the theme to rotate X-axis labels to 90 degrees\npc2_contribution +\n  theme(\n    axis.text.x = element_text(angle = 0),\n    plot.title = element_text(hjust = 0)  # horizontal justification\n  ) +\n  coord_flip() +\n  labs(title = \"Contribution of Variables to PC2\",\n       y = \"Percentage Contribution\",\n       x = \"\",\n       caption = \"PC2 explains 8.7% of the variance\") +\n  scale_y_continuous(labels = scales::percent_format(scale = 1,\n                                                     accuracy = 1))\n\n\n\n\n\n\n\n\n\n[1] J. Pagès, Multiple factor analysis by example using r. CRC Press, 2014.\n\n\n[2] R Core Team, “Prcomp, a function of r: A language and environment for statistical computing.” R Foundation for Statistical Computing, Vienna, Austria, 2023. Available: https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/prcomp. [Accessed: Oct. 16, 2023]"
  }
]