# Abalone

## Libraries

```{r setup, echo=FALSE}
# Install libraries if needed
# install.packages("caret")
# install.packages("corrplot")
# install.packages("Metrics")
# install.packages("caTools")
# install.packages("EnvStats")
# install.packages("car")
# install.packages("corrr")
# install.packages("ggcorrplot")
# install.packages("FactoMineR")
# install.packages("factoextra")
# install.packages("pls")
# install.packages("e1071")
# install.packages("plotly")
```

```{r, warning = FALSE, message = FALSE}
# Load necessary libraries
library(dplyr)
library(tidyverse)  # for handling missing values
# library(EnvStats) # for rosnerTest
# library(caTools)
library(caret)
library(corrplot)
# library(Metrics)
# library(car)        # for outliers test
# library(corrr)      # correlation matrix
# library(ggcorrplot) # correlation graph
# library(FactoMineR) # PCA analysis
library(factoextra) # PCA plots
# library(pls)        # PC regression
# library(e1071)      # to fit transform PCA  
# library(plotly)
```

# 01 - Data Preparation

```{r}
# clear environment
rm(list = ls())

# Constant seed
my_seed = 95

# Load dataset
abalone <- read.csv('./abalone.csv')

# Dataset structure
str(abalone)

# Missing values
colSums(is.na(abalone))
```

The dataset contains 9 variables with 1 categorical variable and 8 numeric variables. The dataset contains no values. For this example in applying principal component analysis, we exclude the categorical variable 'Sex' and focus the PCA on the numerical dimensions of the Abalone. For analyses involving a mix of numeric and non-numeric variables other factor analysis techniques can be used, such as factor analysis of mixed data. \[ref\]

```{r}
# Select only the numeric variables 
abalone <- abalone %>% select(where(is.numeric))

summary(abalone)
```

The summary statistics show the differences in measurement between variables, with some variables such as diameter and viscera weight having small ranges and others, namely rings, having relatively large ranges. For this reason, scaling of the variables is a crucial step in PCA to ensure results accurately capture the variance in the data.

# 02 - Feature Scaling

Standardization ensures all variables, also called features, are on the same scale, and the scale function allows us to center the data to a mean of 0 and variance of 1. This ensures no single feature has an outsized effect during the principal component analysis.

```{r}
# Standardization of numerical features
abalone_sc <- scale(abalone, center = TRUE, scale = TRUE)

summary(abalone_sc)
```

Viewing the data after scaling and centering, values greater than 3 or less than -3 represent outliers more than 3 standard deviations from the mean. Based on the ranges of the variables, we should view a boxplot of the data to further investigate.

```{r}
# Plot a boxplot to visualize potential outliers
par(mar=c(4, 8, 4, 4))
boxplot(abalone_sc, main = "Visualization of scaled and centered data", horizontal = TRUE, las = 1)
```

Are there enough outliers to be a cause for concern? We can see how many lie outside of the third standard deviation of the data for each variable.

```{r}
print(colSums(abalone_sc > 3 | abalone_sc < -3))
```

Of the 4177 observations, at most 62 in a single variable (rings) are outliers. The tolerance for outliers will differ depending on the investigation, but for our purposes this number is well within tolerance for principal component analysis.

Lastly, we can investigate the correlation among the variables. PCA is best used with linearly correlated data. If the data is not correlated, the results of PCA will be less meaningful.

```{r}
# Calculate correlations and round to 2 digits
abalone_corr <- cor(abalone_sc)
corrplot(abalone_corr, method="number")
```

Our scaled and centered data has strong linear correlations and contains a relatively small number of outliers. We can now calculate the principal components of the dataset.

# 03 - PCA via Singular Value Decomposition

The prcomp() function in R performs principal component analysis on a dataset using the singular value decomposition method which utilizes the covariance matrix of the data.

```{r}
# Apply PCA using prcomp()
abalone_pca <- prcomp(abalone_sc)
summary(abalone_pca)
```

## PCA - Cumulative Variance and Number of Principal Components

```{r}
# Principal Component scores vector
pc_scores <- abalone_pca$x

# Std Deviation of Components
component_sdev <- abalone_pca$sdev

# Eigenvector or Loadings
eigenvector <- abalone_pca$rotation

# Mean of variables
component_mean <- abalone_pca$center 

# Scaling factor of Variables
component_scale <- abalone_pca$scale

# Proportion of variance explained by each PC
variance_explained <- component_sdev^2 / sum(component_sdev^2)

# Cumulative proportion of variance explained
cumulative_variance_explained <- cumsum(variance_explained)

# Retain components that explain a percentage of the variance
num_components <- which(cumulative_variance_explained >= 0.92)[1]

# Select the desired number of principal components
selected_pcs <- pc_scores[, 1:num_components]

# Display cumulative variance
cumulative_variance_explained
```

The first 2 principal components alone explain 92% of the variance in the data.

## Loading of First Two Components

The loading are the weights assigned to each variable for that particular principal component.

```{r}
# Access the loadings for the first two principal components
loadings_first_two_components <- eigenvector[, 1:2]

# Print the loadings for the first two principal components
print("Loadings for the first two principal components:")
print(loadings_first_two_components)
```

## PCA - Elements

The values in **`abalone_pca$x`** are the coordinates of each observation in the new principal component space. These coordinates are the scores for each observation along each principal component. The eigenvectors of the covariance or correlation matrix of the data represent the directions of maximum variance in the dataset.

# 05 - Visualization

## Scree Plot - Cumulative Variance Explained

```{r}
fviz_eig(abalone_pca, addlabels = TRUE)
```

The scree plot visualizes the variance captured by each PC. PC1 explains 83.9% of the variance, and PC2 explains 8.7% variance.

## Biplot

The correlation between a variable and a principal component is used as the coordinates of the variable on the PC, shown as dimensions on the biplot. Dim1 corresponds to PC1, and Dim2 to PC2. The representation of variables differs from the plot of the observations: The observations are represented by their projections, but the variables are represented by their correlations. (Abdi and Williams 2010) \[ref\]

```{r}
fviz_pca_biplot(abalone_pca, label = "var", alpha.ind = "contrib", col.var = "blue", repel = TRUE)
```

## Variable Contribution

Top variable contribution for the first two principal components.

```{r}
# Contributions of variables to PC1
pc2_contribution <- fviz_contrib(abalone_pca, choice = "var", axes = 1, top = 20)

# Modify the theme to rotate X-axis labels to 90 degrees
pc2_contribution +
  theme(
    axis.text.x = element_text(angle = 0),
    plot.title = element_text(hjust = 0)  # horizontal justification
  ) +
  coord_flip() +
  labs(title = "Contribution of Variables to PC1",
       y = "Percentage Contribution",
       x = "",
       caption = "PC1 explains 83.9% of the variance") +
  scale_y_continuous(labels = scales::percent_format(scale = 1,
                                                     accuracy = 1))


# Contributions of variables to PC2
pc2_contribution <- fviz_contrib(abalone_pca, choice = "var", axes = 2, top = 12)

# Modify the theme to rotate X-axis labels to 90 degrees
pc2_contribution +
  theme(
    axis.text.x = element_text(angle = 0),
    plot.title = element_text(hjust = 0)  # horizontal justification
  ) +
  coord_flip() +
  labs(title = "Contribution of Variables to PC2",
       y = "Percentage Contribution",
       x = "",
       caption = "PC2 explains 8.7% of the variance") +
  scale_y_continuous(labels = scales::percent_format(scale = 1,
                                                     accuracy = 1))
```
