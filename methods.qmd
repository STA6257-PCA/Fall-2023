# Methods {#methods}

The aim of Principal component analysis (PCA) is to reduce the dimensionality of multivariate data while preserving the variability present in the data. The principal components derived from the dataset are orthogonal variables represented by linear combinations of the original variables which maximize variance. The first principal component captures the most variance, followed by the second principal component, and so on. Principal components can be calculated using eigenvalue decomposition or the singular value decomposition (SVD) of the data matrix, so data must be preproccesed and several assumptions met for PCA to yield meaningful results.

## Assumptions

For PCA to be effective, the data should be continuous (although adaptations of PCA exist for other numeric data structures \[ref\]) and normally distributed, although the the distribution of the data does not truly matter for the use of PCA as an investigative tool. More importantly, the data should be linearly related or the linear combinations of the principal components cannot meaningfully capture the variance of the data. Ideally, the variables should be similar in scale and free from extreme or missing values, although this can be addressed in preprocessing, and implementations of PCA such as robust PCA have been developed to address these challenges. \[ref\]

## Preprocessing

Preprocessing data for PCA is straightforward. Missing data should be handled using a method appropriate for the dataset, such as imputation based on the mean or median of the variable observations. After this the variables should be centered and scaled, to a mean of 0 and a standard deviation of 1, although statistical software libraries for SVD and PCA may include this as an option within the function. \[ref\]

## Principal Component Analysis via Singular Value Decomposition

Let our dataset be represented by the $N \times P$ matrix $X$ comprised of $N$ observations of $P$ variables in the data set, where any element $x_{np}$ represents the $n$th observation of variable $p$ in the dataset and matrix $X$ has rank $R$ where $R \leq min\{N, P\}$. The data in $X$ is centered and scaled, such that the mean of each column $X_p$ is 0 and every $x_np$ has been standardized with unit variance. We can represent this with the formula:

$$
z_{np} = \frac{x_{np} - \bar{x}_{p}}{\sqrt{\sigma_{p}}}
$$ {#eq-1}

Next we define the singular value decomposition of $X$. Let $L$ be the $N \times R$ matrix of left singular vectors of the matrix; that is, the columns of $L$ are made up of the eigenvectors of $XX^T$. Let $R$ be the $P \times R$ matrix of right singular vector; the columns of $R$ are made up of the eigenvectors of $X^TX$. Finally, let $D$ be the diagonal matrix of singular values, meaning the singular values in $D$ are the square roots of the eigenvalues of $XX^T$ and $X^TX$, and $D^2$ is defined as the diagonal matrix of the non-zero eigenvalues. \[ref\] We can define the singular value decomposition of matrix $X$ as:

$$
X = LD{R}^T
$$
