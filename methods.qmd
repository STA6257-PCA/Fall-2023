# Methods {#methods}

The aim of Principal component analysis (PCA) is to reduce the dimensionality of multivariate data while preserving the variability present in the data. The principal components derived from the dataset are orthogonal variables represented by linear combinations of the original variables which maximize variance. The first principal component (PC) captures the most variance, followed by the second orthogonal principal component, and so on. Principal components can be calculated using eigenvalue decomposition or the singular value decomposition (SVD) of the data matrix, so data must be preproccesed and several assumptions met for PCA to yield meaningful results.

## Assumptions

For PCA to be effective, the data should be continuous (although adaptations of PCA exist for other numeric data structures \[ref\]) and normally distributed, although the the distribution of the data does not truly matter for the use of PCA as an investigative tool. More importantly, the data should be linearly related or the linear combinations of the principal components cannot meaningfully capture the variance of the data. Ideally, the variables should be similar in scale and free from extreme or missing values, although this can be addressed in preprocessing, and implementations of PCA such as robust PCA have been developed to address these challenges. \[ref\]

## Preprocessing

Preprocessing data for PCA is straightforward. Missing data should be handled using a method appropriate for the dataset, such as imputation based on the mean or median of the variable observations. After this the variables should be centered and scaled, to a mean of 0 and a standard deviation of 1, although statistical software libraries for SVD and PCA may include this as an option within the function. \[ref\]

## Principal Component Analysis via Singular Value Decomposition

In this approach to PCA, SVD is used to extract the most information (variance) from the data matrix while reducing the dimensionality of the data. The first PC will have the largest possible variance (also called inertia), whose value is defined as a factor score. Factor scores represent a geometric projection of the observations onto the PCs. The calculation of PCs via SVD can be understood with the implementation of matrix operations on a dataset.

Let our dataset be represented by the $N \times P$ matrix $X$ comprised of $N$ observations of $P$ variables in the data set, where any element $x_{np}$ represents the $n$th observation of variable $p$ in the dataset and matrix $X$ has rank $R$ where $R \leq min\{N, P\}$. The data in $X$ is centered and scaled, such that the mean of each column $X_p$ is 0 and every $x_np$ has been standardized with unit variance. We can represent this with the formula:

$$
z_{np} = \frac{x_{np} - \bar{x}_{p}}{\sqrt{\sigma_{p}}}
$$ {#eq-1}

Next we define the singular value decomposition of $X$. Let $L$ be the $N \times R$ matrix of left singular vectors of the matrix; that is, the columns of $L$ are made up of the eigenvectors of $XX^T$. Let $R$ be the $P \times R$ matrix of right singular vector; the columns of $R$ are made up of the eigenvectors of $X^TX$. Finally, let $D$ be the diagonal matrix of singular values, meaning the singular values in $D$ are the square roots of the eigenvalues of $XX^T$ and $X^TX$, and $D^2$ is defined as the diagonal matrix of the non-zero eigenvalues. \[ref\] We can define the singular value decomposition of matrix $X$ as:

$$
X = LD{R}^T
$$ {#eq-2}

We can obtain the principal components of $X$ from the SVD. With the identity matrix $I$, the $I \times R$ matrix of factor scores can be expressed as:

$$
F = LD
$$ {#eq-3}

These factor scores are calculated from the coefficients of the linear combinations in matrix $R$, which can defined as a projection matrix of the original observations onto the principal components, i.e. the product of $X$ and $R$:

$$
F = LD = LDR^TR = XR
$$ {eq-4}

The matrix R is also referred to as a loading matrix, and X is often described as the product of the factor score matrix and the loading matrix:

$$
X = FR^T
$$ {eq-5}

with the decomposition of $F^TF = D^2$ and $R^TR = I$. The loadings represent the weights of the original variables in the computation of the principal components; in other words, the correlation from -1 to 1 of each variable with the factor score.

In a geometric interpretation of PCA, the factor scores measure length on the Cartesian plane. This length represents the projection of the original observations onto the PCs from the origin at $(0, 0)$. This is especially useful as a visualization of higher dimension data in two dimensions by utilizing the first two principal components which capture the most variance in the original data.