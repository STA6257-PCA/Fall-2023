# Methods {#methods}

The aim of Principal component analysis (PCA) is to reduce the dimensionality of multivariate data while preserving the variability present in the data. The principal components derived from the dataset are orthogonal variables represented by linear combinations of the original variables which maximize variance. The first principal component (PC) captures the most variance, followed by the second orthogonal principal component, and so on. There can be as many PCs as there are variables in the original data, but the technique is typically used to simplify high-dimension data for improved interpretability. Principal components can be calculated using eigenvalue decomposition or the singular value decomposition (SVD) of the data matrix, so data must be preproccesed and several assumptions met for PCA to yield meaningful results.

## Assumptions

For PCA to be effective, the data should be continuous (although adaptations of PCA exist for other numeric data structures) and normally distributed, although the the distribution of the data does not truly matter when utilizing PCA as an exploratory methodology. More importantly, the data should be linearly related or the linear combinations of the principal components cannot meaningfully capture the variance of the data. Ideally, the variables should be similar in scale, and free from extreme outliers, or missing values, although this can be addressed in preprocessing, and implementations of PCA such as robust PCA have been developed to address these challenges. @jolliffe2016principal

## Preprocessing

Preprocessing data for PCA is straightforward. Missing data should be handled using a method appropriate for the dataset, such as imputation based on the mean or median of the variable observations. After this the variables should be centered and scaled, to a mean of 0 and a standard deviation of 1, although statistical software libraries for SVD and PCA may include this as an option within the function. @prcomp2023ref

## Eigenvectors

PCA uses eigenvectors and their corresponding eigenvalues to calculate the principal components; a brief overview is given here. Eigen is a German word meaning *inherent* or *characteristic*, and an eigenvector can be described geometrically as a nonzero vector $a$ of a linear transformation matrix $M$ which does not change direction when the transformation is applied; the only change that occurs is a scaling by factor $\lambda$, the eigenvalue of the eigenvector $a$. Such a characteristic vector is useful in PCA, where the goal is to maximize variance while reducing dimensionality, and in this context the eigenvectors and eigenvalues can be thought of as the inherent components of the dataset which contain the most important information. Eigenvalues can be calculated from the characteristic polynomial of the matrix, by taking the determinant of $M - \lambda I$, where $I$ is the identity matrix. Setting this expression equal to zero allows the calculation of the eigenvalues as the roots of the characteristic polynomial; the resulting equation is called the characteristic equation:

$$
det(M - \lambda I) = 0
$$ {#eq-1}

An eigenvalue $\lambda_k$ can be used to solve for some eigenvector $a_k$ with the equation $(M - \lambda I)a = 0$. With PCA, we can use the eigenvectors of the covariance matrix to compute the PCs. @bennett2021linear

## Principal Component Analysis

In this approach to PCA, SVD is used to extract the most information (variance) from the data matrix while reducing the dimensionality of the data. The first principal component will have the largest possible variance (also called inertia), whose value is defined as a factor score. Factor scores represent a geometric projection of the observations onto the PCs. The second PC, orthogonal to the first, has the second largest variance, and the third PC would continue this pattern. The calculation of PCs via SVD can be understood with the use of matrix operations on a dataset. @hopcroft2014foundations

### Centering and Scaling

Let our dataset be represented by the $N \times P$ matrix $X$ comprised of $N$ observations of $P$ variables in the data set, where any element $x_{np}$ represents the $n$th observation of variable $p$ in the dataset. The matrix $X$ has rank $A$ where $A \leq min\{N, P\}$. The data in $X$ is centered and scaled, such that the mean of each column $X_p$ is 0 and every $x_{np}$ has been standardized with scaled unit variance. We can represent this with the formula:

$$
z_{np} = \frac{x_{np} - \bar{x}_{p}}{{\sigma_{p}}}
$$ {#eq-2}

### Eigendecomposition of the Covariance Matrix

The aim of PCA is to find some linear combination of the columns of $X$ which maximizes the variance. If we define $a$ as a vector of constants $a_1, a_2, a_3, â€¦, a_p$, then $Xa$ represents the linear combination of interest. The variance of $Xa$ is represented by $var(Xa) = a^TSa$, with the covariance matrix $S$, and $T$ representing the transpose operator. Finding the $Xa$ with maximum variance equates to finding the vector $a$ which maximizes the quadratic $a^TSa$, where $a^Ta = 1$. We can write this as $a^TSa - \lambda(a^Ta-1)$, with the Lagrange multiplier $\lambda$. @luenberger1997optimization Equating this expression to the null vector $0$ allows us to differentiate with respect to $a$:

$$
Sa - \lambda a = 0 \Rightarrow Sa = \lambda a
$$ {#eq-3}

Therefore, $a$ is a unit-norm eigenvector with eigenvalue $\lambda$ of the covariance matrix $S$. The largest eigenvalue of $S$ is $\lambda_1$ with the eigenvector $a_1$, which we can define for any eigenvector $a$:

$$
var(Xa) = a^TSa = \lambda a^Ta = \lambda
$$ {#eq-4}

Any $p \times p$ real symmetric matrix has exactly $p$ real eigenvalues $\lambda_k$ for $k = 1,...,p$. The corresponding eigenvectors of these eigenvalues can be defined to form an orthonormal set of vectors such that $a_k^Ta_{k^T} = 1$ if $k = k^T$ and zero otherwise. If we consider that $S$ is such a matrix and impose the restriction of orthogonality to the different coefficient vectors of $S$, the full set of eigenvectors of $S$ represent the solutions to finding linear combinations $Xa_k$ which maximize variance while minimizing correlation with prior linear combinations. $Xa_k$ then represent the linear combinations which are the principal components of the dataset with eigenvectors $a_k$ and eigenvalues $\lambda_k$. The elements of $Xa_k$ are the factor scores of the PCs, while the elements of the eigenvectors $a_k$ represent the loadings of the PCs. @jolliffe2016principal

### Singular Value Decomposition

Next we define the singular value decomposition of $X$. Let $L$ be the $N \times A$ matrix of left singular vectors of the matrix; that is, the columns of $L$ are made up of the eigenvectors of $XX^T$. Let $R$ be the $P \times A$ matrix of right singular vectors; the columns of $R$ are made up of the eigenvectors of $X^TX$. Finally, let $D$ be the diagonal matrix of singular values, meaning the singular values in $D$ are the square roots of the eigenvalues of $XX^T$ and $X^TX$, and $D^2$ is defined as the diagonal matrix of the non-zero eigenvalues. We can define the singular value decomposition of matrix $X$ as:

$$
X = LD{R}^T
$$ {#eq-5}

In this context, the eigenvalues represent the variances of the principal components and summarily contain the important information for the dataset, and we can obtain the PCs of $X$ from the SVD. @abdi2010principal With the identity matrix $I$, the $I \times R$ matrix of factor scores can be expressed as:

$$
F = LD
$$ {#eq-6}

These factor scores are calculated from the coefficients of the linear combinations in matrix $R$, which can be defined as a projection matrix of the original observations onto the PCs, i.e. the product of $X$ and $R$:

$$
F = LD = LDR^TR = XR
$$ {#eq-7}

The matrix $R$ is also referred to as a loading matrix, and $X$ is often described as the product of the factor score matrix and the loading matrix:

$$
X = FR^T
$$ {#eq-8}

with the decomposition of $F^TF = D^2$ and $R^TR = I$.

The loadings represent the weights of the original variables in the computation of the PCs; in other words, the correlation from -1 to 1 of each variable with the factor score.

In a geometric interpretation of PCA, the factor scores measure length on the Cartesian plane. This length represents the projection of the original observations onto the PCs from the origin at $(0, 0)$. This is especially useful as a visualization of higher dimension data in two dimensions by utilizing the first two PCs which capture the most variance in the original data. @lever2017points

## Interpretation of the Principal Components

There are several ways to interpret the PCs derived from the analysis. Since the eigenvalues represent the variance of the PCs, the proportion of the eigenvalues explain the proportion of variation in the dataset. Using a scree plot, these eigenvalues are plotted to show how much variation each PC explains. Another commonly used tool is a biplot, a combination of the plots of the factor scores (points) and the loadings (vectors) for two PCs (typically PC1 and PC2). The biplot is meant to visually capture the relationship between the original variables and the principal components. Clusters of points represent highly correlated variables, and vector lengths represent the variability captured in that direction on the principal component axis. While many methods and tools exist to interpret the results of PCA, the usefulness of each depends on the needs of the analysis. @bennett2021linear
