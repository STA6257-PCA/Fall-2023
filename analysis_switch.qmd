# Analysis

```{r, warning = FALSE, message = FALSE, echo=FALSE}
# Clear environment
rm(list = ls())

# Load libraries
library(tidyverse)  #rename; pipe operator
library(olsrr)      #ols_plot_resid_lev; outliers, leverage plot
library(caret)      #findCorrelation
library(kableExtra) #kable_paper; table formatting
library(sjPlot)     #tab_model; format linear regression results
library(factoextra) #fviz_eig; PCA plots
library(caTools)    #sample.split
library(ggcorrplot) #ggcorrplot; correlation graph
library(pls)        #pcr; PC regression
library(Metrics)    #mae; calculate MAE, MSE, RMSE, R^2
# library(htmlwidgets)#html as text
library(htmltools)  #html render
```

## Data Preparation

This analysis uses the Dialysis dataset introduced previously. Within this context, we restructured the variables, enhancing their meaningfulness and facilitating inferential analysis. Additionally, the mean imputation approach addressed missing values.

## Assumptions

PCA relies on certain assumptions, the fulfillment of which is essential for the validity of this technique.

-   **Linearity**: PCA assumes that the dataset is a linear combination of the variables. The variables exhibit relationships among themselves [@chumney2012pcaefa].

-   **Importance of mean and covariance**: PCA assumes that the directions of maximum variance will contain good features for discrimination.

-   **Correlation between features**: PCA assumes a correlation between features.

-   **Normalization of data**: Normalization of data is necessary to apply PCA. Unscaled data can cause relative comparison problems of the dataset [@chumney2012pcaefa]. In addition, the data should contain no significant outliers.

-   **Orthogonality**: The principal components are orthogonal to each other.

-   **Sampling adequacy**: PCA assumes that there is sampling adequacy.

```{r, echo=FALSE, results='hide'}
# Load dataset
train_original <- read.csv('dataset/DFC_STATE.csv')

# Make a working copy
train_data <- train_original

# Constant seed
my_seed = 95
```

```{r, echo=FALSE, results='hide'}
# Rename Variables
train_data <- rename(train_data,
                      better_transfusion = Transfusions..Better.than.expected..STATE.,
        expected_transfusion = Transfusions..As.expected..STATE.,
        worse_transfusion = Transfusions..Worse.than.expected..STATE.,
        better_infection = Infection..Better.than.expected..STATE.,
        expected_infection = Infection..As.expected..STATE.,
        worse_infection = Infection..Worse.than.expected..STATE.,
        Kt_v_1.2 = Percentage.of.adult.HD.patients.with.Kt.V..1.2,
        Kt_v_1.7 = Percentage.Of.Adult.PD.Patients.With.Kt.V..1.7,
        pedriatic_Kt_v_1.2 = Percentage.Of.Pediatric.HD.Patients.With.Kt.V..1.2,
        pediatric_Kt_v_1.8 = Percentage.Of.Pediatric.PD.Patients.With.Kt.V..1.8,
        pediatric_nPCR = Percentage.Of.Pediatric.HD.Patients.With.nPCR.In.Use,
        better_fistula = Fistula.Rate...Better.Than.Expected..STATE.,
        expected_fistula = Fistula.Rate...As.Expected..STATE.,
        worse_fistula = Fistula.Rate...Worse.Than.Expected..STATE.,
        long_term_catheter = Percentage.Of.Adult.Patients.With.Long.Term.Catheter.In.Use ,
        "hypercalcemia_calcium > 10.2Mg" = Percentage.Of.Adult.Patients.With.Hypercalcemia..Serum.Calcium.Greater.Than.10.2.Mg.dL.,
        "phosphorus < 3.5Mg" = Percentage.Of.Adult.Patients.With.Serum.Phosphorus.Less.Than.3.5.Mg.dL,
        "phosphorus (3.5 - 4.5) Mg" = Percentage.Of.Adult.Patients.With.Serum.Phosphorus.Between.3.5.4.5.Mg.dL,
        "phosphorus (4.6 - 5.5) Mg" = Percentage.Of.Adult.Patients.With.Serum.Phosphorus.Between.4.6.5.5.Mg.dL,
        "phosphorus (5.6 - 7) Mg" = Percentage.Of.Adult.Patients.With.Serum.Phosphorus.Between.5.6.7.0.Mg.dL,
        "phosphorus > 7Mg" = Percentage.Of.Adult.Patients.With.Serum.Phosphorus.Greater.Than.7.0.Mg.dL,
        better_hospitalization = Hospitalizations..Better.Than.Expected..STATE.,
        expected_hospitalization = Hospitalizations..As.Expected..STATE.,
        worse_hospitalization = Hospitalizations..Worse.Than.Expected..STATE.,
        better_hospital_readmission = Hospital.Readmission...Better.Than.Expected..STATE.,
        expected_hospital_readmission = Hospital.Readmission...As.Expected..STATE.,
        worse_hospital_readmission = Hospital..Readmission...Worse.Than.Expected..STATE.,
        better_survival = Survival..Better.Than.Expected..STATE.,
        expected_survival = Survival..As.Expected..STATE.,
        worse_survival = Survival..Worse.Than.Expected..STATE.,
        incident_transplant_waitlist_better = Incident.Patients.Transplant.Waitlisting..Better.Than.Expected..STATE.,
        incident_transplant_waitlist_expected = Incident.Patients.Transplant.Waitlisting...As.Expected..STATE.,
        incident_transplant_waitlist_worse = Incident.Patients.Transplant.Waitlisting...Worse.Than.Expected..STATE.,
        prevalent_transplant_waitlist_better = Prevalent.Patients.Transplant.Waitlisting..Better.Than.Expected..STATE.,
        prevalent_transplant_waitlist_expected = Prevalent.Patients.Transplant.Waitlisting...As.Expected..STATE.,
        prevalent_transplant_waitlist_worse = Prevalent.Patients.Transplant.Waitlisting...Worse.Than.Expected..STATE.,
        "Hgb 10g"= Percentage.Of.Patients.With.Hgb.10.g.dL,
        "Hgb 12g" = Percentage.of.patients.with.Hgb.12.g.dL
        )
```

```{r, echo=FALSE, results='hide'}
# Impute Missing Values
# Count the number of missing values in each column
colSums(is.na(train_data))

# Remove categorical columns
train_data$State <- NULL

# Impute missing values with the mean
for (col in colnames(train_data)) {
  mean_value <- mean(train_data[[col]], na.rm = TRUE)
  train_data[[col]] <- ifelse(is.na(train_data[[col]]), mean_value,     train_data[[col]])
}
```

```{r, echo=FALSE, results='hide'}
# Remove rows with missing values
train_data <- na.omit(train_data)

# Count the number of missing values in each column
colSums(is.na(train_data))

# Get duplicated rows
train_data[duplicated(train_data), ]

# Summary statistics of the data
summary(train_data)

# Summary statistics of categorical variables
summary(train_data[, sapply(train_data, is.character)])
```

```{r, echo=FALSE, results='hide'}
# Check dataset structure
str(train_data)
```

## Feature Scaling

### Standardization

It is important to Mean-Center the data prior to PCA model building to ensure the first Principal Component is in the direction of maximum variance.

-   Standardization produces Mean, $\mu$= 0, and Standard Deviation, $\sigma$ = 1.

-   We can rewrite this as:

    $$
    Z = \frac{{x - \mu}}{{\sigma}}
    $$

    $$
    Z \sim N(0,1)
    $$

```{r}
# Find the index position of the target feature 
target_name <- "expected_survival"
target_index <- grep(target_name, 
                     colnames(train_data))
```

```{r}
# Standardization Numerical Features
train_data_sc <- scale(train_data[, -target_index])

```

## PCA Requirements

### Outliers Detection

-   There are some outliers in the data frame including the dependent variable.
-   However, there are three outliers with no high leverage.
-   Outliers are important because these can have a disproportionate influence on results.

```{r}
# Plot a boxplot to visualize potential outliers
boxplot(train_data, main = "Outliers Detection",
        col = "steelblue")
```

```{r}
# Dependent Variable outliers
train_data %>% 
  ggplot(aes(y=expected_survival)) +
  geom_boxplot(fill="steelblue", alpha=0.75) + 
  xlab("Expected Survival")+
  ylab("")+
  ggtitle("Dependent Variable Outliers")
```

### Leverage

```{r, results='hide', fig.show='asis'}
set.seed(my_seed)

# Fit regression model
ordinary_model <- lm(expected_survival ~ ., data = train_data)

# Print the model summary
summary(ordinary_model)

# Residual Diagnostics
ols_plot_resid_lev(ordinary_model)
```

### Removing Outliers

After removing one outlier from the dataset, we discovered that the final results produced minimal variability. Therefore, we decided to present a model that captures all the data points' information; thus, no observations will be removed from the final model.

```{r, results='hide', fig.show='asis'}
# No Outliers subset
no_outliers_df <- slice(train_data, -c(56))

set.seed(my_seed)

# Fit regression model
no_outliers_model <- lm(expected_survival ~ ., data = no_outliers_df)

# Print the model summary
summary(no_outliers_model)

# Residual Diagnostics
ols_plot_resid_lev(no_outliers_model)
```

### Correlations

-   Multicollinearity is present in the data set.

-   28 Correlated features were identified using a threshold = 0.30.

```{r}
# Calculate correlations and round to 2 digits
corr_matrix <- cor(train_data_sc)
corr_matrix <- round(corr_matrix, digits = 2)

# Print names of highly correlated features; threshold > 0.30
high <- findCorrelation(corr_matrix, cutoff = 0.30, names = TRUE)

# Create a data frame with an index column
high_corr_df <- data.frame(
  Count = 1:length(high),
  Feature = high
)

# table format
correlations_table <- kbl(high_corr_df, caption = "Highly Correlated Features") %>%
  kable_paper("hover")

# Capture the HTML of the styled training summary
styled_correlations_table_html <- htmltools::HTML(as.character(correlations_table))
```

```{r}
# Adding toggle switch
styled_corr <- sprintf('
<p id="result-corr">Show/Hide Results: </p> 
<div class="switch">
  <input type="checkbox" id="toggleCorr">
  <label class="slider round" for="toggleCorr"></label>
</div>

<div id="result-container-correlations">
  
  <div id="corrTable" style="display: none;">%s</div>
</div>

<script>
document.getElementById("toggleCorr").addEventListener("change", function() {
  toggleCorrResult();
});

function toggleCorrResult() {
  var resultContainer = document.getElementById("result-container-correlations");
  var corrTable = document.getElementById("corrTable");
  var corrSwitch = document.getElementById("toggleCorr");

  if (corrSwitch.checked) {
    resultContainer.style.display = "block";
    corrTable.style.display = "block";
  } else {
    resultContainer.style.display = "none";
    corrTable.style.display = "none";
  }
}
</script>', styled_correlations_table_html)

htmltools::HTML(styled_corr)
```


```{r}
corrplot::corrplot(corr_matrix, type = "full", tl.pos = "n")
```

## Full Model Regression

-   The Adjusted $R^2$ = 99.99% is an indication of over-fitting, or high variance.
-   This dataset is suitable to apply PCA to find the adequate variance balance.

```{r, results='markup'}
set.seed(my_seed)

# Fit a multiple linear regression model
full_model <- lm(expected_survival ~ ., data = train_data)

# Print a summary of the regression model
full <- tab_model(full_model, title = "Full Model Regression", 
          string.p="P-value", string.stat = "T-score",
          string.se = "Std. Error",
          string.resp = "Response",
          string.ci = "Conf Int.",
          show.se=T, show.stat = T,
           CSS = list(
             css.depvarhead = 'font-weight: bold; text-align: left;',
             css.summary = 'color: #10759B; font-weight: bold;'
           ))
full
```

```{r}
# # Adding the toggle switch
# full_html_out <- sprintf('
# <p id="result-full">Show/Hide Results: </p> 
# <div class="switch">
#   <input type="checkbox" id="togglefull">
#   <label class="slider round" for="togglefull"></label>
# </div>
# 
# <div id="result-container-full">
#   <div id="fullTable" style="display: none;">%s</div>
# </div>
# 
# <script>
# document.getElementById("togglefull").addEventListener("change", function() {
#   togglefullResult();
# });
# 
# function togglefullResult() {
#   var resultContainer = document.getElementById("result-container-full");
#   var fullTable = document.getElementById("fullTable");
#   var fullSwitch = document.getElementById("togglefull");
# 
#   if (fullSwitch.checked) {
#     resultContainer.style.display = "block";
#     fullTable.style.display = "block";
#   } else {
#     resultContainer.style.display = "none";
#     fullTable.style.display = "none";
#   }
# }
# </script>', full)
# 
# htmltools::HTML(full_html_out)

```


## PCA Implementation

### SVD - Singular Value Decomposition

We will focus on Singular Value Decomposition which is a classic approach for PCA analysis.

Singular Value Decomposition is a factorization technique used in linear algebra to decompose a matrix into three matrices.

1.  $L$: A matrix whose columns are the left singular vectors of the original matrix.

2.  $D$: A diagonal matrix whose entries are the singular values of the original matrix.

3.  $R$: A matrix whose columns are the right singular vectors of the original matrix.

The SVD is closely related to the eigenvalue decomposition (EVD), which is another factorization technique used in linear algebra. While the EVD can only be applied to square matrices, the SVD can be applied to any matrix, including rectangular matrices. The SVD is also more numerically stable than the EVD, making it a preferred method for many applications.

-   Note: The Spectral Decomposition approach is used with the princomp() function.

```{r}
# Apply PCA using prcomp()
data_pca <- prcomp(train_data_sc, center = TRUE, scale. = TRUE)
pca_summ <- summary(data_pca)$importance

# Transpose the matrix
transposed_pca_summ <- t(pca_summ)

# table format
pca_summary <- kbl(transposed_pca_summ, caption = "Pricipal Components Analysis",
    digits = 4) %>%
  kable_paper("hover")

# Capture the HTML of the styled training summary
pca_summary_html <- htmltools::HTML(as.character(pca_summary))
```

```{r}
# Adding toggle switch
styled_pca_summary <- sprintf('
<p id="result-pca">Show/Hide Results: </p> 
<div class="switch">
  <input type="checkbox" id="togglepca">
  <label class="slider round" for="togglepca"></label>
</div>

<div id="result-container-pca">
  
  <div id="pcaTable" style="display: none;">%s</div>
</div>

<script>
document.getElementById("togglepca").addEventListener("change", function() {
  togglepcaResult();
});

function togglepcaResult() {
  var resultContainer = document.getElementById("result-container-pca");
  var pcaTable = document.getElementById("pcaTable");
  var pcaSwitch = document.getElementById("togglepca");

  if (pcaSwitch.checked) {
    resultContainer.style.display = "block";
    pcaTable.style.display = "block";
  } else {
    resultContainer.style.display = "none";
    pcaTable.style.display = "none";
  }
}
</script>', pca_summary_html)

htmltools::HTML(styled_pca_summary)
```


### PCA - Elements

-   The values in \`**`` data_pca$x` ``** are the coordinates of each observation in the new principal component space. These coordinates are the scores for each observation along each principal component.

-   The eigenvectors of the covariance, or correlation matrix of the data represent the directions of maximum variance, or information in the dataset.

```{r}
# Principal Component scores vector
pc_scores <- data_pca$x

# Std Deviation of Components
component_sdev <- data_pca$sdev

# Eigenvector, or Loadings
pc_loadings <- data_pca$rotation

# Mean of variables
component_mean <- data_pca$center 

# Scaling factor of Variables
component_scale <- data_pca$scale

```

### Loadings of First Two Components

-   The loading \`data_pca\$rotation\` are the weights assigned to each original variable for that particular principal component.

```{r}
# Access the loadings for the first two principal components
loadings_first_two_components <- pc_loadings[, 1:2]

# Print the loadings for the first two principal components
# print(loadings_first_two_components)

# table format
pc2_loadings <- kbl(loadings_first_two_components, 
    caption = "Loadings of First Two Principal Components",
    digits = 4) %>%
  kable_paper("hover")

# Capture the HTML of the styled training summary
pc2_loadings_html <- htmltools::HTML(as.character(pc2_loadings))
```

```{r}
# Adding the toggle switch
pc2_loadings_out <- sprintf('
<p id="result-pc2">Show/Hide Results: </p> 
<div class="switch">
  <input type="checkbox" id="togglepc2">
  <label class="slider round" for="togglepc2"></label>
</div>

<div id="result-container-pc2">
  
  <div id="pc2Table" style="display: none;">%s</div>
</div>

<script>
document.getElementById("togglepc2").addEventListener("change", function() {
  togglepc2Result();
});

function togglepc2Result() {
  var resultContainer = document.getElementById("result-container-pc2");
  var pc2Table = document.getElementById("pc2Table");
  var pc2Switch = document.getElementById("togglepc2");

  if (pc2Switch.checked) {
    resultContainer.style.display = "block";
    pc2Table.style.display = "block";
  } else {
    resultContainer.style.display = "none";
    pc2Table.style.display = "none";
  }
}
</script>', pc2_loadings_html)

htmltools::HTML(pc2_loadings_out)
```


### PCA - Cumulative Variance
The cumulative measure helps determine the proportion of total variance explained by a certain number of components. A high cumulative variance indicates that a significant amount of the dataset's variability is retained by considering those components.

```{r}
# Proportion of variance explained by each PC
variance_explained <- component_sdev^2 / sum(component_sdev^2)

# Cumulative proportion of variance explained
cumulative_variance_explained <- cumsum(variance_explained)

# Create a data frame with an index column
cumulative_variance_explained_df <- data.frame(
  Pincipal_Component = 1:length(cumulative_variance_explained),
  Cumulative_Variance_Explained = cumulative_variance_explained
)

# Create a kable table with an index column
cumulative <- kbl(cumulative_variance_explained_df, align = "cl",
    caption = "PCA: Cumulative Variance Explained",
    digits = 4) %>%
  kable_paper("hover")

# Capture the HTML of the styled training summary
cumulative_html <- htmltools::HTML(as.character(cumulative))
```

```{r}
# Adding the toggle switch
cummulative_html_out <- sprintf('
<p id="result-cummulative">Show/Hide Results: </p> 
<div class="switch">
  <input type="checkbox" id="togglecummulative">
  <label class="slider round" for="togglecummulative"></label>
</div>

<div id="result-container-cummulative">
  
  <div id="cummulativeTable" style="display: none;">%s</div>
</div>

<script>
document.getElementById("togglecummulative").addEventListener("change", function() {
  togglecummulativeResult();
});

function togglecummulativeResult() {
  var resultContainer = document.getElementById("result-container-cummulative");
  var cummulativeTable = document.getElementById("cummulativeTable");
  var cummulativeSwitch = document.getElementById("togglecummulative");

  if (cummulativeSwitch.checked) {
    resultContainer.style.display = "block";
    cummulativeTable.style.display = "block";
  } else {
    resultContainer.style.display = "none";
    cummulativeTable.style.display = "none";
  }
}
</script>', cumulative_html)

htmltools::HTML(cummulative_html_out)
```


### PCA - Number of Principal Components

-   We can conclude that 9 Principal Components explain 86% of the variance.

```{r}
# Retain components that explain a percentage of the variance
num_components <- which(cumulative_variance_explained >= 0.86)[1]

# Select the desired number of principal components
selected_pcs <- pc_scores[, 1:num_components]

# table format
variance_pc <- kbl(selected_pcs, caption = "Components Explaining 86% Variance",
    digits = 4) %>%
  kable_paper("hover", full_width = F)

# Capture the HTML of the styled training summary
variance_pc_html <- htmltools::HTML(as.character(variance_pc))
```

```{r}
# Adding the toggle switch
variance_pc_html_out <- sprintf('
<p id="result-variance">Show/Hide Results: </p> 
<div class="switch">
  <input type="checkbox" id="togglevariance">
  <label class="slider round" for="togglevariance"></label>
</div>

<div id="result-container-variance">
  
  <div id="varianceTable" style="display: none;">%s</div>
</div>

<script>
document.getElementById("togglevariance").addEventListener("change", function() {
  togglevarianceResult();
});

function togglevarianceResult() {
  var resultContainer = document.getElementById("result-container-variance");
  var varianceTable = document.getElementById("varianceTable");
  var varianceSwitch = document.getElementById("togglevariance");

  if (varianceSwitch.checked) {
    resultContainer.style.display = "block";
    varianceTable.style.display = "block";
  } else {
    resultContainer.style.display = "none";
    varianceTable.style.display = "none";
  }
}
</script>', variance_pc_html)

htmltools::HTML(variance_pc_html_out)
```


## Visualization

### Scree Plot - Cumulative Variance Explained

-   PC1 explains 40.8% variance.

-   PC2 explains 9.5% variance.

```{r}
fviz_eig(data_pca, addlabels = TRUE)
```

### Biplot

-   The correlation between a variable and a principal component (PC) is used as the coordinates of the variable on the PC. The representation of variables differs from the plot of the observations: The observations are represented by their projections, but the variables are represented by their correlations @abdi2010principal.

-   PC1 is represented in black which displays the longest distance of its projection.

-   PC2 is represented in blue which displays a shorter distance as expected.

```{r}
fviz_pca_biplot(data_pca, 
                geom = c("point", "arrow"),
                geom.var = "arrow")
```

### Correlation Circle

The plot below is also known as variable correlation plots. It shows the relationships between all variables. It can be interpreted as follow:

-   Positively correlated variables are grouped together.

-   Negatively correlated variables are positioned on opposite sides of the plot origin (opposed quadrants).

-   The distance between variables and the origin measures the quality of the variables on the factor map. Variables that are away from the origin are well represented on the factor map.

```{r}
# Control variable colors using their contributions
fviz_pca_var(data_pca, col.var = "contrib",
   gradient.cols = c("white", "blue", "red"),
   geom.var = "arrow",
   ggtheme = theme_minimal())
```

### Variable Contribution

Top variable contribution for the first two principal components.

```{r}
# Contributions of variables to PC1
pc2_contribution <- fviz_contrib(data_pca, choice = "var", axes = 1, top = 20)

# Modify the theme to rotate X-axis labels to 90 degrees
pc2_contribution +
  theme(
    axis.text.x = element_text(angle = 0),
    plot.title = element_text(hjust = 0)  # horizontal justification
  ) +
  coord_flip() +
  labs(title = "Contribution of Variables to PC1",
       y = "Percentage Contribution",
       x = "",
       caption = "PC1 explains 40.8% variance") +
  scale_y_continuous(labels = scales::percent_format(scale = 1,
                                                     accuracy = 1))


# Contributions of variables to PC2
pc2_contribution <- fviz_contrib(data_pca, choice = "var", axes = 2, top = 12)

# Modify the theme to rotate X-axis labels to 90 degrees
pc2_contribution +
  theme(
    axis.text.x = element_text(angle = 0),
    plot.title = element_text(hjust = 0)  # horizontal justification
  ) +
  coord_flip() +
  labs(title = "Contribution of Variables to PC2",
       y = "Percentage Contribution",
       x = "",
       caption = "PC2 explains 9.5% variance") +
  scale_y_continuous(labels = scales::percent_format(scale = 1,
                                                     accuracy = 1))
```

## Model Building

### Data Splitting into Training & Test set

```{r}
# reproducible random sampling
set.seed(my_seed)  
 
# Create Target y-variable for the training set
y <- train_data$expected_survival  
# Split the data into training and test sets 
split <- sample.split(y, SplitRatio = 0.7) 
training_set <- subset(train_data, split == TRUE) 
test_set <- subset(train_data, split == FALSE) 
```

### Feature Scaling: Standardization

-   Standardization ensures all features are on the same scale, and this method is less sensitive to outliers.

```{r}
# Feature Scaling: Standardization
# Perform centering and scaling on the training and test sets
sc <- preProcess(training_set[, -target_index], 
                 method = c("center", "scale"))
training_set[, -target_index] <- predict(
  sc, training_set[, -target_index])
test_set[, -target_index] <- predict(sc, test_set[, -target_index])

```

### Applying PCA to Training & Test sets

```{r}
# Perform Principal Component Analysis (PCA) preprocessing on the training data
pca <- preProcess(training_set[, -target_index], 
                  method = 'pca', pcaComp = 8)

# Apply PCA transformation to original training set
training_set <- predict(pca, training_set)

# Reorder columns, moving the dependent feature index to the end
training_set <- training_set[c(2:9, 1)]

# Apply PCA transformation to original test set
test_set <- predict(pca, test_set)

# Reorder columns, moving the dependent feature index to the end
test_set <- test_set[c(2:9, 1)]

```

```{r, echo=FALSE}
#PRESS & Predicted $R^2$ Functions
#PRESS - predicted residual sums of squares
PRESS <- function(linear.model) {
  # calculate the predictive residuals
  pr <- residuals(linear.model)/(1-lm.influence(linear.model)$hat)
  # calculate the PRESS
  PRESS <- sum(pr^2)
  
  return(PRESS)
}

pred_r_squared <- function(linear.model) {
  # Use anova() to get the sum of squares for the linear model
  lm.anova <- anova(linear.model)
  # Calculate the total sum of squares
  tss <- sum(lm.anova$'Sum Sq')
  # Calculate the predictive R^2
  pred.r.squared <- 1-PRESS(linear.model)/(tss)
  
  return(pred.r.squared)
}
```

## PCA Full Model: 8 Principal Components

```{r}
# reproducible random sampling
set.seed(my_seed)

# Fit a multiple linear regression model
pca_full_model <- lm(expected_survival ~ ., data = training_set)

# Print a summary of the regression model
tab_model(pca_full_model, title = "8 Principal Components Regression",
          string.p="P-value", string.stat = "T-score",
          string.se = "Std. Error",
          string.resp = "Response",
          string.ci = "Conf Int.",
          show.se=T, show.stat = T,
          CSS = list(
             css.depvarhead = 'font-weight: bold; text-align: left;',
             css.summary = 'color: #10759B; font-weight: bold;'
           ))

# Calculate PRESS
# cat("PRESS: ", PRESS(pca_full_model), "\n")
PRESS_8pc <- PRESS(pca_full_model)
# Calculate predicted R^2
# cat("Predicted R^2: ", pred_r_squared(pca_full_model), "\n")
R2_8pcs <- pred_r_squared(pca_full_model)

predict_8pc <- cbind(PRESS_8pc, R2_8pcs)
# Print PRESS, predicted R^2
# table format
kbl(predict_8pc, caption = "8 Principal Components Prediction Metrics",
    digits = 4) %>%
  kable_paper("hover", full_width = F)
```

### Visualization of Uncorrelated PCA Matrix

```{r}
# Visual of Principal Components un-correlation
corr_matrix <- cor(training_set)
ggcorrplot(corr_matrix)
```

## PCA: 2 Principal Components

```{r}
# Create a subset with 2 principal components
significant_pcs = c(1,2,9)
train_pca <- training_set[, significant_pcs]
test_pca <- test_set[, significant_pcs]
```

```{r}
# reproducible random sampling
set.seed(my_seed)

# Fit a multiple linear regression model
reg_model <- lm(expected_survival ~ ., 
                data = train_pca)

# Print a summary of the regression model
tab_model(reg_model, title = "2 Principal Components Regression",
          string.p="P-value", string.stat = "T-score",
          string.se = "Std. Error",
          string.resp = "Response",
          string.ci = "Conf Int.",
          show.se=T, show.stat = T,
          CSS = list(
             css.depvarhead = 'font-weight: bold; text-align: left;',
             css.summary = 'color: #10759B; font-weight: bold;'
           ))

# Calculate PRESS
# cat("PRESS: ", PRESS(reg_model), "\n")
PRESS_2pc <- PRESS(reg_model)

# Calculate predicted R^2
# cat("Predicted R^2: ", pred_r_squared(reg_model), "\n")
R2_2pc <- pred_r_squared(reg_model)

# Print 2PC prediction results
predict_2pc <- cbind(PRESS_2pc, R2_2pc)

# table format
kbl(predict_2pc, caption = "2 Principal Components Prediction Metrics",
    digits = 4) %>%
  kable_paper("hover", full_width = F)
```

### **Principal Components Regression**

-   PCA is used to calculate principal components that can then be used in principal components regression. This type of regression is often used when multicollinearity exists between predictors in a data set.

```{r}
# reproducible random sampling
set.seed(my_seed)

y = train_pca$expected_survival

# fit PCR
pcr_model <- pcr(y ~ PC1+PC2, data=train_pca, validation="CV")
print(pcr_model)

# table format
# kbl(pcr_model$residuals, caption = "PCA Residuals",
#     digits = 4) %>%
#   kable_paper("hover", full_width = F)
```

### PCA: Cross-Validation Model

```{r}
# reproducible random sampling
set.seed(my_seed)

# Cross-validation with n folds
k_10 <- trainControl(method = "cv", number = 10)

# training the model 
model_cv <- train(expected_survival ~ ., 
                  data = train_pca,
                  method = "lm",
                  trControl = k_10)

# Print Model Performance
print(model_cv)

# Metrics
cv_results = model_cv$results
kbl(cv_results, caption = "PCA: Cross-Validation Metrics",
    digits = 4) %>%
  kable_paper("hover", full_width = F)
```

## Predictions

```{r}
# Find the index position of the target feature
pred_target_index <- grep(target_name, 
                     colnames(test_pca))
#cat("Target Feature Index =", pred_target_index)

# Create Predicted Target Feature (y-test) 
y_test <- test_pca[pred_target_index]
```

```{r}
# Predictions using the Cross-Validation model
y_pred = predict(model_cv, newdata = test_pca[, -pred_target_index])

```

```{r}
# Prediction Results from y_predictions
y_pred <- round(y_pred, digits = 0)

```

```{r}
# Transform y_test from data frame to numeric
y_test <- as.numeric(unlist(y_test))

prediction_comparison <- cbind(y_pred, y_test)
# table format
kbl(prediction_comparison) %>%
  kable_paper("hover", full_width = F)
```

### Prediction Metrics

```{r}
# Calculate Mean Absolute Error (MAE)
MAE_value <- mae(y_pred, y_test)
#cat("MAE =", mae_value)

# Calculate MSE
MSE_predict <- mean((y_pred - y_test)^2)
#cat("\nMSE =", mse_predict)

# Calculate RMSE
RMSE_predict <- sqrt(mean((y_pred - y_test)^2))
#cat("\nRMSE =", rmse_predict)

# Calculate R-squared (R^2)
predicted_R2 <- 1 - sum((y_test - y_pred)^2) / 
  sum((y_test - mean(y_test))^2)
# cat("\nPredicted R^2 =", predicted_r2)

prediction_metrics_df <- cbind(MAE_value, MSE_predict,
                               RMSE_predict, predicted_R2)
# table format
kbl(prediction_metrics_df, digits = 4) %>%
  kable_paper("hover", full_width = F)
```

## Training Conclusion

In conclusion, this project has demonstrated the effectiveness of Principal Component Analysis (PCA) in dimension reduction with the following key points:

-   PCA was able to reduce from 37 features down to just 2 principal components.

-   The best score of $R^2$ = 97.61% was from the Linear Regression with Cross-validation model.

-   The predicted $R^2$ = 96%

-   The average deviation between the predicted values, and observed values for 'Expected Survival' is RMSE = 33.8.

-   The model has not been exposed to unseen data with a large amount of observations to asses its robustness, and reliability.

