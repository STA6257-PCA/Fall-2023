# Results

The Dialysis dataset consists of 56 observations of 39 variables with 1 discrete variable (States/Territories) and 38 continuous variables. For the principal component analysis, 37 of the 38 continuous variables were used; the variable expected_survival was excluded from PCA in order to use it as a target for model building in the latter part of the analysis.

Principal component analysis was performed using a singular value decomposition approach. Among the resulting principal components, the first PC captures 40.80% of the variance in the data, and the first two principal components capture 50.27% of the variance. The first four PCs capture 67.66% of the variance, or just over two-thirds; after the fourth PC, the variance captured by each successive PC begins to diminish relative to PCs one through four. The first ten PCs capture 88.67% of the variance, and in terms of dimensionality reduction over 90% of the information in the dataset can be explained by only 11 PCs when compared to the original 38 continuous variables.

The variables which contribute the most to PC1 are expected_hospital_readmission, expected_transfusion, and expected_hospitalization, although the percent of total contribution to PC1 by any one variable is not outsized relative to the remaining variables. PC2, which is orthogonal to PC1, has relatively large contributions from the five variables measuring levels of phosphorus; patterns or trends in the data such as these can be further explored using other methodologies after being highlighted in PCA @bro2014principal.

From here, the analysis extends PCA via model building using linear regression with a technique known as principal component regression, with expected_survival used as the response variable. The data is first split into training and testing sets; then the data is centered and scaled, after which PCA is applied to each set. The first model is created for illustrative purposes, using 8 principle components from the training set of 39 observations. The estimates and significance of each PC regressor demonstrates the differences between variance captured from the data and usefulness in a linear model; for example, PC4 is a significant regressor despite capturing less variance than PC3 in the training data.

The analysis concludes by building and comparing two linear regression models using the first two prinicipal components. The first model is a straightforward linear model using the lm() function from the stats package in R @R. The second model uses 10-fold cross-validation for a linear model using the train() and trainControl() functions from the caret package @caret. Both models produce an $R^2$ above 96% and a predicted $R^2$ above 95% with a 1% advantage on the cross-validation model. Although the interpretations of the regressors for these PCA models is different than those of a linear regression on the original variables, the lower dimensionality of the data may be desirable as a more simple model which still captures a large portion of the variance in the original data.
