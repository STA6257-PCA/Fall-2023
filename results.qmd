# Discussion and Results

Principal Component Analysis (PCA) is a foundational multivariate analysis technique that has been widely employed to extract essential information from intricate multivariate datasets and effectively reduce dimensionality. Due to its simplicity and versatility, PCA has become one of the widely adopted tools for understanding and exploring data features in a multivariate dataset. This approach leverages singular value decomposition to restructure datasets and facilitates subsequent statistical analysis. By doing so, PCA simplifies complexity, eliminating superfluous details and redundant information arising from the original dataset. The outcome is a set of principal components that capture most of the variance explaining the original variables. To accomplish this, PCA first converts the dataset into a covariance matrix and then employs singular value decomposition to identify eigenvalues and eigenvectors, representing the loadings of the newly generated principal components. These components can typically account for over 70-80% of the original variables' variances.

PCA's importance in data analysis is underscored by its adaptability to various scenarios and data types, including binary, ordinal, compositional, and discrete data. Moreover, the PCA algorithm has proven effective in reducing the dimensions of vast datasets with high accuracy, significantly improving classification tasks. It plays a crucial role in exploratory data analysis and preliminary data processing, acting as a feature extraction and dimensionality reduction tool. One of its main benefits is its ability to mitigate multicollinearity issues, which can otherwise lead to biased results in statistical analyses.

Despite its numerous advantages, PCA does come with certain limitations. For instance, it is sensitive to the presence of outliers, which can distort the results and compromise its effectiveness. Furthermore, the new features or components generated through PCA are not readily interpretable, making it challenging to explain their meaning in a straightforward manner. Nonetheless, PCA remains a pivotal technique in data analysis, offering a powerful means to navigate complex datasets and uncover their underlying structures.

The Dialysis dataset used in this application of PCA consists of 55 observations of 39 variables with 1 discrete variable (States/Territories) and 38 continuous variables. For the principal component analysis, 37 of the 38 continuous variables were used; the variable expected_survival was excluded from PCA in order to use it as a target for model building in the latter part of the analysis.

Principal component analysis was performed using a singular value decomposition approach. Among the resulting principal components, the first PC captures 40.80% of the variance in the data, and the first two principal components capture 50.27% of the variance. The first four PCs capture 67.66% of the variance, or just over two-thirds; after the fourth PC, the variance captured by each successive PC begins to diminish relative to PCs one through four. The first ten PCs capture 88.67% of the variance, and in terms of dimensionality reduction over 90% of the information in the dataset can be explained by only 11 PCs when compared to the original 38 continuous variables.

The variables which contribute the most to PC1 are expected_hospital_readmission, expected_transfusion, and expected_hospitalization, although the percent of total contribution to PC1 by any one variable is not outsized relative to the remaining variables. PC2, which is orthogonal to PC1, has relatively large contributions from the five variables measuring levels of phosphorus; patterns or trends in the data such as these can be further explored using other methodologies after being highlighted in PCA @bro2014principal.

From here, the analysis extends PCA via model building using linear regression with a technique known as principal component regression, with expected_survival used as the response variable. The data is first split into training and testing sets; then the data is centered and scaled, after which PCA is applied to each set. The first model is created for illustrative purposes, using 8 principle components from the training set of 39 observations. The estimates and significance of each PC regressor demonstrates the differences between variance captured from the data and usefulness in a linear model; for example, PC4 is a significant regressor despite capturing less variance than PC3 in the training data.

The analysis concludes by building and comparing two linear regression models using the first two prinicipal components. The first model is a straightforward linear model using the lm() function from the stats package in R @lm2023ref. The second model uses 10-fold cross-validation for a linear model using the train() and trainControl() functions from the caret package in R @caret2023ref. Both models produce an $R^2$ above 96% and a predicted $R^2$ above 95% with a 1% advantage on the cross-validation model. Although the interpretations of the regressors for these PCA models is different than those of a linear regression on the original variables, the lower dimensionality of the data may be desirable as a more simple model which still captures a large portion of the variance in the original data.
