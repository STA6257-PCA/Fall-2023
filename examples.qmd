# Examples

Moving beyond the theoretical foundations of principal components, how is PCA applied to data? We offer two examples; the first a demonstration of the manual calculation of principal components, and the second implementing PCA on a large dataset using R.

## Manual calculation of principal components

In this illustration, we have access to the two grades of four students in a statistics subject. We aim to employ principal component analysis as a means to reduce the dimensionality from two variables to a singular variable. This transformation will effectively represent students' performance in the subject with a more compact and interpretable measure. This example is adapted from the resource *How to compute principal components* @eduflair2020.

| Scores        | **Basic Stats** | **Advanced Stats** |
|---------------|-----------------|--------------------|
| **Student 1** | **4**           | **11**             |
| **Student 2** | **8**           | **4**              |
| **Student 3** | **13**          | **5**              |
| **Student 4** | **7**           | **14**             |
| **Mean**      | $\bar{x}$=8     | $\bar{y}$=8.5      |

### Calculate the covariance matrix $M$

$$
\begin{bmatrix}
\text{cov}(x,x) & \text{cov}(x,y) \\
\text{cov}(y,x) & \text{cov}(y,y) \\
\end{bmatrix}
$$

$\Rightarrow$ $\text{cov}(x,x)$ = $\text{var}(x)$ = $\mathbf{E}$($x^2$) - $\mathbf{E}$ $(x)^2$ = $$ \frac{(16+0+25+1)}{3}=14 $$

$\Rightarrow$ $\text{cov}(y,y)$ = $\text{var}(y)$ = $\mathbf{E}$($y^2$) - $\mathbf{E}$ $(y)^2$ = $$ \frac{(6.25+20.25+12.25+30.25)}{3}=23 $$

$\Rightarrow$ $\text{cov}(x,y)$ = $\text{cov}(y,x)$ = $\mathbf{E}$($xy$) - $\mathbf{E}$ ($x$)$\mathbf{E}$ ($y$) = $$ \frac{(-10+0-17.5-5.5)}{3}=-11 $$

$\Rightarrow$ Covariance Matrix $M$ $$ 
\begin{bmatrix}
14 & -11 \\
-11 & 23 \\
\end{bmatrix} $$

### Compute the singular value decomposition (SVD)

We can obtain the principal components and loadings from SVD of the covariance matrix M since covariance matrix M is a square matrix:

$$ 
\begin{bmatrix}
14 & -11 \\
-11 & 23 \\
\end{bmatrix} \times Any\ vector = \lambda \times Any\ vector\ ,\ (vector\neq 0) 
$$

$$det(M - \lambda I) = 0$$

#### Obtain eigenvalues of the covariance matrix $\rightarrow$ $\lambda_1$ & $\lambda_2$

$$
I = \begin{bmatrix}
1 & 0 \\
0 & 1 \\
\end{bmatrix}
$$

$$
\lambda I = \lambda \times \begin{bmatrix}
1 & 0 \\
0 & 1 \\
\end{bmatrix} = \begin{bmatrix} 
\lambda  & 0 \\
0 & \lambda  \\
\end{bmatrix}
$$

$$
\begin{bmatrix}
14 & -11 \\
-11&  23 \\
\end{bmatrix} -\begin{bmatrix}
\lambda  & 0 \\
0 & \lambda  \\
\end{bmatrix} = \begin{bmatrix}
14-\lambda  & -11 \\
-11 & 23-\lambda  \\
\end{bmatrix}
$$

$\Rightarrow$

$$
det\begin{bmatrix}
 14-\lambda & -11 \\
 -11& 23-\lambda  \\
\end{bmatrix}=0
$$

$\Rightarrow$ $$(14-\lambda)(23-\lambda) - (-11)(-11) = 0 $$

$$\lambda^2 +37\lambda -201 = 0$$

$\Rightarrow$ $\lambda_1$ = 30.3849, $\lambda_2$ = 6.6152 (eigenvalues for Covariance Matrix $M$)

#### Obtain eigenvector of $\lambda_1$

$$(M - \lambda_1I)\times U_1 = \mathbf{0} $$

$\Rightarrow$ $$\begin{bmatrix}
 14-\lambda & -11 \\
 -11& 23-\lambda  \\
\end{bmatrix}\times\begin{bmatrix}
u_1 \\
u_2 \\
\end{bmatrix}=\mathbf{0}
$$

$$(14- \lambda)u_1 - 11u_2 = 0 $$ $\Rightarrow$ $$-16.3849u_1 -11u_2 = 0$$ $\Rightarrow$ $$-16.3849u_1= 11u_2 $$ $\Rightarrow$ $$u_1 = \frac{11}{-16.3849}u_2$$

$$
\begin{bmatrix}
u_1 \\
u_2 \\
\end{bmatrix}= u_2\begin{bmatrix}
\frac{11}{-16.3849}\\
1 \\
\end{bmatrix}
$$

$\Rightarrow$ $$u_2\begin{bmatrix}
-11\\ 16.3849
\end{bmatrix}$$

$$
\begin{bmatrix}
16.3849 & 11 \\
\end{bmatrix} \times \begin{bmatrix}
u_1 \\ u_2
\end{bmatrix} = \begin{bmatrix}
11 \\ -16.3849
\end{bmatrix}
$$

**Normalized Eigenvector**

$\Rightarrow$ $\lambda_1$: $e_1$

$$
\frac{1}{\sqrt{{11^2 +16.3849^2}}}\begin{bmatrix}
11 \\ -16.3849
\end{bmatrix}= \begin{bmatrix}
0.5574 \\ -0.8303
\end{bmatrix} 
$$

$\Rightarrow$ $\lambda_2$ $e_2$ (Right singular vector) =

$$
\begin{bmatrix}
0.8303 \\ 0.5574
\end{bmatrix} 
$$

|               | **Basic Stats** | **Advanced Stats** |
|---------------|-----------------|--------------------|
| **Student 1** | **4**           | **11**             |
| **Student 2** | **8**           | **4**              |
| **Student 3** | **13**          | **5**              |
| **Student 4** | **7**           | **14**             |
| **Mean**      | $\bar{x}$=8     | $\bar{y}$=8.5      |

### Derive The new dataset

**First Principal Component (PC1)**

$$
P_{11} = e_1^T \times 
\begin{bmatrix}
4-mean(x) \\ 11 -mean(y)
\end{bmatrix}
 = \begin{bmatrix}
0.5574 & -0.8303 \\
\end{bmatrix} 
\begin{bmatrix}
4-8 \\ 11-8.5
\end{bmatrix} =-4.3052 
$$

$$
P_{12} = e_1^T \times 
\begin{bmatrix}
8-mean(x) \\ 4 -mean(y)
\end{bmatrix}
 = \begin{bmatrix}
0.5574 & -0.8303 \\
\end{bmatrix} 
\begin{bmatrix}
8-8 \\ 4-8.5
\end{bmatrix} =3.7361 
$$

$$
P_{13} = e_1^T \times 
\begin{bmatrix}
13-mean(x) \\ 5 -mean(y)
\end{bmatrix}
 = \begin{bmatrix}
0.5574 & -0.8303 \\
\end{bmatrix} 
\begin{bmatrix}
13-8 \\ 5-8.5
\end{bmatrix}= 5.6928
$$

$$
P_{14}= e_1^T \times 
\begin{bmatrix}
7-mean(x) \\ 14 -mean(y)
\end{bmatrix}
 = \begin{bmatrix}
0.5574 & -0.8303 \\
\end{bmatrix} 
\begin{bmatrix}
7-8 \\ 14-8.5
\end{bmatrix} = -5.1238
$$

<!-- -->

**The new dataset (Left singular vector)**

| **First Principal Component (PC1)** | **Student 1** | **Student 2** | **Student 3** | **Student 4** |
|:--------------------:|:-----------:|:-----------:|:-----------:|:-----------:|
|                                     |  **-4.3052**  |  **3.7361**   |  **5.6928**   |  **-5.1238**  |

### Verifying and visualizing the results

```{r echo=FALSE, message=FALSE, warning=FALSE}
#Load Libraries
library(factoextra)

#Import Example Data
data = data.frame(Student = 1:4, BasicStats=c(4,8,13,7), AdvancedStats=c(11,4,5,14))
#dim(data)

#Structure of Data
str(data) #check variable types which matters in PCA 
#summary(data) #details about variable scales and missing values

#Based on Info from Summary: handle missing values and exclude categorical variable
#na.omit(data) No missing values here
data_sample = data[,-c(1)] #exclude categorical variable

#Run PCA
data_pca = prcomp(data_sample, scale = TRUE)

#Summary of Analysis
summary(data_pca)

#Elements of PCA Object (all outputs of PCA analysis)
#names(data_pca)

#sdev:standard deviation
#rotation: eigenvectors (loadings per variable within each PC)
#center: mean of the original variable
#scale: standard deviations of the original variable
#x: principal component values/scores

#Scree Plot of Variance
fviz_eig(data_pca,
         addlabels = TRUE)
#Biplot with Default Settings
fviz_pca_biplot(data_pca, repel = TRUE)
```

### Results

The first principal component of the data captures 80.7% of the variation (or information) while reducing the dimensionality of the dataset from 2 variables to 1. Small datasets such as this make the hand-calculation of principal components feasible and easy to follow, but the strengths of PCA are especially evident when software is used to enable principal component analysis for large datasets. In the next example, we demonstrate how PCA can be used with a large dataset using the R programming language.

## PCA on a large dataset using R

For this application of PCA, the Abalone dataset from the UCI Machine Learning Repository is used @misc_abalone_1. This dataset contain 4177 observations of 9 variables which record characteristics of each abalone including sex, length, diameter, height, weights, and the number of rings. The variables, apart from sex, are continuous and correlated making the dataset an ideal candidate for demonstrating dimensionality reduction via PCA.

### Libraries

First, the appropriate and necessary libraries are loaded in R. These provide the functions which serve as the backbone of the analysis, handling the computational aspects of PCA as well as visualizing the results.

```{r, warning = FALSE, message = FALSE}
# Load necessary libraries
library(tidyverse)  # for handling missing values
library(corrplot) # for plotting the correlation matrix
library(factoextra) # PCA plots
```

### Data Preparation

```{r}
# clear environment
rm(list = ls())

# Load dataset
abalone <- read.csv('./abalone/abalone.csv')

# Dataset structure
str(abalone)

# Missing values
colSums(is.na(abalone))
```

The dataset contains 9 variables with 1 categorical variable and 8 numeric variables. The dataset contains no missing values. For this example in applying principal component analysis, we exclude the categorical variable 'Sex' and focus the PCA on the numerical dimensions of the Abalone. For analyses involving a mix of numeric and non-numeric variables other factor analysis techniques can be used, such as factor analysis of mixed data @pages2014multiple.

```{r}
# Select only the numeric variables 
abalone <- abalone %>% select(where(is.numeric))

summary(abalone)
```

The summary statistics show the differences in measurement between variables, with some variables such as diameter and viscera weight having small ranges and others, namely rings, having relatively large ranges. For this reason, scaling of the variables is a crucial step in PCA to ensure results accurately capture the variance in the data.

### Feature Scaling

Standardization ensures all variables, also called features, are on the same scale, and the scale function allows us to center the data to a mean of 0 and variance of 1. This ensures no single feature has an outsized effect during the principal component analysis.

```{r}
# Standardization of numerical features
abalone_sc <- scale(abalone, center = TRUE, scale = TRUE)

summary(abalone_sc)
```

Viewing the data after scaling and centering, values greater than 3 or less than -3 represent outliers more than 3 standard deviations from the mean. Based on the ranges of the variables, we should view a boxplot of the data to further investigate.

```{r}
# Plot a boxplot to visualize potential outliers
par(mar=c(4, 8, 4, 4))
boxplot(abalone_sc, col = "steelblue", main = "Visualization of scaled and centered data", horizontal = TRUE, las = 1)
```

Are there enough outliers to be a cause for concern? We can see how many lie outside of the third standard deviation of the data for each variable.

```{r}
print(colSums(abalone_sc > 3 | abalone_sc < -3))
```

Of the 4177 observations, at most 62 in a single variable (rings) are outliers. The tolerance for outliers will differ depending on the investigation, but for our illustrative purposes this number is well within tolerance for principal component analysis.

Lastly, we can investigate the correlation among the variables. PCA is best used with linearly correlated data. If the data is not correlated, the results of PCA will be less meaningful.

```{r}
# Calculate correlations and round to 2 digits
abalone_corr <- cor(abalone_sc)
corrplot(abalone_corr, method="number")
```

Our scaled and centered data has strong linear correlations and contains a relatively small number of outliers. We can now calculate the principal components of the dataset.

### PCA via Singular Value Decomposition

The prcomp() function @prcomp2023ref performs principal component analysis on a dataset using the singular value decomposition method, which utilizes the covariance matrix of the data.

```{r}
# Apply PCA using prcomp()
abalone_pca <- prcomp(abalone_sc)
summary(abalone_pca)
```

#### PCA - Cumulative Variance and Number of Principal Components

```{r}
# Principal Component scores vector
pc_scores <- abalone_pca$x

# Std Deviation of Components
component_sdev <- abalone_pca$sdev

# Eigenvector or Loadings
eigenvector <- abalone_pca$rotation

# Mean of variables
component_mean <- abalone_pca$center 

# Scaling factor of Variables
component_scale <- abalone_pca$scale

# Proportion of variance explained by each PC
variance_explained <- component_sdev^2 / sum(component_sdev^2)

# Cumulative proportion of variance explained
cumulative_variance_explained <- cumsum(variance_explained)

# Retain components that explain a percentage of the variance
num_components <- which(cumulative_variance_explained >= 0.92)[1]

# Select the desired number of principal components
selected_pcs <- pc_scores[, 1:num_components]

# Display cumulative variance
cumulative_variance_explained
```

The first 2 principal components alone explain 92% of the variance in the data.

#### Loading of First Two Components

The loading are the weights assigned to each variable for that particular principal component.

```{r}
# Access the loadings for the first two principal components
loadings_first_two_components <- eigenvector[, 1:2]

# Print the loadings for the first two principal components
print("Loadings for the first two principal components:")
print(loadings_first_two_components)
```

#### PCA - Elements

The values in **`abalone_pca$x`** are the coordinates of each observation in the new principal component space. These coordinates are the scores for each observation along each principal component. The eigenvectors of the covariance or correlation matrix of the data represent the directions of maximum variance in the dataset.

### Visualization

#### Scree Plot - Cumulative Variance Explained

```{r}
fviz_eig(abalone_pca, addlabels = TRUE)
```

The scree plot visualizes the variance captured by each PC. PC1 explains 83.9% of the variance, and PC2 explains 8.7% variance.

#### Biplot

The correlation between a variable and a principal component is used as the coordinates of the variable on the PC, shown as dimensions on the biplot. Dim1 corresponds to PC1, and Dim2 to PC2. The representation of variables differs from the plot of the observations: The observations are represented by their projections, but the variables are represented by their correlations @abdi2010principal.

```{r}
fviz_pca_biplot(abalone_pca, label = "var", alpha.ind = "contrib", col.var = "blue", repel = TRUE)
```

#### Variable Contribution

Top variable contribution for the first two principal components.

```{r}
# Contributions of variables to PC1
pc2_contribution <- fviz_contrib(abalone_pca, choice = "var", axes = 1, top = 20)

# Modify the theme to rotate X-axis labels to 90 degrees
pc2_contribution +
  theme(
    axis.text.x = element_text(angle = 0),
    plot.title = element_text(hjust = 0)  # horizontal justification
  ) +
  coord_flip() +
  labs(title = "Contribution of Variables to PC1",
       y = "Percentage Contribution",
       x = "",
       caption = "PC1 explains 83.9% of the variance") +
  scale_y_continuous(labels = scales::percent_format(scale = 1,
                                                     accuracy = 1))


# Contributions of variables to PC2
pc2_contribution <- fviz_contrib(abalone_pca, choice = "var", axes = 2, top = 12)

# Modify the theme to rotate X-axis labels to 90 degrees
pc2_contribution +
  theme(
    axis.text.x = element_text(angle = 0),
    plot.title = element_text(hjust = 0)  # horizontal justification
  ) +
  coord_flip() +
  labs(title = "Contribution of Variables to PC2",
       y = "Percentage Contribution",
       x = "",
       caption = "PC2 explains 8.7% of the variance") +
  scale_y_continuous(labels = scales::percent_format(scale = 1,
                                                     accuracy = 1))
```

### Results

The first principal component captures 83.9% of the variance in the data. This linear combination has relatively equal loadings for whole weight, diameter, length, shell weight, viscera weight, and shucked weight, with height and rings having lower loadings. The second principal component is mostly influenced by the variable rings which makes up over 80% of the contribution to PC2. The biplot is an effective visualization of how each variable contributes to PC1, or dimension 1 on the graph, and PC2, or dimension 2 on the graph. The length and direction of each vector represent the contribution of each variable to the principal components; whole weight and rings are the longest, representing the largest contributions to PC1 and PC2 respectively.

PCA is primarily an exploratory tool, which allows us to visualize high-dimensional data in lower dimensions as shown above in the biplot and accompanying scree plot. These PCs can be used to explore data in other ways, such as looking for trends and patterns in the data or identifying clusters and outliers. In the formal analysis in the following chapter, the applications of PCA are further explored through the development of a regression model on the principal components of a dataset.
