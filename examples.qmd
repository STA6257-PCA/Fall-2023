# Examples

## Step-by-Step Calculation of Principal Component Analysis

In this illustration, we have access to the two grades of four students in a statistics subject. We aim to employ Principal Component Analysis (PCA) as a means to reduce the dimensionality from two variables to a singular variable. This transformation will effectively represent students' performance in the subject with a more compact and interpretable measure. This example is adapted from the resource *How to compute principal components*. @eduflair2020

| Scores        | **Basic Stats** | **Advanced Stats** |
|---------------|-----------------|--------------------|
| **Student 1** | **4**           | **11**             |
| **Student 2** | **8**           | **4**              |
| **Student 3** | **13**          | **5**              |
| **Student 4** | **7**           | **14**             |
| **Mean**      | $\bar{x}$=8     | $\bar{y}$=8.5      |

**Calculate the Covariance Matrix** $M$

$$
\begin{bmatrix}
\text{cov}(x,x) & \text{cov}(x,y) \\
\text{cov}(y,x) & \text{cov}(y,y) \\
\end{bmatrix}
$$

$\Rightarrow$ $\text{cov}(x,x)$ = $\text{var}(x)$ = $\mathbf{E}$($x^2$) - $\mathbf{E}$ $(x)^2$ = $$ \frac{(16+0+25+1)}{3}=14 $$

$\Rightarrow$ $\text{cov}(y,y)$ = $\text{var}(y)$ = $\mathbf{E}$($y^2$) - $\mathbf{E}$ $(y)^2$ = $$ \frac{(6.25+20.25+12.25+30.25)}{3}=23 $$

$\Rightarrow$ $\text{cov}(x,y)$ = $\text{cov}(y,x)$ = $\mathbf{E}$($xy$) - $\mathbf{E}$ ($x$)$\mathbf{E}$ ($y$) = $$ \frac{(-10+0-17.5-5.5)}{3}=-11 $$

$\Rightarrow$ Covariance Matrix $M$ $$ 
\begin{bmatrix}
14 & -11 \\
-11 & 23 \\
\end{bmatrix}
$$

**Compute the singular value decomposition (SVD): Calculate Eigenvalues and Eigenvectors of the Covariance Matrix** $M$

We can obtain the principal component and loadings from SVD of the covariance matrix M since covariance matrix M is a square matrix:

$$ 
\begin{bmatrix}
14 & -11 \\
-11 & 23 \\
\end{bmatrix} \times Any\ vector = \lambda \times Any\ vector\ ,\ (vector\neq 0) 
$$

$$det(M - \lambda I) = 0$$

**Obtain Eigenvalues of the Covariance Matrix** $\rightarrow$ $\lambda_1$ & $\lambda_2$

Identity Matrix $I$ =

$$
\begin{bmatrix}
1 & 0 \\
0 & 1 \\
\end{bmatrix}
$$

$\lambda I$ = $$ \lambda \times \begin{bmatrix}
1 & 0 \\
0 & 1 \\
\end{bmatrix} = \begin{bmatrix} 
\lambda  & 0 \\
0 & \lambda  \\
\end{bmatrix}
$$

$$
\begin{bmatrix}
14 & -11 \\
-11&  23 \\
\end{bmatrix} -\begin{bmatrix}
\lambda  & 0 \\
0 & \lambda  \\
\end{bmatrix} = \begin{bmatrix}
14-\lambda  & -11 \\
-11 & 23-\lambda  \\
\end{bmatrix}
$$

$\Rightarrow$

$$
det\begin{bmatrix}
 14-\lambda & -11 \\
 -11& 23-\lambda  \\
\end{bmatrix}=0
$$

$\Rightarrow$ $$(14-\lambda)(23-\lambda) - (-11)(-11) = 0 $$

$$\lambda^2 +37\lambda -201 = 0$$

$\Rightarrow$ $\lambda_1$ = 30.3849, $\lambda_2$ = 6.6152 (eigenvalues for Covariance Matrix $M$)

**Obtain Eigenvector of** $\lambda_1$

$$(M - \lambda_1I)\times U_1 = \mathbf{0} $$

$\Rightarrow$ $$\begin{bmatrix}
 14-\lambda & -11 \\
 -11& 23-\lambda  \\
\end{bmatrix}\times\begin{bmatrix}
u_1 \\
u_2 \\
\end{bmatrix}=\mathbf{0}
$$

$$(14- \lambda)u_1 - 11u_2 = 0 $$ $\Rightarrow$ $$-16.3849u_1 -11u_2 = 0$$ $\Rightarrow$ $$-16.3849u_1= 11u_2 $$ $u_1 = \frac{11}{-16.3849}u_2$

$$
\begin{bmatrix}
u_1 \\
u_2 \\
\end{bmatrix}= u_2\begin{bmatrix}
\frac{11}{-16.3849}\\
1 \\
\end{bmatrix}
$$

$\Rightarrow$ $$u_2\begin{bmatrix}
-11\\ 16.3849
\end{bmatrix}$$

$$
\begin{bmatrix}
16.3849 & 11 \\
\end{bmatrix} \times \begin{bmatrix}
u_1 \\ u_2
\end{bmatrix} = \begin{bmatrix}
11 \\ -16.3849
\end{bmatrix}
$$

Normalized Eigenvector\

$\Rightarrow$ $\lambda_1$: $e_1$

$$
\frac{1}{\sqrt{{11^2 +16.3849^2}}}\begin{bmatrix}
11 \\ -16.3849
\end{bmatrix}= \begin{bmatrix}
0.5574 \\ -0.8303
\end{bmatrix} 
$$

$\Rightarrow$ $\lambda_2$ $e_2$ (Right singular vector) =

$$
\begin{bmatrix}
0.8303 \\ 0.5574
\end{bmatrix} 
$$

|               | **Basic Stats** | **Advanced Stats** |
|---------------|-----------------|--------------------|
| **Student 1** | **4**           | **11**             |
| **Student 2** | **8**           | **4**              |
| **Student 3** | **13**          | **5**              |
| **Student 4** | **7**           | **14**             |
| **Mean**      | $\bar{x}$=8     | $\bar{y}$=8.5      |

Derive The New Dataset

| **First Principal Component (PC1)** | $P_{11}$ | $P_{12}$ | $P_{13}$ | $P_{14}$ |
|-------------------------------------|----------|----------|----------|----------|

$$
P_{11} = e_1^T \times 
\begin{bmatrix}
4-mean(x) \\ 11 -mean(y)
\end{bmatrix}
 = \begin{bmatrix}
0.5574 & -0.8303 \\
\end{bmatrix} 
\begin{bmatrix}
4-8 \\ 11-8.5
\end{bmatrix} =-4.3052 
$$

$$
P_{12} = e_1^T \times 
\begin{bmatrix}
8-mean(x) \\ 4 -mean(y)
\end{bmatrix}
 = \begin{bmatrix}
0.5574 & -0.8303 \\
\end{bmatrix} 
\begin{bmatrix}
8-8 \\ 4-8.5
\end{bmatrix} =3.7361 
$$

$$
P_{13} = e_1^T \times 
\begin{bmatrix}
13-mean(x) \\ 5 -mean(y)
\end{bmatrix}
 = \begin{bmatrix}
0.5574 & -0.8303 \\
\end{bmatrix} 
\begin{bmatrix}
13-8 \\ 5-8.5
\end{bmatrix}= 5.6928
$$

$$
P_{14}= e_1^T \times 
\begin{bmatrix}
7-mean(x) \\ 14 -mean(y)
\end{bmatrix}
 = \begin{bmatrix}
0.5574 & -0.8303 \\
\end{bmatrix} 
\begin{bmatrix}
7-8 \\ 14-8.5
\end{bmatrix} = -5.1238
$$

### The New Dataset (Left singular vector)

| **First Principal Component (PC1)** | **Student 1** | **Student 2** | **Student 3** | **Student 4** |
|:-----------------------------------:|:-------------:|:-------------:|:-------------:|:-------------:|
|                                     |  **-4.3052**  |  **3.7361**   |  **5.6928**   |  **-5.1238**  |

Use R code to analyze the same example

```{r}
#Load Libraries
library(MASS)
library(factoextra)
library(ggplot2)

#Import Example Data
data = data.frame(Student = 1:4, BasicStats=c(4,8,13,7), AdvancedStats=c(11,4,5,14))
dim(data)

#Structure of Data
str(data) #check variable types which matters in PCA 
summary(data) #details about variable scales and missing values

#Based on Info from Summary: handle missing values and exclude categorical variable
#na.omit(data) No missing values here
data_sample = data[,-c(1)] #exclude categorical variable

#Run PCA
data_pca = prcomp(data_sample, scale = TRUE)


#Summary of Analysis
summary(data_pca)

#Elements of PCA Object (all outputs of PCA analysis)
names(data_pca) 
#sdev:standard deviation
#rotation: eigenvectors (loadings per variable within each PC)
#center: mean of the original variable
#scale: standard deviations of the original variable
#x:principal component values/scores

#Scree Plot of Variance
fviz_eig(data_pca,
         addlabels = TRUE)
#Biplot with Default Settings
fviz_pca_biplot(data_pca)

#Biplotwith Labeled Variables
#fviz_pca_biplot(data_pca,label = "var")

```

## PCA using R

### Libraries

```{r setup, echo=FALSE}
# Install libraries if needed
# install.packages("caret")
# install.packages("corrplot")
# install.packages("Metrics")
# install.packages("caTools")
# install.packages("EnvStats")
# install.packages("car")
# install.packages("corrr")
# install.packages("ggcorrplot")
# install.packages("FactoMineR")
# install.packages("factoextra")
# install.packages("pls")
# install.packages("e1071")
# install.packages("plotly")
```

```{r, warning = FALSE, message = FALSE}
# Load necessary libraries
library(dplyr)
library(tidyverse)  # for handling missing values
# library(EnvStats) # for rosnerTest
# library(caTools)
library(caret)
library(corrplot)
# library(Metrics)
# library(car)        # for outliers test
# library(corrr)      # correlation matrix
# library(ggcorrplot) # correlation graph
# library(FactoMineR) # PCA analysis
library(factoextra) # PCA plots
# library(pls)        # PC regression
# library(e1071)      # to fit transform PCA  
# library(plotly)
```

### Data Preparation

```{r}
# clear environment
rm(list = ls())

# Constant seed
my_seed = 95

# Load dataset
abalone <- read.csv('./abalone/abalone.csv')

# Dataset structure
str(abalone)

# Missing values
colSums(is.na(abalone))
```

The dataset contains 9 variables with 1 categorical variable and 8 numeric variables. The dataset contains no missing values. For this example in applying principal component analysis, we exclude the categorical variable 'Sex' and focus the PCA on the numerical dimensions of the Abalone. For analyses involving a mix of numeric and non-numeric variables other factor analysis techniques can be used, such as factor analysis of mixed data. @pages2014multiple

```{r}
# Select only the numeric variables 
abalone <- abalone %>% select(where(is.numeric))

summary(abalone)
```

The summary statistics show the differences in measurement between variables, with some variables such as diameter and viscera weight having small ranges and others, namely rings, having relatively large ranges. For this reason, scaling of the variables is a crucial step in PCA to ensure results accurately capture the variance in the data.

### Feature Scaling

Standardization ensures all variables, also called features, are on the same scale, and the scale function allows us to center the data to a mean of 0 and variance of 1. This ensures no single feature has an outsized effect during the principal component analysis.

```{r}
# Standardization of numerical features
abalone_sc <- scale(abalone, center = TRUE, scale = TRUE)

summary(abalone_sc)
```

Viewing the data after scaling and centering, values greater than 3 or less than -3 represent outliers more than 3 standard deviations from the mean. Based on the ranges of the variables, we should view a boxplot of the data to further investigate.

```{r}
# Plot a boxplot to visualize potential outliers
par(mar=c(4, 8, 4, 4))
boxplot(abalone_sc, main = "Visualization of scaled and centered data", horizontal = TRUE, las = 1)
```

Are there enough outliers to be a cause for concern? We can see how many lie outside of the third standard deviation of the data for each variable.

```{r}
print(colSums(abalone_sc > 3 | abalone_sc < -3))
```

Of the 4177 observations, at most 62 in a single variable (rings) are outliers. The tolerance for outliers will differ depending on the investigation, but for our purposes this number is well within tolerance for principal component analysis.

Lastly, we can investigate the correlation among the variables. PCA is best used with linearly correlated data. If the data is not correlated, the results of PCA will be less meaningful.

```{r}
# Calculate correlations and round to 2 digits
abalone_corr <- cor(abalone_sc)
corrplot(abalone_corr, method="number")
```

Our scaled and centered data has strong linear correlations and contains a relatively small number of outliers. We can now calculate the principal components of the dataset.

### PCA via Singular Value Decomposition

The prcomp() function in R performs principal component analysis on a dataset using the singular value decomposition method which utilizes the covariance matrix of the data. @prcomp2023ref

```{r}
# Apply PCA using prcomp()
abalone_pca <- prcomp(abalone_sc)
summary(abalone_pca)
```

#### PCA - Cumulative Variance and Number of Principal Components

```{r}
# Principal Component scores vector
pc_scores <- abalone_pca$x

# Std Deviation of Components
component_sdev <- abalone_pca$sdev

# Eigenvector or Loadings
eigenvector <- abalone_pca$rotation

# Mean of variables
component_mean <- abalone_pca$center 

# Scaling factor of Variables
component_scale <- abalone_pca$scale

# Proportion of variance explained by each PC
variance_explained <- component_sdev^2 / sum(component_sdev^2)

# Cumulative proportion of variance explained
cumulative_variance_explained <- cumsum(variance_explained)

# Retain components that explain a percentage of the variance
num_components <- which(cumulative_variance_explained >= 0.92)[1]

# Select the desired number of principal components
selected_pcs <- pc_scores[, 1:num_components]

# Display cumulative variance
cumulative_variance_explained
```

The first 2 principal components alone explain 92% of the variance in the data.

#### Loading of First Two Components

The loading are the weights assigned to each variable for that particular principal component.

```{r}
# Access the loadings for the first two principal components
loadings_first_two_components <- eigenvector[, 1:2]

# Print the loadings for the first two principal components
print("Loadings for the first two principal components:")
print(loadings_first_two_components)
```

#### PCA - Elements

The values in **`abalone_pca$x`** are the coordinates of each observation in the new principal component space. These coordinates are the scores for each observation along each principal component. The eigenvectors of the covariance or correlation matrix of the data represent the directions of maximum variance in the dataset.

### Visualization

#### Scree Plot - Cumulative Variance Explained

```{r}
fviz_eig(abalone_pca, addlabels = TRUE)
```

The scree plot visualizes the variance captured by each PC. PC1 explains 83.9% of the variance, and PC2 explains 8.7% variance.

#### Biplot

The correlation between a variable and a principal component is used as the coordinates of the variable on the PC, shown as dimensions on the biplot. Dim1 corresponds to PC1, and Dim2 to PC2. The representation of variables differs from the plot of the observations: The observations are represented by their projections, but the variables are represented by their correlations. (Abdi and Williams 2010) \[ref\]

```{r}
fviz_pca_biplot(abalone_pca, label = "var", alpha.ind = "contrib", col.var = "blue", repel = TRUE)
```

#### Variable Contribution

Top variable contribution for the first two principal components.

```{r}
# Contributions of variables to PC1
pc2_contribution <- fviz_contrib(abalone_pca, choice = "var", axes = 1, top = 20)

# Modify the theme to rotate X-axis labels to 90 degrees
pc2_contribution +
  theme(
    axis.text.x = element_text(angle = 0),
    plot.title = element_text(hjust = 0)  # horizontal justification
  ) +
  coord_flip() +
  labs(title = "Contribution of Variables to PC1",
       y = "Percentage Contribution",
       x = "",
       caption = "PC1 explains 83.9% of the variance") +
  scale_y_continuous(labels = scales::percent_format(scale = 1,
                                                     accuracy = 1))


# Contributions of variables to PC2
pc2_contribution <- fviz_contrib(abalone_pca, choice = "var", axes = 2, top = 12)

# Modify the theme to rotate X-axis labels to 90 degrees
pc2_contribution +
  theme(
    axis.text.x = element_text(angle = 0),
    plot.title = element_text(hjust = 0)  # horizontal justification
  ) +
  coord_flip() +
  labs(title = "Contribution of Variables to PC2",
       y = "Percentage Contribution",
       x = "",
       caption = "PC2 explains 8.7% of the variance") +
  scale_y_continuous(labels = scales::percent_format(scale = 1,
                                                     accuracy = 1))
```
